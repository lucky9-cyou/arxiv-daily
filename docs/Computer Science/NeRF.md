
### NeRF
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-28**|**Self-supervised Pre-training for Transferable Multi-modal Perception**|Xiaohao Xu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a self-supervised pre-training framework, NS-MAE, for multi-modal representation learning in autonomous driving, which learns transferable representations across diverse input modalities, architectures, and downstream tasks.|[2405.17942v1](http://arxiv.org/abs/2405.17942v1)|null|
|**2024-05-28**|**A Refined 3D Gaussian Representation for High-Quality Dynamic Scene Reconstruction**|Bin Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper introduces a hybrid 3D Gaussian representation combining deformation fields, hash encoding, and denoising masks to achieve high-quality rendering and efficient dynamic scene reconstruction while reducing memory usage.|[2405.17891v1](http://arxiv.org/abs/2405.17891v1)|null|
|**2024-05-28**|**HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal High-Frequency Components for Endoscopic Scene Reconstruction**|Haoyu Zhao et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes HFGS, a novel approach for deformable endoscopic reconstruction that addresses under-reconstruction in both static and dynamic scenes by incorporating deformation fields and high-frequency emphasizing reconstruction from spatial and temporal perspectives.|[2405.17872v1](http://arxiv.org/abs/2405.17872v1)|null|
|**2024-05-28**|**Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh**|Xiangjun Gao et.al.|The key contributions from the paper's abstract and introduction can be summarized as:* The authors propose a method for manipulating 3D Gaussian Splatting (3DGS) representations while preserving high-quality rendering, using a triangular mesh as a proxy.* The method introduces a triangle shape-aware Gaussian binding strategy with self-adaptation, which allows for flexible manipulation of 3DGS and maintains rendering quality even with inaccurate meshes.* The paper demonstrates state-of-the-art results in various 3DGS manipulation tasks, including large deformations, local manipulations, and soft body simulations.|[2405.17811v1](http://arxiv.org/abs/2405.17811v1)|null|
|**2024-05-27**|**F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting**|Xiangyu Sun et.al.|The key contributions from the paper's abstract and introduction are:The Factorized 3D Gaussian Splatting (F-3DGS) method significantly reduces storage requirements while maintaining image quality, by approximating dense clusters of Gaussians with a limited number of elements through efficient factorization. The method employs structured coordinates and decomposed representations of Gaussians, exploiting structural patterns and redundancies to compress the model size while preserving essential characteristics. F-3DGS achieves a storage reduction of over 90% while maintaining comparable image quality, and demonstrates fast rendering speeds and compact storage.|[2405.17083v2](http://arxiv.org/abs/2405.17083v2)|null|
|**2024-05-27**|**PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting**|Zipeng Wang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents Pyramidal 3D Gaussian Splatting (PyGS), a method that addresses the challenges of scaling 3D Gaussian Splatting to large-scale scenes by introducing a hierarchical pyramidal structure and dynamic weighting, achieving significant performance leaps and rendering speedups.|[2405.16829v2](http://arxiv.org/abs/2405.16829v2)|null|
|**2024-05-26**|**Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors**|Soumava Paul et.al.|The paper presents a method, SparseSplat360, that uses pre-trained 2D diffusion models and iterative updates to reconstruct a 360 3D scene from sparse views, achieving improved performance over existing methods and generating high-quality scenes with details coherent with observations.|[2405.16517v1](http://arxiv.org/abs/2405.16517v1)|null|
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|The key contributions from the paper's abstract and introduction are:* The introduction of Neural Elevation Models (NEMos), which adapt Neural Radiance Fields to a 2.5D continuous and differentiable terrain model, allowing for a lightweight representation of terrain through an implicit continuous and differentiable height field.* A novel method for jointly training a height field and radiance field within a NeRF framework, leveraging quantile regression.* A path planning algorithm that performs gradient-based optimization of a continuous cost function for minimizing distance, slope changes, and control effort, enabled by the differentiability of the height field.* The ability to generate high-quality reconstructions and produce smoother paths compared to discrete path planning methods, as demonstrated through experiments on simulated and real-world terrain imagery.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes HDR-GS, a novel framework for 3D High Dynamic Range (HDR) imaging, which uses a Dual Dynamic Range Gaussian point cloud model, Parallel Differentiable Rasterization processes, and recalibrated camera parameters to efficiently render HDR views and LDR images with controllable exposure time.|[2405.15125v2](http://arxiv.org/abs/2405.15125v2)|[link](https://github.com/caiyuanhao1998/hdr-gs)|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes GS-Hider, a steganography framework for 3D Gaussian Splatting (3DGS) that embeds 3D scenes and images into original 3DGS point clouds in an invisible manner, demonstrating robust security, high fidelity, large capacity, and strong versatility.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|The paper introduces an approach that leverages ray tracing to improve Neural Radiance Fields (NeRFs) for rendering high-frequency view-dependent appearances, particularly specular reflections, in real-world scenes, achieving more efficient rendering and higher quality reflections compared to prior methods.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The authors propose Neural Directional Encoding (NDE), a feature-grid-like approach that efficiently encodes high-frequency angular signals and spatially varying directional information for novel-view synthesis of specular objects, achieving high-quality modeling and fast evaluation.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a two-staged pipeline that normalizes images with varying lighting and shadow conditions to improve camera relocalization, utilizing a hash-encoded NeRF with a re-devised truncated dynamic low-pass filter and numerical gradient averaging technique.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a novel feed-forward framework, called LDM, that generates high-quality 3D mesh assets with illumination-decoupled textures from single images or text prompts, utilizing a multi-view diffusion model, transformer-based SDF field prediction, and gradient-based mesh optimization.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The authors propose JointRF, a novel end-to-end joint optimization scheme for dynamic Neural Radiance Field (NeRF) representation and compression, achieving improved quality and compression efficiency.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose DoGaussian, a distributed method for training 3D Gaussian Splatting (3DGS) on large-scale scenes, reducing training time by 6+ times while achieving state-of-the-art rendering quality and maintaining high GPU memory efficiency.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The authors propose Gaussian Time Machine (GTM), a real-time rendering method that models time-dependent attributes of Gaussian primitives, achieving state-of-the-art rendering fidelity on 3 datasets while being 100 times faster than NeRF-based counterparts and disentangling appearance changes.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:MOSS, a novel framework, introduces motion-aware Gaussian split on the human surface using kinematic information and surface deformation detection to reconstruct 3D clothed humans with realistic clothing deformation and joint details.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|The paper introduces a novel method that extends the scope of existing pose estimation methods to unknown targets, i.e., targets for which the CAD model is not available. Their method leverages a three-steps approach: an "in-the-wild" Neural Radiance Field (NeRF) that employs learnable appearance embeddings to represent varying illumination conditions, is trained on a sparse collection of images and used to generate a diverse dataset. This dataset is then used to train a pose estimation network, demonstrating the ability to estimate the 6D pose of an unknown target relative to a monocular camera in a Hardware-In-the-Loop setup.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|Tianqi Liu et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper introduces MVSGaussian, a generalizable 3D Gaussian representation approach derived from Multi-View Stereo, which achieves real-time rendering, better synthesis quality, and faster per-scene optimization, outperforming existing generalizable methods.|[2405.12218v1](http://arxiv.org/abs/2405.12218v1)|null|
|**2024-05-20**|**Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents**|Guanlin Wu et.al.|The key contributions from the paper's abstract and introduction are:* The integration of neural radiance field (NeRF) and 3D Gaussian splatting (3D-GS) in 6G networks for immersive communications, which requires efficient representation, transmission, and reconstruction of 3D contents.* The discussion of the pros and cons of NeRF and 3D-GS, including their computational efficiency, storage requirements, and rendering quality.* The importance of embracing these radiance field rendering approaches in 6G networks to support emerging 3D applications with enhanced quality of experience.* The need for new technical challenges to be tackled, including distributed training and inference methods, joint computation and communication designs, and the minimization of end-to-end latency while preserving the quality of experience requirements.|[2405.12155v1](http://arxiv.org/abs/2405.12155v1)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|Fotios Logothetis et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents a novel multi-view photometric stereo method that leverages pixel intensity information and explicitly models point light attenuation, casting shadows, and optimizing a fully neural material renderer to achieve competitive reconstruction accuracy.|[2405.12057v1](http://arxiv.org/abs/2405.12057v1)|null|
|**2024-05-19**|**Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems**|Shengxiang Sun et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction, ensuring it is under 50 words:The paper proposes a modified gradient-based texture optimization method to discover realistic-looking adversarial objects for autonomous driving systems, introducing an evaluative mechanism called the 'Judge' to assess and enhance the realism of generated objects.|[2405.11629v1](http://arxiv.org/abs/2405.11629v1)|null|
|**2024-05-19**|**R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless Environments**|Huiying Yang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:This paper proposes a novel modeling approach using Neural Radiance Fields (NeRF) to characterize the dynamics of electromagnetic fields in RIS-enabled wireless environments, accurately capturing complex signal propagation and facilitating efficient RIS deployment.|[2405.11541v1](http://arxiv.org/abs/2405.11541v1)|null|
|**2024-05-18**|**MotionGS : Compact Gaussian Splatting SLAM by Motion Filter**|Xinli Guo et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper presents a novel 3D Gaussian Splatting (3DGS)-based SLAM approach, MotionGS, that integrates deep feature extraction, dual keyframe selection, and 3DGS for accurate and efficient real-time tracking and high-fidelity reconstruction with reduced memory usage.|[2405.11129v1](http://arxiv.org/abs/2405.11129v1)|[link](https://github.com/antonio521/motiongs)|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|Here is a summary of the paper's abstract and introduction in under 50 words:This paper surveys the integration of Large Language Models (LLMs) with 3D data, highlighting their potential to advance spatial comprehension and interaction in embodied AI systems, and identifies key challenges and opportunities for future research to harness the full potential of 3D-LLMs.|[2405.10255v1](http://arxiv.org/abs/2405.10255v1)|null|
|**2024-05-15**|**From NeRFs to Gaussian Splats, and Back**|Siming He et.al.|The key contributions from the paper's abstract and introduction are:* Developing a procedure to convert back and forth between implicit representations of a scene, such as neural radiance fields (NeRFs), and explicit representations, such as Gaussian splatting (GS).* Showcasing the benefits of this conversion, including the ability to achieve better generalization to new views, compact representation, and real-time rendering.* Highlighting the limitations of single-representation approaches, such as NeRFs' inability to generalize to new views and GS's inability to accurate represent complex scenes.Note that the abstract and introduction primarily provide an overview of the paper's main contribution and goals, and do not include much technical detail.|[2405.09717v1](http://arxiv.org/abs/2405.09717v1)|[link](https://github.com/grasp-lyrl/nerftogsandback)|
|**2024-05-14**|**Dynamic NeRF: A Review**|Jinwei Lin et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:This review provides an in-depth analysis of the development and implementation principles of Dynamic Neural Radiance Fields (NeRF), highlighting the difficulties and potential of this technology for practical applications, and comparing and discussing various features and methods of Dynamic NeRF.|[2405.08609v1](http://arxiv.org/abs/2405.08609v1)|null|
|**2024-05-13**|**Synergistic Integration of Coordinate Network and Tensorial Feature for Improving Neural Radiance Fields from Sparse Inputs**|Mingyu Kim et.al.|The key contributions from the paper's abstract and introduction are:* The proposed method synergistically integrates a coordinate-based network with a multi-plane representation to improve NeRFs from sparse inputs.* The coordinate-based network is responsible for capturing low-frequency details, while the multi-plane representation focuses on capturing fine-grained details.* The residual concatenation of coordinates and multi-plane features across the first two hidden layer blocks enhances the efficiency in responding to coordinates.* The proposed method achieves comparable results to explicit encoding with fewer parameters and outperforms baselines in dynamic NeRFs from sparse inputs.|[2405.07857v1](http://arxiv.org/abs/2405.07857v1)|[link](https://github.com/mingyukim87/synergynerf)|
|**2024-05-12**|**Point Resampling and Ray Transformation Aid to Editable NeRF Models**|Zhenyang Li et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a point-based editable NeRF pipeline, PR^2T-NeRF, which utilizes an implicit ray transformation strategy and a differentiable neural-point resampling module to achieve state-of-the-art performance in 3D object removal and inpainting tasks while supporting high-quality rendering.|[2405.07306v1](http://arxiv.org/abs/2405.07306v1)|null|
