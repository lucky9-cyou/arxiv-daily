
### NeRF
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-27**|**F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting**|Xiangyu Sun et.al.|The paper's abstract and introduction discuss the limitations of 3D Gaussian Splatting (3DGS), which achieves rapid rendering speeds while maintaining excellent image quality but requires substantial storage and large memory footprints due to the numerous 3D Gaussians involved. The authors propose Factorized 3D Gaussian Splatting (F-3DGS), which significantly reduces storage costs by employing factorization techniques, including canonical polyadic and vector-matrix decompositions, to represent dense 3D Gaussians using fewer Gaussians. The approach achieves a 90% reduction in storage usage while maintaining comparable image quality, making it a feasible solution for real-time applications with limited resources.|[2405.17083v1](http://arxiv.org/abs/2405.17083v1)|null|
|**2024-05-27**|**PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting**|Zipeng Wang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in one sentence under 50 words:The paper presents Pyramidal 3D Gaussian Splatting (PyGS), a method that scales 3D Gaussian Splatting to large scenes by arranging Gaussians in a pyramidal structure, initializing them with a rapidly trained NeRF, and dynamically weighting their contributions for efficient rendering.|[2405.16829v1](http://arxiv.org/abs/2405.16829v1)|null|
|**2024-05-26**|**Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors**|Soumava Paul et.al.|The paper presents SparseSplat360, a method that leverages pretrained 2D diffusion models with low-cost fine-tuning to reconstruct 360-degree scenes from sparse views, using an iterative update strategy to fuse generated views with initial sparse inputs.|[2405.16517v1](http://arxiv.org/abs/2405.16517v1)|null|
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|Here are the key contributions summarized in a single sentence under 50 words:The paper introduces Neural Elevation Models (NEMos), which integrate Neural Radiance Fields with a height field for terrain representation, enabling continuous and differentiable terrain mapping and path planning using joint training and quantile regression.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes High Dynamic Range Gaussian Splatting (HDR-GS), a novel framework for 3D HDR imaging that efficiently renders novel HDR views and reconstructs LDR images with controllable exposure time, outperforming state-of-the-art NeRF-based methods while enjoying faster training and inference speed.|[2405.15125v2](http://arxiv.org/abs/2405.15125v2)|[link](https://github.com/caiyuanhao1998/hdr-gs)|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes GS-Hider, a steganography framework for 3D Gaussian Splatting (3DGS) that can embed 3D scenes and images into original point cloud files, ensuring security, fidelity, and high capacity, with applications in copyright protection, encrypted communication, and compression of 3DGS.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|The paper proposes an approach to improve Neural Radiance Fields (NeRFs) by introducing ray tracing to render high-frequency view-dependent appearance, particularly for objects with shiny reflections, while reducing computational expense and increasing rendering speed.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|The key contributions of the paper are the introduction of Neural Directional Encoding (NDE) and its application to novel-view synthesis of specular objects, achieving both high-quality modeling of view-dependent effects and fast evaluation, and proposing a novel spatio-spatial parameterization to encode near-field interreflections.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a two-staged pipeline that normalizes images with varying lighting and shadow conditions, implements a hash-encoded NeRF for fast training and robust camera pose refinement, and addresses noisy image gradient computing issues through a re-devised filter and gradient averaging technique.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The authors propose Light-Directional Mesh (LDM), a novel feed-forward framework generating high-fidelity, textured 3D mesh from a single image or text prompts, utilizing a multi-view diffusion model, transformer-based SDF prediction, and gradient-based mesh optimization.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|The paper proposes JointRF, a novel end-to-end joint optimization scheme for dynamic Neural Radiance Field (NeRF) representation and compression, achieving improved quality and compression efficiency, particularly in handling large motions and long-duration sequences.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The authors propose DoGaussian, a distributed training method for 3D Gaussian Splatting (3DGS) that splits scenes into blocks, trains 3DGS locally on each block, and guarantees consistency through a shared 3D Gaussian model, achieving 6+ times faster training time while maintaining high rendering quality.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper presents Gaussian Time Machine (GTM), a real-time rendering method that models time-dependent attributes of Gaussian primitives, achieving state-of-the-art rendering fidelity, 100 times faster rendering than NeRF-based methods, and disentangling appearance changes for smooth interpolation.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|The paper introduces an innovative framework, Motion-Based 3D Clothed Humans Synthesis (MOSS), which employs kinematic information to achieve motion-aware Gaussian split on the human surface, overcoming limitations of current methodologies that neglect the influence of motion on surface deformation.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|The key contributions from the paper's abstract and introduction are:The development of a novel method that enables an "off-the-shelf" spacecraft pose estimation model to be applied on an unknown target, which is a key step towards autonomous rendezvous and proximity operations in Active Debris Removal missions. The method uses a Neural Radiance Field (NeRF) to generate a large and diverse dataset from a sparse set of spaceborne images, which is then used to train an off-the-shelf pose estimation network. The method is validated on Hardware-In-the-Loop images of SPEED+, demonstrating the ability to successfully estimate the pose of an unknown target using a model-agnostic pose estimation network.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|Tianqi Liu et.al.|Here is a summary of the key contributions in a single sentence under 50 words:MVSGaussian introduces a generalizable 3D Gaussian representation approach that leverages Multi-View Stereo and a hybrid Gaussian rendering method, achieving real-time rendering with better synthesis quality, 13.3x less training computational cost, and fast per-scene optimization.|[2405.12218v1](http://arxiv.org/abs/2405.12218v1)|null|
|**2024-05-20**|**Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents**|Guanlin Wu et.al.|The key contributions of the paper's abstract and introduction are:* The integration of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3D-GS) techniques in 6G wireless networks for immersive communication experiences.* A comprehensive overview of the basics of radiance field rendering techniques, their applications, and implementation challenges over wireless networks.* The over-the-air training of NeRF and 3D-GS models over wireless networks using various learning techniques, with a focus on federated learning design over a hierarchical device-edge-cloud architecture.* The discussion of three practical rendering architectures of NeRF and 3D-GS models at wireless network edges, model compression approaches, and joint computation and communication designs to enhance rendering efficiency.* The proposal of a new semantic communication-enabled 3D content transmission design, where radiance field models are exploited as semantic knowledge bases to reduce communication overhead for distributed inference.|[2405.12155v1](http://arxiv.org/abs/2405.12155v1)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|Fotios Logothetis et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents a novel multi-view photometric stereo method that leverages per-pixel intensity renderings, models point light attenuation, and optimizes a fully neural material renderer to achieve competitive reconstruction accuracy using only 6 lights.|[2405.12057v1](http://arxiv.org/abs/2405.12057v1)|null|
|**2024-05-19**|**Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems**|Shengxiang Sun et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a modified gradient-based texture optimization method to discover realistic-looking adversarial objects for autonomous driving systems, introducing an evaluative mechanism called the "Judge" to assess and refine the realism of generated objects.|[2405.11629v1](http://arxiv.org/abs/2405.11629v1)|null|
|**2024-05-19**|**R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless Environments**|Huiying Yang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose a novel NeRF-based ray tracing method to accurately model dynamic electromagnetic fields in RIS-enabled wireless environments, enabling efficient RIS deployment and improving communication efficiency.|[2405.11541v1](http://arxiv.org/abs/2405.11541v1)|null|
|**2024-05-18**|**MotionGS : Compact Gaussian Splatting SLAM by Motion Filter**|Xinli Guo et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper proposes a novel 3DGS-based SLAM approach, MotionGS, which integrates deep feature extraction, dual keyframe selection, and 3D Gaussian splatting to achieve high-fidelity scene representation, accurate real-time tracking, and reduced memory usage.|[2405.11129v1](http://arxiv.org/abs/2405.11129v1)|[link](https://github.com/antonio521/motiongs)|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper provides a comprehensive survey of methodologies enabling large language models to process, understand, and generate 3D data, highlighting their potential to advance spatial comprehension and interaction within embodied AI systems, and identifying challenges and future research directions.|[2405.10255v1](http://arxiv.org/abs/2405.10255v1)|null|
|**2024-05-15**|**From NeRFs to Gaussian Splats, and Back**|Siming He et.al.|The paper presents a method to convert between implicit scene representations, such as neural radiance fields (NeRFs), and explicit scene representations, such as Gaussian splatting (GS). The authors demonstrate that their approach can achieve the best of both worlds, providing both superior rendering quality and real-time rendering capability. The approach is illustrated through a number of examples, including rendering images from training views, fitting or updating a NeRF, and modifying the NeRF to update the map and distilled features.|[2405.09717v1](http://arxiv.org/abs/2405.09717v1)|[link](https://github.com/grasp-lyrl/nerftogsandback)|
|**2024-05-14**|**Dynamic NeRF: A Review**|Jinwei Lin et.al.|Here is a summary of the paper's abstract and introduction in one sentence under 50 words:The paper reviews the development and implementation principles of Dynamic NeRF, an implicit method for 3D reconstruction and representation with high resolution, highlighting its potential in practical applications and future research directions.|[2405.08609v1](http://arxiv.org/abs/2405.08609v1)|null|
|**2024-05-13**|**Synergistic Integration of Coordinate Network and Tensorial Feature for Improving Neural Radiance Fields from Sparse Inputs**|Mingyu Kim et.al.|The key contributions from the paper's abstract and introduction are:* The proposed method synergistically integrates a coordinate-based network with a multi-plane representation to improve the performance of neural radiance fields (NeRFs) from sparse inputs.* The method uses residual connections to combine the two features, allowing them to preserve their inherent properties and improving the stability and efficiency of training.* The proposed method outperforms existing methods in dynamic NeRFs from sparse inputs, achieving comparable results to explicit encoding with fewer parameters.* The method reduces the number of parameters by skipping the allocation of a spatial low-resolution grid and replacing it with coordinate-based features.* The proposed method is effective in capturing low-frequency details and reducing overfitting, making it suitable for real-world applications.|[2405.07857v1](http://arxiv.org/abs/2405.07857v1)|[link](https://github.com/mingyukim87/synergynerf)|
|**2024-05-12**|**Point Resampling and Ray Transformation Aid to Editable NeRF Models**|Zhenyang Li et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes an implicit ray transformation strategy and a differentiable neural-point resampling (DNR) module to enable point-based editable NeRF pipeline PR^2T-NeRF, allowing for direct object pose manipulation and inpainting of empty regions, achieving state-of-the-art performance in 3D object removal and inpainting tasks.|[2405.07306v1](http://arxiv.org/abs/2405.07306v1)|null|
|**2024-05-12**|**Hologram: Realtime Holographic Overlays via LiDAR Augmented Reconstruction**|Ekansh Agrawal et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper introduces a novel approach to generating real-time 3D facial reconstructions using LiDAR augmented reconstruction, leveraging monocular depth estimation, LiDAR + TrueDepth, and template modeling techniques to achieve high-fidelity and interactive holographic overlays.|[2405.07178v1](http://arxiv.org/abs/2405.07178v1)|null|
|**2024-05-11**|**TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural Radiance Field Optimization**|Zhen Tan et.al.|The contributions of this paper are a novel approach, Truncated Depth NeRF (TD-NeRF), that enables training Neural Radiance Fields (NeRF) from unknown camera poses by jointly optimizing learnable parameters of the radiance field and camera poses, and employs a robust inter-frame point constraint that enhances robustness against depth noise during training, resulting in superior performance and more accurate depth geometry.|[2405.07027v1](http://arxiv.org/abs/2405.07027v1)|[link](https://github.com/nubot-nudt/td-nerf)|
|**2024-05-10**|**LIVE: LaTex Interactive Visual Editing**|Jinwei Lin et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors introduce LIVE, a novel design method for interactive LaTeX graphic items, enabling the creation of dynamic and interactive components, such as Gitems, to enhance the reading experience and efficiency of academic papers, with open-source code available on GitHub.|[2405.06762v1](http://arxiv.org/abs/2405.06762v1)|null|
|**2024-05-10**|**SketchDream: Sketch-based Text-to-3D Generation and Editing**|Feng-Lin Liu et.al.|The key contributions from the paper's abstract and introduction are:* proposing a text-driven 3D content generation and editing method, SketchDream, which supports NeRF generation from given hand-drawn sketches and achieves free-view sketch-based local editing;* introducing a sketch-based multi-view image generation diffusion model that utilizes depth guidance to establish spatial correspondence and ensures 3D consistency;* proposing a coarse-to-fine editing approach that generates realistic results with refined details by local enhancement;* showcasing the ability of SketchDream to generate high-quality 3D objects under generalized categories and support detailed editing of reconstructed or generated NeRFs.|[2405.06461v2](http://arxiv.org/abs/2405.06461v2)|null|
