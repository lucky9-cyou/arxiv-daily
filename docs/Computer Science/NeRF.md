
### NeRF
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-30**|**$\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving**|Nan Huang et.al.|The paper proposes a self-supervised method, S3Gaussian, for 3D scene reconstruction from street scenes without requiring tracked 3D bounding boxes or costly annotations, enabling efficient decomposition of dynamic and static elements for photo-realistic reconstruction.|[2405.20323v1](http://arxiv.org/abs/2405.20323v1)|[link](https://github.com/nnanhuang/s3gaussian)|
|**2024-05-30**|**TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes**|Minghao Guo et.al.|Here is a summary of the key contributions in a single sentence under 50 words:TetSphere splatting, an explicit Euclidean representation, efficiently reconstructs 3D shapes with high-quality geometry by deforming tetrahedral spheres through a combination of differentiable rendering and geometric energy optimization, outperforming existing representations in terms of mesh quality, computational efficiency, and thin structure preservation.|[2405.20283v1](http://arxiv.org/abs/2405.20283v1)|null|
|**2024-05-30**|**NeRF View Synthesis: Subjective Quality Assessment and Objective Metrics Evaluation**|Pedro Martin et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper presents a comprehensive study on evaluating the quality of Neural Radiance Fields (NeRF) view synthesis methods, introducing a new dataset, and assessing the performance of various objective quality metrics against subjective scores for different scene classes and camera trajectories.|[2405.20078v1](http://arxiv.org/abs/2405.20078v1)|null|
|**2024-05-30**|**IReNe: Instant Recoloring in Neural Radiance Fields**|Alessio Mazzucchelli et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:IReNe introduces a novel approach for instant recoloring of Neural Radiance Fields (NeRFs), addressing the limitations of current methods, which are slow, lack precision at object boundaries, and struggle with multi-view consistency.|[2405.19876v1](http://arxiv.org/abs/2405.19876v1)|null|
|**2024-05-30**|**HINT: Learning Complete Human Neural Representations from Limited Viewpoints**|Alessandro Sanvito et.al.|Here is a summary of the paper's abstract and introduction in one sentence under 50 words:The paper proposes HINT, a NeRF-based algorithm that generates detailed and complete human models from limited viewing angles using symmetry prior, regularization constraints, and large human dataset training, outperforming previous state-of-the-art methods by 15% PSNR.|[2405.19712v1](http://arxiv.org/abs/2405.19712v1)|null|
|**2024-05-30**|**View-Consistent Hierarchical 3D SegmentationUsing Ultrametric Feature Fields**|Haodi He et.al.|Here is a summarized version of the key contributions from the paper's abstract and introduction:**Key Contributions:**1. **Ultrametric Feature Fields**: A novel formulation for 3D scene segmentation that uses ultrametric distances to distill view-inconsistent 2D masks into a 3D representation that is both view-consistent and hierarchical.2. **Hierarchical Segmentation**: A method that produces a hierarchy of segmentations at arbitrary levels of granularity, allowing for view-consistent and hierarchical segmentations.3. **View Consistency**: A metric that measures the 3D consistency of image segmentations, used to evaluate the performance of the proposed method.4. **Segmentation Injectivity**: A metric that measures if each pixel belongs to only one mask for each level of granularity, used to evaluate the quality of the segmentations.5. **View Consistency Score**: A metric that measures the consistency between segmentations in different views, used to evaluate the performance of the proposed method.**Method Overview:**The proposed method takes as input multi-view posed images and view-inconsistent segmentation masks, and produces a 3D point cloud. The method uses an ultrametric feature field to distill the segmentation information into a hierarchical and view-consistent representation. During inference, the method applies a 3D watershed transform to segment the 3D point cloud and produces a hierarchy of segmentations at arbitrary levels of granularity.|[2405.19678v1](http://arxiv.org/abs/2405.19678v1)|null|
|**2024-05-29**|**Neural Radiance Fields for Novel View Synthesis in Monocular Gastroscopy**|Zijie Jiang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The paper proposes a method for novel view synthesis from monocular gastroscopic data using neural radiance fields (NeRF). The key contributions are:1. Incorporating geometry-based loss into the NeRF training process to address the view sparsity issue and improve the quality of novel view synthesis.2. Using a pre-reconstructed point cloud from structure-from-motion (SfM) to constrain the learned geometry and improve the training results.3. Synthesizing high-quality RGB images and depth maps from novel viewpoints not included in the training data.Overall, the paper presents an innovative approach to novel view synthesis from monocular gastroscopic data, which could potentially improve the diagnosis and treatment of stomach-related diseases.|[2405.18863v1](http://arxiv.org/abs/2405.18863v1)|null|
|**2024-05-29**|**NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild**|Weining Ren et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:NeRF On-the-go, a simple yet effective approach, enables robust novel view synthesis in complex, real-world scenes from casually captured images, efficiently eliminating distractors and achieving faster convergence speed, outperforming state-of-the-art techniques in diverse and dynamic real-world applications.|[2405.18715v1](http://arxiv.org/abs/2405.18715v1)|[link](https://github.com/cvg/nerf-on-the-go)|
|**2024-05-28**|**Self-supervised Pre-training for Transferable Multi-modal Perception**|Xiaohao Xu et.al.|The key contributions of the paper's abstract and introduction are:• Proposing a self-supervised pre-training framework, NS-MAE, for multi-modal representation learning for autonomous driving, which can be applied to both multi-modal and single-modal perception models.• Enabling self-supervised learning and optimization unification of transferable multi-modal representation through a plug-and-play design in the spirit of multi-modal reconstruction in Neural Radiance Fields (NeRF).• Verifying the transferability of multi-modal representation learned via NS-MAE on diverse 3D perception tasks with varying amounts of fine-tuning data.Note: The summary is under 50 words.|[2405.17942v1](http://arxiv.org/abs/2405.17942v1)|null|
|**2024-05-28**|**A Refined 3D Gaussian Representation for High-Quality Dynamic Scene Reconstruction**|Bin Zhang et.al.|Based on the paper's abstract and introduction, the key contributions can be summarized as:* A novel framework for high-quality dynamic scene reconstruction, which combines the advantages of NeRF and 3D-GS.* A hybrid representation that reduces memory usage by using deformation fields, hash encoding, and tiny MLPs.* A learnable denoising mask that effectively identifies and removes noise points, enhancing rendering quality.* Static constraints and motion consistency constraints that mitigate motion artifacts and improve rendering accuracy.These contributions enable the framework to achieve efficient and realistic rendering of dynamic scenes, while reducing memory usage and improving rendering quality.|[2405.17891v1](http://arxiv.org/abs/2405.17891v1)|null|
|**2024-05-28**|**HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal High-Frequency Components for Endoscopic Scene Reconstruction**|Haoyu Zhao et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes HFGS, a novel approach for deformable endoscopic reconstruction that addresses under-reconstruction issues in both static and dynamic scenes by incorporating deformation fields, spatial high-frequency emphasis reconstruction, and temporal high-frequency emphasis reconstruction.|[2405.17872v2](http://arxiv.org/abs/2405.17872v2)|null|
|**2024-05-28**|**Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh**|Xiangjun Gao et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a method for manipulating 3D Gaussian Splatting (3DGS) using a triangular mesh, enabling large deformations, local manipulations, and soft body simulations while maintaining high-quality rendering, and outperforms existing methods in terms of PSNR, SSIM, and LPIPS.|[2405.17811v1](http://arxiv.org/abs/2405.17811v1)|null|
|**2024-05-27**|**F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting**|Xiangyu Sun et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes Factorized 3D Gaussian Splatting (F-3DGS), a novel approach that reduces storage requirements while maintaining image quality by factorizing dense clusters of Gaussians, enabling efficient representation and rendering of 3D scenes.|[2405.17083v2](http://arxiv.org/abs/2405.17083v2)|null|
|**2024-05-27**|**PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting**|Zipeng Wang et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper introduces Pyramidal 3D Gaussian Splatting (PyGS), which addresses the challenges of scaling 3D Gaussian Splatting to large-scale scenes by organizing Gaussians in a pyramidal structure and dynamically weighting their contribution for fast and high-fidelity rendering.|[2405.16829v3](http://arxiv.org/abs/2405.16829v3)|null|
|**2024-05-26**|**Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors**|Soumava Paul et.al.|Here is a summary of the key contributions in one sentence under 50 words:The paper proposes SparseSplat360, a method that fine-tunes 2D diffusion models to reconstruct 360 scenes with sparse views, achieving high-quality results by filling in missing details and removing artifacts through a cascade of in-painting and artifact removal models.|[2405.16517v1](http://arxiv.org/abs/2405.16517v1)|null|
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|Here are the key contributions from the paper's abstract and introduction summarized in a single sentence under 50 words:The paper introduces Neural Elevation Models (NEMos), which adapt Neural Radiance Fields to 2.5D continuous terrain models, enabling navigation and path planning on changing terrain using imagery as the input, with a compact representation and differentiable height field.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes High Dynamic Range Gaussian Splatting (HDR-GS), a novel framework for 3D HDR imaging that efficiently renders novel HDR views and reconstructs LDR images with controllable exposure time, outperforming state-of-the-art NeRF-based methods with faster training and inference speed.|[2405.15125v2](http://arxiv.org/abs/2405.15125v2)|[link](https://github.com/caiyuanhao1998/hdr-gs)|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes GS-Hider, a novel steganography framework for 3D Gaussian Splatting (3DGS), which effectively embeds and extracts 3D scenes and images while ensuring security, fidelity, and versatility, maximizing its capacity for hidden messages and flexible usage.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|Here is a summary of the key contributions from the abstract and introduction in a single sentence under 50 words:The paper presents an approach to Neural Radiance Fields (NeRF) that leverages ray tracing to synthesize consistent and photorealistic reflections in real-world scenes, enabling more efficient rendering of specular appearance and nearby content compared to existing methods.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors introduce Neural Directional Encoding (NDE), a novel method that accurately models view-dependent appearance and interreflection effects for rendering specular objects, achieving high-quality results and real-time inference speeds on both synthetic and real-world datasets.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|The paper proposes a two-staged pipeline for camera relocalization by normalizing images with varying lighting conditions, using a hash-encoded NeRF for scene representation, and implementing a re-devised truncated dynamic low-pass filter and numerical gradient averaging technique to optimize camera poses.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes LDM, a novel feed-forward framework generating high-quality textured meshes from single images or text prompts, utilizing a multi-view diffusion model, transformer-based SDF field prediction, and gradient-based mesh optimization for efficient and high-fidelity 3D asset generation.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose JointRF, an end-to-end learning scheme that jointly optimizes dynamic NeRF representation and compression, achieving superior quality and compression efficiency by utilizing a compact residual feature grid and sequential feature compression.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The authors propose DoGaussian, a distributed method for training 3D Gaussian Splatting (3DGS) on large-scale scenes, reducing training time by 6+ times while maintaining rendering performance and achieving state-of-the-art quality.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|The paper proposes Gaussian Time Machine (GTM), a real-time rendering method that models time-dependent attributes of Gaussian primitives using a lightweight MLP, enabling accurate reconstruction of scenes with vastly varying appearances at 80FPS, 100 times faster than NeRF-based methods.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|The paper introduces Motion-Based 3D Clothed Humans Synthesis (MOSS), a novel framework that employs kinematic information to achieve motion-aware Gaussian split on the human surface, improving visual quality in 3D clothed human synthesis from monocular videos by 33.94% and 16.75% over state-of-the-art methods.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper presents a novel method that enables the training of an "off-the-shelf" spacecraft pose estimation network from a sparse set of images of an unknown target, leveraging Neural Radiance Fields (NeRFs) to generate a large and diverse training set that captures the target's appearance under varying illumination conditions.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|Tianqi Liu et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The authors propose MVSGaussian, a generalizable Gaussian Splatting approach that leverages Multi-View Stereo, hybrid Gaussian rendering, and a consistent aggregation strategy to achieve real-time rendering and view synthesis with state-of-the-art performance.|[2405.12218v1](http://arxiv.org/abs/2405.12218v1)|null|
|**2024-05-20**|**Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents**|Guanlin Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper provides a comprehensive overview of integrating neural radiance field (NeRF) and 3D Gaussian splatting (3D-GS) in 6G wireless networks, discussing their applications, implementation challenges, and novel techniques for efficient transmission, reconstruction, and rendering of 3D contents.|[2405.12155v1](http://arxiv.org/abs/2405.12155v1)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|Fotios Logothetis et.al.|The paper presents a novel multi-view photometric stereo (MV-PS) method that leverages per-pixel intensity renderings to improve shape estimation. Key contributions include:* A new approach that explicitly models point light attenuation and casts shadows to better approximate incoming radiance.* A fully neural material renderer that minimizes prior assumptions and is jointly optimized with the surface.* The ability to achieve competitive reconstruction accuracy using only 6 lights, matching the state-of-the-art performance when normal maps are also fused.* A volumetric rendering approach that uses Laplace density function to convert SDF values to transparency and accumulate depth, normals, and rendered intensity along each ray.|[2405.12057v1](http://arxiv.org/abs/2405.12057v1)|null|
