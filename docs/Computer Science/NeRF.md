
### NeRF
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-28**|**Self-supervised Pre-training for Transferable Multi-modal Perception**|Xiaohao Xu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose a self-supervised pre-training framework, NS-MAE, for multi-modal perception in autonomous driving, which learns transferable representations by masked multi-modal reconstruction in neural radiance fields and achieves improved performance in various downstream tasks.|[2405.17942v1](http://arxiv.org/abs/2405.17942v1)|null|
|**2024-05-28**|**A Refined 3D Gaussian Representation for High-Quality Dynamic Scene Reconstruction**|Bin Zhang et.al.|The key contributions mentioned in the abstract and introduction are:* A hybrid representation combining deformation fields, hash encoding, and 3D Gaussian Splatting (3D-GS) to reduce memory usage while achieving efficient and realistic rendering of dynamic scenes.* A learnable denoising mask that can effectively identify and remove noise points in 3D-GS, enhancing the purity and quality of scene rendering.* Static constraints and motion consistency constraints to minimize noise in points during motion, ensuring accurate and efficient rendering of dynamic scenes.These contributions aim to address the limitations of existing methods, such as increased memory usage and noise points, and to improve the rendering quality and speed of dynamic scenes.|[2405.17891v1](http://arxiv.org/abs/2405.17891v1)|null|
|**2024-05-28**|**HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal High-Frequency Components for Endoscopic Scene Reconstruction**|Haoyu Zhao et.al.|The paper proposes HFGS, a novel approach for deformable endoscopic reconstruction that addresses under-reconstruction challenges from spatial and temporal frequency perspectives, leveraging deformation fields and introducing spatial and temporal high-frequency emphasis reconstruction modules to improve rendering quality.|[2405.17872v1](http://arxiv.org/abs/2405.17872v1)|null|
|**2024-05-28**|**Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh**|Xiangjun Gao et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a method for manipulating 3D Gaussian Splatting (3DGS) using a triangular mesh as a proxy, enabling direct transfer of mesh manipulation to 3DGS while maintaining high-quality rendering and high tolerance for mesh accuracy.|[2405.17811v1](http://arxiv.org/abs/2405.17811v1)|null|
|**2024-05-27**|**F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting**|Xiangyu Sun et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes Factorized 3D Gaussian Splatting (F-3DGS), a method that reduces storage requirements for 3D Gaussian Splatting (3DGS) by factorizing coordinates and attributes, enabling efficient representation of dense clusters of Gaussians while preserving image quality.|[2405.17083v2](http://arxiv.org/abs/2405.17083v2)|null|
|**2024-05-27**|**PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting**|Zipeng Wang et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper introduces Pyramidal 3D Gaussian Splatting (PyGS), a hierarchical approach that efficiently represents large-scale scenes by combining pyramidal Gaussians with NeRF initialization, achieving a significant performance leap and rendering time acceleration of over 400 times.|[2405.16829v2](http://arxiv.org/abs/2405.16829v2)|null|
|**2024-05-26**|**Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors**|Soumava Paul et.al.|The paper proposes SparseSplat360, a method that uses pre-trained 2D diffusion models to reconstruct 360-degree scenes from sparse views by filling in missing details and cleaning novel views, achieving state-of-the-art results on the Mip-NeRF360 dataset.|[2405.16517v1](http://arxiv.org/abs/2405.16517v1)|null|
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|The key contributions from the paper's abstract and introduction are:* The introduction of Neural Elevation Models (NEMos), which adapt Neural Radiance Fields to a 2.5D continuous and differentiable terrain model.* A novel method for jointly training a height field and radiance field within a NeRF framework, leveraging quantile regression.* A path planning algorithm that performs gradient-based optimization of a continuous cost function for minimizing distance, slope changes, and control effort.* Experiments on simulated and real-world terrain imagery, demonstrating NEMo's ability to generate high-quality reconstructions and produce smoother paths compared to discrete path planning methods.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The authors propose a new framework, High Dynamic Range Gaussian Splatting (HDR-GS), which efficiently renders novel HDR views and reconstructs LDR images with user input exposure time, achieving 3.84 dB and 1.91 dB improvement on LDR and HDR novel view synthesis respectively while enjoying 1000x inference speed and 6.3% training time reduction.|[2405.15125v2](http://arxiv.org/abs/2405.15125v2)|[link](https://github.com/caiyuanhao1998/hdr-gs)|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a steganography framework, GS-Hider, which embeds multimodal messages, such as 3D scenes and images, into 3D Gaussian Splatting (3DGS) point cloud files, enabling invisible and secure transmission while maintaining high fidelity and rendering speeds.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|Here is a single sentence summarizing the key contributions from the abstract and introduction:The paper presents a ray tracing-based approach to improve Neural Radiance Fields (NeRFs) for rendering highly specular objects, allowing for consistent reflections of nearby and distant content while reducing computational costs and increasing rendering quality.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors introduce Neural Directional Encoding (NDE), a feature-grid-like technique that encodes view-dependent appearance and directional information to accurately model specular objects, addressing challenging interreflection effects, and achieves high-quality rendering and real-time inference.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|The paper proposes a two-staged pipeline for camera relocalization under varying lighting conditions, which normalizes images with hash-encoded NeRFs and uses a re-devised truncated dynamic low-pass filter and numerical gradient averaging technique to optimize pose refinement, achieving state-of-the-art results on several datasets.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes LDM, a novel feed-forward framework that generates high-fidelity, illumination-decoupled textured meshes from a single image or text prompts, utilizing a tensorial SDF representation and a gradient-based mesh optimization layer.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The authors propose JointRF, an end-to-end joint optimization scheme for dynamic NeRF representation and compression, which achieves improved quality and compression efficiency by compactly representing the radiance field and entropy-minimizing its features.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|The paper proposes DoGaussian, a method that trains 3D Gaussian Splatting (3DGS) in a distributed manner to accelerate training on large-scale scenes while achieving state-of-the-art rendering quality.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The proposed Gaussian Time Machine (GTM) models time-dependent attributes of Gaussian primitives using a lightweight MLP, decomposes color into static and dynamic terms, and enables real-time rendering of scenes with vastly varying appearances with high fidelity and geometry consistency.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors introduce MOSS, a novel framework for single-view clothed human reconstruction, which leverages kinematic information to achieve motion-aware Gaussian split and surface deformation detection, resulting in state-of-the-art visual quality and improved realism in 3D representation.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper presents a novel method that enables the training of an "off-the-shelf" spacecraft pose estimation network from a sparse set of images using a Neural Radiance Field (NeRF) model, demonstrating the feasibility of autonomous rendezvous and proximity operations in space debris removal missions.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|Tianqi Liu et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The authors propose MVSGaussian, a generalizable Gaussian Splatting method derived from Multi-View Stereo, which efficiently reconstructs unseen scenes with real-time rendering and better synthesis quality, and introduces a multi-view geometric consistent aggregation strategy for fast per-scene optimization.|[2405.12218v1](http://arxiv.org/abs/2405.12218v1)|null|
|**2024-05-20**|**Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents**|Guanlin Wu et.al.|Here is a summary of the key contributions in the abstract and introduction:The paper explores the integration of neural radiance field (NeRF) and 3D Gaussian splatting (3D-GS) in 6G networks for efficient representation, transmission, and reconstruction of 3D contents.|[2405.12155v1](http://arxiv.org/abs/2405.12155v1)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|Fotios Logothetis et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper presents a novel neural multi-view photometric stereo method that explicitly models point light attenuation, casts shadows, and optimizes a fully neural material renderer, achieving competitive reconstruction accuracy using only 6 lights and outperforming state-of-the-art methods.|[2405.12057v1](http://arxiv.org/abs/2405.12057v1)|null|
|**2024-05-19**|**Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems**|Shengxiang Sun et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a method to generate realistic-looking adversarial objects for autonomous driving systems by introducing a 'Judge' mechanism that evaluates the realism of generated textures and integrates it into the loss function, enabling the learning of objects that are both adversarial and realistic.|[2405.11629v1](http://arxiv.org/abs/2405.11629v1)|null|
|**2024-05-19**|**R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless Environments**|Huiying Yang et.al.|Here is a summary of the key contributions in under 50 words:The paper proposes a novel Neural Radiance Field (NeRF) based ray tracing method to model dynamic electromagnetic fields in Reconfigurable Intelligent Surface (RIS)-enabled wireless environments. The method captures complex propagation dynamics and accurately represents signal emission and transmission.|[2405.11541v1](http://arxiv.org/abs/2405.11541v1)|null|
|**2024-05-18**|**MotionGS : Compact Gaussian Splatting SLAM by Motion Filter**|Xinli Guo et.al.|The paper proposes a novel 3DGS-based SLAM approach, MotionGS, which integrates deep feature extraction, dual keyframe selection, and 3DGS, and achieves high-fidelity scene representation, real-time tracking, and accurate mapping with reduced memory usage.|[2405.11129v1](http://arxiv.org/abs/2405.11129v1)|[link](https://github.com/antonio521/motiongs)|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper provides a comprehensive overview of methodologies enabling Large Language Models (LLMs) to process, understand, and generate 3D data, highlighting their potential to advance spatial comprehension and interaction within embodied Artificial Intelligence (AI) systems.|[2405.10255v1](http://arxiv.org/abs/2405.10255v1)|null|
|**2024-05-15**|**From NeRFs to Gaussian Splats, and Back**|Siming He et.al.|The key contributions from the paper's abstract and introduction are:* The authors develop a procedure to convert between implicit representations of scenes, such as Neural Radiance Fields (NeRFs), and explicit representations, such as Gaussian Splatting (GS).* This conversion enables the best of both worlds: the generalization of NeRFs to novel views and the real-time rendering and modification capabilities of GS.* The authors show that their approach achieves better PSNR, SSIM, and LPIPS metrics on dissimilar views compared to traditional methods, and is particularly useful for robotics applications where scene understanding and localization are crucial.In essence, the paper proposes a new technique for converting between NeRFs and GS, which allows for the advantages of each approach to be combined, leading to improved performance in scene understanding and localization tasks.|[2405.09717v1](http://arxiv.org/abs/2405.09717v1)|[link](https://github.com/grasp-lyrl/nerftogsandback)|
|**2024-05-14**|**Dynamic NeRF: A Review**|Jinwei Lin et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper provides a comprehensive review of the development and implementation principles of Dynamic Neural Radiance Field (NeRF), analyzing its history, key methods, and features, with a focus on its potential applications in 3D modeling, representation, and reconstruction.|[2405.08609v1](http://arxiv.org/abs/2405.08609v1)|null|
|**2024-05-13**|**Synergistic Integration of Coordinate Network and Tensorial Feature for Improving Neural Radiance Fields from Sparse Inputs**|Mingyu Kim et.al.|The key contributions from the paper's abstract and introduction are:* The proposed method, synergistically integrating a coordinate-based network with a multi-plane representation, achieves comparable results to explicit encoding with fewer parameters and outperforms others in dynamic NeRFs with sparse inputs.* The coordinate-based network captures low-frequency details, while the multi-plane representation focuses on capturing fine-grained details.* The proposed method reduces the number of parameters by skipping the allocation of a spatial low-resolution grid and instead replacing it with coordinate-based features.(Note: The response is limited to 50 words.)|[2405.07857v1](http://arxiv.org/abs/2405.07857v1)|[link](https://github.com/mingyukim87/synergynerf)|
|**2024-05-12**|**Point Resampling and Ray Transformation Aid to Editable NeRF Models**|Zhenyang Li et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes an implicit ray transformation strategy and a differentiable neural-point resampling module to enable point-based editable NeRF pipeline for 3D object removal and inpainting tasks, achieving state-of-the-art performance and high-quality rendering visualization.|[2405.07306v1](http://arxiv.org/abs/2405.07306v1)|null|
