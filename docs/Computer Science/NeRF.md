
### NeRF
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-30**|**$\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving**|Nan Huang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose a self-supervised method, S3Gaussian, to decompose dynamic and static 3D scenes without explicit annotations, achieving state-of-the-art rendering quality on novel view synthesis and scene reconstruction tasks using 4D consistency and spatial-temporal field networks.|[2405.20323v1](http://arxiv.org/abs/2405.20323v1)|[link](https://github.com/nnanhuang/s3gaussian)|
|**2024-05-30**|**TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes**|Minghao Guo et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper introduces TetSphere splatting, an explicit, Lagrangian geometry representation that reconstructs high-quality 3D shapes using deformed tetrahedral spheres, offering faster optimization, enhanced mesh quality, and reliable preservation of thin structures.|[2405.20283v1](http://arxiv.org/abs/2405.20283v1)|null|
|**2024-05-30**|**NeRF View Synthesis: Subjective Quality Assessment and Objective Metrics Evaluation**|Pedro Martin et.al.|Here are the key contributions from the paper's abstract and introduction:* Creation of a new set of front-facing (FF) synthetic and real visual scenes with camera poses that can be used to assess NeRF methods.* Evaluation of the impact of NeRF view synthesis (NVS) on perceived quality using a well-known and reliable subjective assessment methodology, considering several scene classes and recently proposed NVS methods.* Evaluation of objective quality assessment metrics developed for 2D images and video, using several scene classes, including real and synthetic 360ยบ and FF scenes.* Providing a comparative evaluation of several NeRF methods and objective quality metrics, across different classes of visual scenes.|[2405.20078v1](http://arxiv.org/abs/2405.20078v1)|null|
|**2024-05-30**|**IReNe: Instant Recoloring in Neural Radiance Fields**|Alessio Mazzucchelli et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:IReNe is a novel approach that enables swift, near real-time color editing in Neural Radiance Fields (NeRFs) by leveraging a pre-trained NeRF model, a trainable segmentation module, and automated classification of neurons for efficient fine-tuning and consistent edits.|[2405.19876v1](http://arxiv.org/abs/2405.19876v1)|null|
|**2024-05-30**|**HINT: Learning Complete Human Neural Representations from Limited Viewpoints**|Alessandro Sanvito et.al.|Here is a summarized key contribution in a single sentence under 50 words:The paper proposes HINT, a NeRF-based algorithm that learns a detailed and complete human model from limited viewing angles by introducing a symmetry prior, regularization constraints, and training cues from large human datasets.|[2405.19712v1](http://arxiv.org/abs/2405.19712v1)|null|
|**2024-05-30**|**View-Consistent Hierarchical 3D SegmentationUsing Ultrametric Feature Fields**|Haodi He et.al.|Here are the key contributions from the paper's abstract and introduction, summarized in a single sentence under 50 words:The paper presents a novel method for lifting multi-granular and view-inconsistent image segmentations into a hierarchical and 3D-consistent representation using ultrametric feature fields within a Neural Radiance Field (NeRF) framework.|[2405.19678v1](http://arxiv.org/abs/2405.19678v1)|null|
|**2024-05-29**|**Neural Radiance Fields for Novel View Synthesis in Monocular Gastroscopy**|Zijie Jiang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a novel method for synthesizing photo-realistic images from monocular gastroscopic data using neural radiance fields (NeRF), incorporating geometry-based loss to address view sparsity and improve rendering quality.|[2405.18863v1](http://arxiv.org/abs/2405.18863v1)|null|
|**2024-05-29**|**NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild**|Weining Ren et.al.|Here is a summary of the key contributions in a single sentence under 50 words:NeRF On-the-go introduces a novel approach that enables robust novel view synthesis in complex, dynamic scenes by efficiently eliminating distractors and achieving faster convergence, outperforming state-of-the-art methods through comprehensive experiments.|[2405.18715v1](http://arxiv.org/abs/2405.18715v1)|[link](https://github.com/cvg/nerf-on-the-go)|
|**2024-05-28**|**Self-supervised Pre-training for Transferable Multi-modal Perception**|Xiaohao Xu et.al.|Here is a summary of the key contributions mentioned in the abstract and introduction:The authors propose a self-supervised pre-training framework for multi-modal representation learning, called NS-MAE, which is designed to provide pre-trained model initializations for efficient and high-performance fine-tuning.|[2405.17942v1](http://arxiv.org/abs/2405.17942v1)|null|
|**2024-05-28**|**A Refined 3D Gaussian Representation for High-Quality Dynamic Scene Reconstruction**|Bin Zhang et.al.|Here are the key contributions from the paper's abstract and introduction:**Key Contributions:**1. A hybrid representation combining deformation fields, hash encoding, and 3D Gaussian Splatting to achieve dynamic scene rendering with reduced memory usage.2. A learnable denoising mask to effectively eliminate noise points from the scene and enhance rendering quality.3. Static constraints and motion consistency constraints to minimize noise in points during motion and ensure accurate and efficient rendering of dynamic scenes.These contributions aim to address the challenges of rendering dynamic scenes, improving rendering quality, and reducing storage consumption, making the method suitable for various tasks such as novel view synthesis and dynamic mapping.|[2405.17891v1](http://arxiv.org/abs/2405.17891v1)|null|
|**2024-05-28**|**HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal High-Frequency Components for Endoscopic Scene Reconstruction**|Haoyu Zhao et.al.|The paper proposes HFGS, a novel approach for deformable endoscopic reconstruction that addresses under-reconstruction in both static and dynamic scenes from spatial and temporal frequency perspectives, achieving superior rendering quality and improving dynamic awareness in neural rendering.|[2405.17872v2](http://arxiv.org/abs/2405.17872v2)|null|
|**2024-05-28**|**Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh**|Xiangjun Gao et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a method to manipulate 3D Gaussian Splatting (3DGS) using a triangular mesh, achieving large deformation, local manipulation, and soft body simulation while maintaining high-quality rendering, with a high tolerance for mesh accuracy.|[2405.17811v1](http://arxiv.org/abs/2405.17811v1)|null|
|**2024-05-27**|**F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting**|Xiangyu Sun et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper proposes Factorized 3D Gaussian Splatting (F-3DGS), a novel approach that significantly reduces storage requirements by approximating dense clusters of Gaussians with fewer Gaussians through efficient factorization, while maintaining comparable quality in rendered images.|[2405.17083v2](http://arxiv.org/abs/2405.17083v2)|null|
|**2024-05-27**|**PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting**|Zipeng Wang et.al.|The paper presents Pyramidal 3D Gaussian Splatting (PyGS) which combines 3D Gaussian Splatting with NeRF initialization to achieve high-fidelity visual results and accelerated rendering performance in large-scale scenes, addressing challenges such as scale integration and viewpoint changes.|[2405.16829v3](http://arxiv.org/abs/2405.16829v3)|null|
|**2024-05-26**|**Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors**|Soumava Paul et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper presents SparseSplat360, a method that combines latent diffusion models and 3D Gaussians to reconstruct 360-degree scenes from sparse views, achieving improved performance and multi-view consistency through a novel distillation algorithm.|[2405.16517v1](http://arxiv.org/abs/2405.16517v1)|null|
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors introduce Neural Elevation Models (NEMos), which adapt Neural Radiance Fields to a 2.5D terrain model, combining the benefits of NeRFs' fast generation and rich visual reconstruction with a continuous and differentiable height field for path planning.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper presents HDR-GS, a novel framework for high dynamic range novel view synthesis that efficiently renders HDR views and reconstructs LDR images with controllable exposure time, outperforming state-of-the-art methods while enjoying significant inference speed and training time improvements.|[2405.15125v2](http://arxiv.org/abs/2405.15125v2)|[link](https://github.com/caiyuanhao1998/hdr-gs)|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|Here is the summary of the key contributions in a single sentence under 50 words:The paper proposes GS-Hider, a steganography framework for 3D Gaussian Splatting (3DGS) that embeds 3D scenes and images into original 3DGS point clouds, ensuring security, fidelity, and flexibility, with large capacity and strong versatility.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|The paper proposes a ray tracing-based approach to improve Neural Radiance Fields (NeRFs) for rendering highly specular objects with detailed view-dependent appearance, addressing limitations of prior methods that struggle to render consistent reflections of nearby scene content and rely on computationally expensive neural networks.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents Neural Directional Encoding (NDE), a feature-grid-like encoding for neural radiance fields (NeRF) that accurately models view-dependent effects, including specular high-lights and glossy interreflections, and achieves fast evaluation, outperforming the state-of-the-art on view synthesis of specular objects.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a two-staged pipeline that normalizes images with varying lighting conditions for camera relocalization, using a hash-encoded NeRF and a re-devised truncated dynamic low-pass filter and numerical gradient averaging technique to improve pose optimization.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes LDM, a novel feed-forward framework that generates high-quality 3D meshes with illumination-decoupled textures from text or single images in seconds, using a tensorial SDF representation and decoupled color field, and introduces an adaptive conversion strategy to enhance convergence.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes JointRF, an end-to-end joint optimization scheme for dynamic NeRF representation and compression, achieving significantly improved quality and compression efficiency to support streamable dynamic and long-sequence radiance fields.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The authors propose DoGaussian, a distributed training method for 3D Gaussian Splatting (3DGS) that splits scenes into blocks, uses Alternating Direction Method of Multipliers (ADMM) for consistency, and achieves 6+ times faster training time with state-of-the-art rendering quality.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|Here is a summary of the key contributions in under 50 words:The paper proposes Gaussian Time Machine (GTM), a real-time rendering method that models time-dependent attributes of Gaussian primitives for reconstructing 3D scenes with varying appearances, achieving state-of-the-art rendering fidelity and speed, while disentangling appearance changes and rendering smooth appearance interpolation.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|Here is a summary of the paper's key contributions in a single sentence under 50 words:The authors introduce MOSS, a novel framework that employs kinematic information to achieve motion-aware Gaussian split on the human surface, restoring realistic joint details and fine clothing folds in large-scale motion, with state-of-the-art visual quality and real-time rendering.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The paper presents a novel method that enables an "off-the-shelf" spacecraft pose estimator to be applied on an unknown target by leveraging Neural Radiance Fields (NeRFs) to generate a training set that captures the diversity of both pose distribution and illumination conditions. The method first acquires a sparse set of images of the target, then processes them on-ground to synthesize additional views and train a NeRF, which is used to generate a large training set. Finally, an off-the-shelf spacecraft pose estimation network is trained on this set. The paper also discusses the limitations of existing model-based and model-agnostic spacecraft pose estimation methods, which assume prior knowledge of the target's CAD model, and presents its own method as a solution to estimate the pose of an unknown target observed from a sparse set of viewpoints.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|Tianqi Liu et.al.|Here is a summarized key contribution in a single sentence under 50 words:MVSGaussian, a new generalizable 3D Gaussian representation approach derived from Multi-View Stereo, efficiently reconstructs unseen scenes with real-time rendering, better synthesis quality, and fast per-scene optimization, outperforming existing methods with similar rendering speed and reduced training computational cost.|[2405.12218v1](http://arxiv.org/abs/2405.12218v1)|null|
|**2024-05-20**|**Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents**|Guanlin Wu et.al.|Here are the main contributions from the paper's abstract and introduction:* The paper discusses the integration of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3D-GS) in 6G wireless networks for immersive communication applications, which require the efficient representation, transmission, and reconstruction of 3D contents.* The authors highlight the benefits of NeRF and 3D-GS, including photorealistic rendering, high visual quality, and reduced computation resources.* They also identify the challenges of integrating NeRF and 3D-GS in 6G wireless networks, including the need for new distributed training and inference methods, joint computation and communication designs, and efficient transmission of radiance field models.* The paper proposes solutions to address these challenges, including over-the-air training of NeRF and 3D-GS models, model compression approaches, rendering acceleration techniques, and a new semantic communication enabled 3D content transmission design.|[2405.12155v1](http://arxiv.org/abs/2405.12155v1)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|Fotios Logothetis et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:* The paper presents a novel multi-view photometric stereo (PS) method that leverages neural shape representations and learned renderers.* Unlike state-of-the-art multi-view PS methods, this approach explicitly models point light attenuation and raytraces cast shadows to approximate incoming radiance.* The method optimizes a fully neural material renderer and achieves competitive reconstruction accuracy using only 6 lights, matching the performance of state-of-the-art methods when using all lights and normal maps.Note that the paper is quite detailed and technical, and this summary focuses on the main contributions mentioned in the abstract and introduction.|[2405.12057v1](http://arxiv.org/abs/2405.12057v1)|null|
