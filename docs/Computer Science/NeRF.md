
### NeRF
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-30**|**$\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving**|Nan Huang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose a self-supervised 3D Gaussian Splatting method, S3Gaussian, that decomposes dynamic and static elements from 4D consistency without requiring 3D annotations, achieving state-of-the-art rendering quality for novel view synthesis and scene reconstruction tasks.|[2405.20323v1](http://arxiv.org/abs/2405.20323v1)|[link](https://github.com/nnanhuang/s3gaussian)|
|**2024-05-30**|**TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes**|Minghao Guo et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The authors propose TetSphere splatting, an explicit, Lagrangian representation for reconstructing 3D shapes, which directly yields superior mesh quality without relying on neural networks or post-processing, and exhibits significant computational efficiency.|[2405.20283v1](http://arxiv.org/abs/2405.20283v1)|null|
|**2024-05-30**|**NeRF View Synthesis: Subjective Quality Assessment and Objective Metrics Evaluation**|Pedro Martin et.al.|The key contributions of the paper are:1. Creation of a new set of front-facing (FF) synthetic and real visual scenes with corresponding camera poses, which can be used to assess Neural Radiance Fields (NeRF) methods.2. Evaluation of the impact of NeRF view synthesis (NVS) on perceived quality using a well-known and reliable subjective assessment methodology, considering several scene classes and recently proposed NVS methods.3. Evaluation of objective quality assessment metrics developed for 2D images and video in the context of NeRF view synthesis, considering several scene classes, including real and synthetic content for front-face and 360-degree camera trajectories.|[2405.20078v1](http://arxiv.org/abs/2405.20078v1)|null|
|**2024-05-30**|**IReNe: Instant Recoloring in Neural Radiance Fields**|Alessio Mazzucchelli et.al.|Here is a summary of the key contributions in 49 words or less:IReNe introduces a novel approach for instant recoloring of Neural Radiance Fields (NeRF), addressing limitations of existing methods with a pre-trained NeRF, trainable segmentation module, and selective fine-tuning of diffuse neurons, leading to swift, near real-time color editing with improved object boundary control and multi-view consistency.|[2405.19876v1](http://arxiv.org/abs/2405.19876v1)|null|
|**2024-05-30**|**HINT: Learning Complete Human Neural Representations from Limited Viewpoints**|Alessandro Sanvito et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes HINT, a NeRF-based algorithm that generates detailed and complete human models from limited viewing angles by introducing symmetry prior, regularization constraints, and large human datasets, achieving improved performance by 15% PSNR compared to previous state-of-the-art algorithms.|[2405.19712v1](http://arxiv.org/abs/2405.19712v1)|null|
|**2024-05-30**|**View-Consistent Hierarchical 3D SegmentationUsing Ultrametric Feature Fields**|Haodi He et.al.|Based on the abstract and introduction, the key contributions of the paper are:* A novel formulation for 3D scene segmentation using ultrametric feature fields* A method to lift multi-granular and view-inconsistent 2D segmentations into a hierarchical and 3D-consistent representation* A dataset with hierarchical segmentation annotations based on the NeRF Blender Dataset* Evaluation metrics, including Normalized Covering Score, Segmentation Injectivity Score, and View Consistency ScoreThe paper aims to address the challenge of lifting multi-granular and view-inconsistent image segmentations into a hierarchical and 3D-consistent representation, which is a necessary step for many computer vision tasks, such as scene understanding, object recognition, and robotics.The paper introduces a novel approach using ultrametric feature fields, which is able to capture hierarchical and 3D-consistent segmentations, and evaluates its performance on a range of metrics, including intersection over union (IoU), segmentation injectivity, and view consistency. The paper also provides supplementary materials, including additional implementation details, evaluation metrics, and baselines.|[2405.19678v1](http://arxiv.org/abs/2405.19678v1)|null|
|**2024-05-29**|**Neural Radiance Fields for Novel View Synthesis in Monocular Gastroscopy**|Zijie Jiang et.al.|The key contributions from the paper's abstract and introduction are:* The authors propose a novel method to synthesize photo-realistic images for novel viewpoints within the stomach from pre-captured monocular gastroscopic images.* The method uses neural radiance fields (NeRF) to learn an implicit and continuous representation of the scene appearance and geometry, and incorporates geometry priors from a pre-reconstructed point cloud to constrain the learned geometry.* The authors evaluate their method with two datasets and compare it to two other NeRF methods, achieving consistently superior results in synthesizing high-quality RGB images for novel viewpoints not included in the training.* The method is capable of enabling the practitioners to freely adjust the viewing trajectory to obtain the best observations inside the stomach.|[2405.18863v1](http://arxiv.org/abs/2405.18863v1)|null|
|**2024-05-29**|**NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild**|Weining Ren et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper introduces NeRF On-the-go, a simple yet effective approach that enables robust novel view synthesis in complex, real-world scenes from casually captured images, efficiently eliminating distractors and achieving a faster convergence speed compared to state-of-the-art techniques.|[2405.18715v1](http://arxiv.org/abs/2405.18715v1)|[link](https://github.com/cvg/nerf-on-the-go)|
|**2024-05-28**|**Self-supervised Pre-training for Transferable Multi-modal Perception**|Xiaohao Xu et.al.|Here is a summary of the key contributions in the abstract and introduction in a single sentence under 50 words:The proposed NeRF-Supervised Masked Auto Encoder (NS-MAE) pre-training framework enables transferable multi-modal representation learning for autonomous driving, unifying self-supervised learning and optimization with neural radiance fields, and achieving robustness in degraded environments.|[2405.17942v1](http://arxiv.org/abs/2405.17942v1)|null|
|**2024-05-28**|**A Refined 3D Gaussian Representation for High-Quality Dynamic Scene Reconstruction**|Bin Zhang et.al.|The key contributions of the paper are:1. **Hybrid Representation**: A combination of deformation fields, hash encoding, and 3D-GS (Gaussian Splatting) reduces memory usage while achieving efficient and realistic rendering of dynamic scenes.2. **Denoising Mask**: A learnable denoising mask eliminates noise points in the scene, further compressing 3D-GS.3. **Static Constraints and Motion Consistency Constraints**: Minimizing the impact of the deformation field on static points and distinguishing between dynamic and static points enhances rendering quality and stability.Please let me know if you need any further assistance.|[2405.17891v1](http://arxiv.org/abs/2405.17891v1)|null|
|**2024-05-28**|**HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal High-Frequency Components for Endoscopic Scene Reconstruction**|Haoyu Zhao et.al.|The paper proposes a novel approach, HFGS, for deformable endoscopic reconstruction that addresses under-reconstruction and dynamic awareness, incorporating deformation fields, spatial and temporal high-frequency emphasis reconstruction, and leveraging flow priors to minimize discrepancies in frequency spectra and optical flow predictions.|[2405.17872v2](http://arxiv.org/abs/2405.17872v2)|null|
|**2024-05-28**|**Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh**|Xiangjun Gao et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a method for manipulating 3D Gaussian Splatting (3DGS) using a triangular mesh, enabling large deformations, local manipulations, and soft body simulations while maintaining high-quality rendering, with a high tolerance for mesh accuracy.|[2405.17811v1](http://arxiv.org/abs/2405.17811v1)|null|
|**2024-05-27**|**F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting**|Xiangyu Sun et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The authors propose Factorized 3D Gaussian Splatting (F-3DGS), a novel approach that significantly reduces the storage requirements of 3D Gaussian Splatting while maintaining image quality, by factorizing dense clusters of Gaussians into smaller numbers of Gaussians, achieving a 90% reduction in storage usage compared to traditional 3DGS.|[2405.17083v2](http://arxiv.org/abs/2405.17083v2)|null|
|**2024-05-27**|**PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting**|Zipeng Wang et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper presents Pyramidal 3D Gaussian Splatting (PyGS), which addresses the challenges of scaling 3D Gaussian Splatting to large-scale scenes by introducing a hierarchical pyramidal structure and dynamic weighting, achieving a significant performance leap and rendering time reduction.|[2405.16829v3](http://arxiv.org/abs/2405.16829v3)|null|
|**2024-05-26**|**Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors**|Soumava Paul et.al.|The paper presents SparseSplat360, a method that uses a cascade of diffusion models to improve sparse-view reconstruction of 360° scenes, achieving high-quality results from as few as 9 input views.|[2405.16517v1](http://arxiv.org/abs/2405.16517v1)|null|
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|The key contributions of the paper are:* Introducing Neural Elevation Models (NEMos), which adapt Neural Radiance Fields to a 2.5D continuous and differentiable terrain model.* Proposing a novel method for jointly training a height field and radiance field within a NeRF framework, leveraging quantile regression.* Introducing a path planning algorithm that performs gradient-based optimization of a continuous cost function for minimizing distance, slope changes, and control effort, enabled by the differentiability of the height field.The paper aims to address the limitations of traditional terrain representations, such as digital elevation models, which are expensive to generate and provide limited information. The proposed NEMo approach leverages the strengths of NeRFs in capturing complex terrain detail and the suitability of DEMs for path planning, enabling the creation of more robust and versatile navigation solutions.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|The paper proposes High Dynamic Range Gaussian Splatting (HDR-GS), a novel framework for 3D HDR imaging that efficiently renders novel HDR views and reconstructs LDR images with a user input exposure time, offering a 3.84 dB and 1.91 dB improvement over the state-of-the-art NeRF-based method on LDR and HDR novel view synthesis, respectively.|[2405.15125v2](http://arxiv.org/abs/2405.15125v2)|[link](https://github.com/caiyuanhao1998/hdr-gs)|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:This paper proposes a 3D Gaussian Splatting (3DGS) steganography framework, GS-Hider, which embeds 3D scenes and images into original GS point clouds, ensuring secrecy, fidelity, and versatility, with a coupled secured feature attribute, parallel decoders, and high performance.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper's approach, based on ray tracing, addresses the limitations of Neural Radiance Fields in rendering highly specular objects by casting reflection rays into the scene geometry, synthesizing consistent reflections, and reducing computation requirements.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|The key contributions of the paper are the introduction of Neural Directional Encoding (NDE), a view-dependent appearance encoding of NeRF for rendering specular objects, and its application to a feature-grid-like neural encoding that accurately models the appearance of shiny objects.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes a two-stage pipeline that normalizes images with varying lighting and shadow conditions for camera relocalization, using a hash-encoded NeRF and a re-devised truncated dynamic low-pass filter and numerical gradient averaging technique to achieve state-of-the-art results.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose LDM, a novel feed-forward framework generating high-fidelity, illumination-decoupled textured meshes from a single image or text prompts, utilizing tensorial SDF representation, adaptive conversion to density, and gradient-based mesh optimization for high-quality output.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose JointRF, an end-to-end joint optimization scheme for dynamic Neural Radiance Field representation and compression, achieving improved quality and compression efficiency, and outperforming state-of-the-art methods in terms of rate-distortion performance.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:This paper proposes DoGaussian, a method that trains 3D Gaussian Splatting (3DGS) distributedly, reducing training time by 6+ times for large-scale scenes while achieving state-of-the-art rendering quality, by splitting scenes into blocks and using ADMM for distributed training with global 3D Gaussian consensus.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|The paper proposes Gaussian Time Machine (GTM), a real-time rendering method that models time-dependent attributes of Gaussian primitives with discrete time embedding vectors, allowing for efficient reconstruction of 3D scenes with vastly varying appearances.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The authors introduce MOSS, a novel framework that addresses the challenges of single-view clothed human reconstruction by incorporating kinematic information to achieve motion-aware Gaussian splitting and surface deformation detection, resulting in state-of-the-art visual quality and efficient rendering.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|The key contributions of the paper are:1. A novel method to estimate the 6D pose of an unknown target spacecraft relative to a monocular camera, without prior knowledge of the target's CAD model.2. A three-step approach for spacecraft pose estimation, comprising image acquisition, on-ground processing, and autonomous pose estimation.3. The use of a Neural Radiance Field (NeRF) to generate high-quality images with varying illumination conditions, enhancing the training set for off-the-shelf spacecraft pose estimation models.4. Successful validation of the method on Hardware-In-the-Loop (HIL) images from the SPEED+ dataset, achieving performance comparable to models trained on synthetic images.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|Tianqi Liu et.al.|Here is a summary of the key contributions in a single sentence under 50 words:MVSGaussian, a novel generalizable 3D Gaussian representation approach, leverages Multi-View Stereo to efficiently reconstruct unseen scenes with real-time rendering, better synthesis quality, and reduced training computational cost compared to existing methods.|[2405.12218v1](http://arxiv.org/abs/2405.12218v1)|null|
|**2024-05-20**|**Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents**|Guanlin Wu et.al.|The key contributions of the paper's abstract and introduction are:* The paper reviews the integration of neural radiance field (NeRF) and 3D Gaussian splatting (3D-GS) in 6G wireless networks to support emerging 3D applications with enhanced quality of experience.* The authors highlight the importance of efficient representation, transmission, and reconstruction of 3D contents in 6G networks, and discuss the potential of NeRF and 3D-GS to provide photorealistic rendering results for complex scenes.* The paper presents a comprehensive overview of the integration of NeRF and 3D-GS in 6G networks, including the review of radiance field rendering techniques, the consideration of over-the-air training of NeRF and 3D-GS models, and the discussion of rendering architectures and model compression approaches.* The paper also discusses the challenges of integrating NeRF and 3D-GS in 6G networks, including the need for distributed training and inference methods, joint computation and communication designs, and the importance of minimizing end-to-end latency while preserving quality of experience.|[2405.12155v1](http://arxiv.org/abs/2405.12155v1)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|Fotios Logothetis et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents a novel multi-view photometric stereo method that leverages per-pixel intensity renderings, point light attenuation, and explicit raytracing of cast shadows, achieving competitive reconstruction accuracy using only 6 lights and outperforming existing methods on the DiLiGenT-MV benchmark.|[2405.12057v1](http://arxiv.org/abs/2405.12057v1)|null|
