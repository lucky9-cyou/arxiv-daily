
### NeRF
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-28**|**Self-supervised Pre-training for Transferable Multi-modal Perception**|Xiaohao Xu et.al.|The key contributions of the paper are: a self-supervised pre-training framework for multi-modal representation learning for autonomous driving, denoted as NS-MAE, which is designed to provide pre-trained model initializations for efficient and high-performance fine-tuning; a unified self-supervised multi-modal perception pre-training framework that learns transferable representations; and a physics-informed unknown-region filter that decouples objects along the height axis.|[2405.17942v1](http://arxiv.org/abs/2405.17942v1)|null|
|**2024-05-28**|**A Refined 3D Gaussian Representation for High-Quality Dynamic Scene Reconstruction**|Bin Zhang et.al.|The key contributions from the paper's abstract and introduction are:* A hybrid representation combining deformation fields, hash encoding, and 3D Gaussian Splatting (3D-GS) to reduce memory usage and achieve efficient and realistic rendering of dynamic scenes.* A learnable denoising mask to filter out noise points from the scene, enhancing rendering quality.* Static constraints and motion consistency constraints to minimize noise in points during motion, ensuring accurate and efficient rendering of dynamic scenes.The paper's main contributions can be summarized as:* Developing a novel framework that leverages deformation fields, hash encoding, and 3D-GS to render dynamic scenes with reduced memory usage and improved quality.* Introducing a learnable denoising mask to effectively eliminate noise points and enhance rendering quality.* Proposing static constraints and motion consistency constraints to mitigate noise in points during motion and ensure accurate and efficient rendering of dynamic scenes.|[2405.17891v1](http://arxiv.org/abs/2405.17891v1)|null|
|**2024-05-28**|**HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal High-Frequency Components for Endoscopic Scene Reconstruction**|Haoyu Zhao et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The authors propose HFGS, a novel approach for deformable endoscopic reconstruction that addresses under-reconstruction challenges from spatial and temporal frequency perspectives, enhancing dynamic awareness and rendering quality through Spatial and Temporal High-Frequency Emphasis Reconstruction modules.|[2405.17872v1](http://arxiv.org/abs/2405.17872v1)|null|
|**2024-05-28**|**Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh**|Xiangjun Gao et.al.|The key contributions of the paper are:* Proposing a method to manipulate 3D Gaussian Splatting (3DGS) using a triangular mesh as the proxy, which allows for direct transfer of mesh manipulation to 3DGS with self-adaptation and maintaining high-quality rendering.* Introducing a triangle shape-aware Gaussian binding strategy with self-adaptation, which has a high tolerance for mesh accuracy and supports various 3DGS manipulations.|[2405.17811v1](http://arxiv.org/abs/2405.17811v1)|null|
|**2024-05-27**|**F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting**|Xiangyu Sun et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes Factorized 3D Gaussian Splatting (F-3DGS), a novel approach that reduces storage requirements and maintains high-quality image rendering through efficient factorization of 3D Gaussians, enabling fast rendering and low storage consumption.|[2405.17083v2](http://arxiv.org/abs/2405.17083v2)|null|
|**2024-05-27**|**PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting**|Zipeng Wang et.al.|The paper presents Pyramidal 3D Gaussian Splatting (PyGS), which addresses challenges in scaling 3D Gaussian Splatting to large-scale scenes by introducing a hierarchical assembly of Gaussians, initialized through a rapidly trained grid-based NeRF, and dynamically weighted using a compact gating network.|[2405.16829v2](http://arxiv.org/abs/2405.16829v2)|null|
|**2024-05-26**|**Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors**|Soumava Paul et.al.|The paper proposes SparseSplat360, a method that leverages pre-trained 2D diffusion models to improve 360 3D scene reconstruction from sparse views by employing a cascade of in-painting and artifact removal models, achieving high-quality results with minimal fine-tuning.|[2405.16517v1](http://arxiv.org/abs/2405.16517v1)|null|
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:* Neural Elevation Models (NEMos) adapt Neural Radiance Fields to a 2.5D continuous and differentiable terrain model, allowing for novel applications in terrain modeling and path planning.* NEMos jointly train a height field and radiance field within a NeRF framework, leveraging quantile regression to learn the height information from images.* The paper presents a novel approach to continuous terrain elevation representation, allowing for direct generation from 2D camera images without additional depth data.* The authors demonstrate the ability to generate high-quality reconstructions and plan smoother paths compared to discrete path planning methods.Note: The reply is under 50 words.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose High Dynamic Range Gaussian Splatting (HDR-GS), a novel framework that efficiently renders novel HDR views and reconstructs LDR images with controllable exposure time, outperforming state-of-the-art methods while enjoying 1000x faster inference speed and 6.3% training time.|[2405.15125v2](http://arxiv.org/abs/2405.15125v2)|[link](https://github.com/caiyuanhao1998/hdr-gs)|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose a steganography framework, GS-Hider, to embed 3D scenes and images into original 3D Gaussian Splatting (3DGS) point cloud files in an invisible manner, with robust security, high fidelity, large capacity, and versatility, suitable for copyright protection, encrypted communication, and compression of 3DGS.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|The paper presents a novel approach to Neural Radiance Fields (NeRFs) that addresses the limitations of prior methods in rendering highly specular objects by introducing ray tracing, allowing for more efficient and accurate rendering of reflections and view-dependent appearance.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|The paper proposes Neural Directional Encoding (NDE), a novel method for rendering specular objects like shiny metals or glossy paints, allowing for high-frequency and high-quality modeling of view-dependent effects and fast inference.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|The authors propose a two-staged pipeline for camera relocalization under varying lighting conditions, normalizing images with varying lighting and shadow conditions using a hash-encoded NeRF and a re-devised truncated dynamic low-pass filter, achieving state-of-the-art results.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in one sentence under 50 words:The authors propose LDM, a novel feed-forward framework that generates high-fidelity, illumination-decoupled textured meshes from single images or text prompts, utilizing a tensorial SDF representation and adaptive conversion strategy to improve convergence speed and quality.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|Here is a single sentence summarizing the key contributions from the abstract and introduction:The paper proposes JointRF, a novel end-to-end joint optimization scheme for dynamic Neural Radiance Field (NeRF) representation and compression, achieving significantly improved quality and compression efficiency, and outperforming state-of-the-art methods.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|Here is a single sentence summary of the abstract and introduction under 50 words:The authors propose DoGaussian, a method that trains 3D Gaussian Splatting (3DGS) in a distributed manner to reduce training time and memory usage, achieving state-of-the-art rendering quality and acceleration while maintaining rendering performance.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes Gaussian Time Machine (GTM), a real-time rendering method that models time-dependent attributes of Gaussian primitives to reconstruct 3D scenes with varying appearances, achieving state-of-the-art rendering fidelity and 100 times faster rendering than NeRF-based methods.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper introduces MOSS, a framework that synthesizes 3D clothed humans with motion-aware Gaussian splitting, which achieves state-of-the-art visual quality by incorporating kinematic information to control Gaussian deformation and address local occlusions in single-view videos.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|The key contributions from the paper's abstract and introduction are:* A novel method that enables an "off-the-shelf" spacecraft pose estimator, which is supposed to know the target CAD model, to be applied on an unknown target.* The method relies on an "in-the-wild" NeRF model that employs learnable appearance embeddings to represent varying illumination conditions found in natural scenes.* The NeRF model is trained using a sparse collection of images that depict the target, and then generates a large dataset that is diverse both in terms of viewpoint and illumination.* The method is demonstrated on Hardware-In-the-Loop images of SPEED+, which emulate lighting conditions close to those encountered on orbit, and shows that the method successfully enables the training of an off-the-shelf spacecraft pose estimation network from a sparse set of images.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|Tianqi Liu et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The authors propose a generalizable 3D Gaussian Splatting approach (MVSGaussian) that leverages Multi-View Stereo, efficient hybrid rendering, and consistent aggregation to enable fast and high-quality novel view synthesis for unseen scenes.|[2405.12218v1](http://arxiv.org/abs/2405.12218v1)|null|
|**2024-05-20**|**Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents**|Guanlin Wu et.al.|The key contributions from the paper's abstract and introduction are:1. The authors provide a comprehensive overview on integrating neural radiance field (NeRF) and 3D Gaussian splatting (3D-GS) in sixth-generation (6G) networks for immersive communication applications.2. The authors highlight the importance of efficient representation, transmission, and reconstruction of 3D contents over wireless networks.3. They introduce radiance field rendering approaches, NeRF and 3D-GS, which provide photorealistic rendering results for complex scenes.4. The authors discuss the pros and cons of NeRF and 3D-GS, including the lightweight models of NeRF but computationally heavy, and the explicit representation of 3D-GS but requiring large amounts of data.5. They identify the need for joint computation and communication designs, and propose a new semantic communication-enabled 3D content transmission design.6. The authors provide a survey of existing works on radiance field rendering, 6G wireless networks, and smart securing communication in 6G.|[2405.12155v1](http://arxiv.org/abs/2405.12155v1)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|Fotios Logothetis et.al.|Here is a single sentence summarizing the key contributions of the paper's abstract and introduction:The paper proposes a novel multi-view photometric stereo method that explicitly leverages per-pixel intensity renderings, models point light attenuation, and optimizes a fully neural material renderer, achieving state-of-the-art performance and robustness to poor normal estimates in low light scenarios.|[2405.12057v1](http://arxiv.org/abs/2405.12057v1)|null|
|**2024-05-19**|**Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems**|Shengxiang Sun et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a modified gradient-based texture optimization method to discover realistic-looking adversarial objects for autonomous driving systems, introducing a Judge agent to assess texture realism and integrate it into the loss function to ensure concurrent learning of realistic and adversarial textures.|[2405.11629v1](http://arxiv.org/abs/2405.11629v1)|null|
|**2024-05-19**|**R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless Environments**|Huiying Yang et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The authors propose a NeRF-based ray tracing method to model dynamic electromagnetic fields in RIS-enabled environments, capturing complex propagation dynamics and accurately predicting signal strength at different receiver locations, outperforming existing methods in both simulated and measured data.|[2405.11541v1](http://arxiv.org/abs/2405.11541v1)|null|
|**2024-05-18**|**MotionGS : Compact Gaussian Splatting SLAM by Motion Filter**|Xinli Guo et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes a novel 3DGS-based SLAM approach, MotionGS, which integrates deep feature extraction, dual keyframe selection, and 3DGS, achieving high-fidelity scene representation, accurate real-time tracking, and efficient rendering with reduced memory usage.|[2405.11129v1](http://arxiv.org/abs/2405.11129v1)|[link](https://github.com/antonio521/motiongs)|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper surveys the intersection of Large Language Models (LLMs) and 3D data, highlighting LLMs' potential to advance spatial comprehension and interaction in AI systems, and examining their integration with various 3D data representations for tasks such as scene understanding, captioning, and navigation.|[2405.10255v1](http://arxiv.org/abs/2405.10255v1)|null|
|**2024-05-15**|**From NeRFs to Gaussian Splats, and Back**|Siming He et.al.|The paper's abstract and introduction highlight the key contributions as follows:* A new procedure is developed to convert between implicit representations (Neural Radiance Fields (NeRFs)) and explicit representations (Gaussian Splatting (GS)) for scene understanding in robotics.* The proposed approach allows for achieving the best of both worlds: the superior rendering quality of NeRFs on novel views and the real-time rendering capabilities of GS.* The conversion procedure is demonstrated to be efficient and effective, with minor computational cost compared to training the two from scratch.The key contributions mentioned in the paper include:1. An efficient procedure to convert between NeRFs and GS, allowing for the fusion of the strengths of both representations.2. The development of a new representation, GSNeRF, which combines the rendering capabilities of GS with the interpretability of NeRFs.3. The demonstration of the effectiveness of the proposed approach in achieving high-quality rendering and real-time rendering capabilities.Note that these contributions are summarized in a single sentence under 50 words as requested.|[2405.09717v1](http://arxiv.org/abs/2405.09717v1)|[link](https://github.com/grasp-lyrl/nerftogsandback)|
|**2024-05-14**|**Dynamic NeRF: A Review**|Jinwei Lin et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:This review discusses the development and principles of Dynamic NeRF, a novel implicit method for 3D reconstruction and representation, focusing on its potential applications, implementation principles, and comparisons of different features, aiming to provide a comprehensive understanding of the technology.|[2405.08609v1](http://arxiv.org/abs/2405.08609v1)|null|
|**2024-05-13**|**Synergistic Integration of Coordinate Network and Tensorial Feature for Improving Neural Radiance Fields from Sparse Inputs**|Mingyu Kim et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a method that synergistically integrates multi-plane representation with coordinate-based network to improve Neural Radiance Fields (NeRFs) from sparse inputs, achieving comparable results with fewer parameters and outperforming others in static and dynamic NeRF tasks.|[2405.07857v1](http://arxiv.org/abs/2405.07857v1)|[link](https://github.com/mingyukim87/synergynerf)|
|**2024-05-12**|**Point Resampling and Ray Transformation Aid to Editable NeRF Models**|Zhenyang Li et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes an implicit ray transformation strategy and a plug-and-play inpainting module, differentiable neural-point resampling (DNR), to enable efficient object removal and scene inpainting tasks in NeRF-aided editing, achieving state-of-the-art performance and supporting high-quality rendering visualization.|[2405.07306v1](http://arxiv.org/abs/2405.07306v1)|null|
