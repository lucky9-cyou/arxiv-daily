
### NeRF
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-30**|**$\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving**|Nan Huang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The authors propose S3Gaussian, a self-supervised method for decomposing dynamic and static 3D Gaussians in street scenes without manual annotations, achieving state-of-the-art rendering quality on scene reconstruction and novel view synthesis tasks.|[2405.20323v1](http://arxiv.org/abs/2405.20323v1)|[link](https://github.com/nnanhuang/s3gaussian)|
|**2024-05-30**|**TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes**|Minghao Guo et.al.|Here is a summary of the key contributions in a single sentence under 50 words:TetSphere splatting presents an explicit, Lagrangian representation that efficiently reconstructs 3D shapes with high-quality geometry, utilizing tetrahedral meshes and differentiable rendering, and outperforms existing methods in terms of mesh quality, optimization speed, and thin structure preservation.|[2405.20283v1](http://arxiv.org/abs/2405.20283v1)|null|
|**2024-05-30**|**NeRF View Synthesis: Subjective Quality Assessment and Objective Metrics Evaluation**|Pedro Martin et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a comprehensive subjective quality assessment of Neural Radiance Fields (NeRF) view synthesis methods, evaluating the impact of various artifacts and proposing a new dataset and objective quality metrics for assessing the quality of NeRF synthesized views.|[2405.20078v1](http://arxiv.org/abs/2405.20078v1)|null|
|**2024-05-30**|**IReNe: Instant Recoloring in Neural Radiance Fields**|Alessio Mazzucchelli et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The authors introduce IReNe, a novel approach that enables near real-time color editing in Neural Radiance Fields (NeRF) by selectively fine-tuning the last layer of the network, leveraging a trainable segmentation module, and classifying neurons into view-dependent and diffuse appearance types to ensure consistent edits across different views.|[2405.19876v1](http://arxiv.org/abs/2405.19876v1)|null|
|**2024-05-30**|**HINT: Learning Complete Human Neural Representations from Limited Viewpoints**|Alessandro Sanvito et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes HINT, a NeRF-based algorithm that learns a detailed and complete human model from limited viewing angles, using symmetry prior, regularization constraints, and large human dataset training cues, outperforming previous state-of-the-art algorithms by 15% PSNR.|[2405.19712v1](http://arxiv.org/abs/2405.19712v1)|null|
|**2024-05-30**|**View-Consistent Hierarchical 3D SegmentationUsing Ultrametric Feature Fields**|Haodi He et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The authors present a novel method for lifting multi-granular and view-inconsistent 2D image segmentations into a hierarchical and 3D-consistent representation using ultrametric feature fields within a Neural Radiance Field (NeRF) framework, outperforming existing open-vocabulary 3D segmentation methods and introducing a new synthetic dataset and evaluation metrics.|[2405.19678v1](http://arxiv.org/abs/2405.19678v1)|null|
|**2024-05-29**|**Neural Radiance Fields for Novel View Synthesis in Monocular Gastroscopy**|Zijie Jiang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a novel method that uses neural radiance fields (NeRF) to synthesize photo-realistic images of the stomach from monocular gastroscopic images, achieving high-quality results and addressing the limitations of traditional 3D reconstruction techniques.|[2405.18863v1](http://arxiv.org/abs/2405.18863v1)|null|
|**2024-05-29**|**NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild**|Weining Ren et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors introduce NeRF On-the-go, a novel method that enables robust synthesis of novel views in complex, in-the-wild scenes from casually captured images, efficiently eliminating distractors and achieving a faster convergence speed, outperforming existing state-of-the-art techniques.|[2405.18715v1](http://arxiv.org/abs/2405.18715v1)|[link](https://github.com/cvg/nerf-on-the-go)|
|**2024-05-28**|**Self-supervised Pre-training for Transferable Multi-modal Perception**|Xiaohao Xu et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The authors propose NS-MAE, a self-supervised pre-training paradigm for transferable multi-modal representation learning, which leverages neural radiance fields and masked auto-encoders to learn robust and generalizable representations for autonomous driving perception tasks.|[2405.17942v1](http://arxiv.org/abs/2405.17942v1)|null|
|**2024-05-28**|**A Refined 3D Gaussian Representation for High-Quality Dynamic Scene Reconstruction**|Bin Zhang et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper proposes a refined 3D Gaussian representation for high-quality dynamic scene reconstruction, utilizing a hybrid representation combining deformation fields, hash encoding, and 3D Gaussians, and introducing a learnable denoising mask and motion consistency constraints to mitigate noise and artifacts, achieving high rendering quality and speed while reducing memory usage.|[2405.17891v1](http://arxiv.org/abs/2405.17891v1)|null|
|**2024-05-28**|**HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal High-Frequency Components for Endoscopic Scene Reconstruction**|Haoyu Zhao et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The authors propose HFGS, a novel approach that addresses under-reconstruction challenges in deformable endoscopic reconstruction by incorporating deformation fields, spatial high-frequency emphasis reconstruction, and temporal high-frequency emphasis reconstruction, resulting in superior rendering quality and dynamic awareness.|[2405.17872v2](http://arxiv.org/abs/2405.17872v2)|null|
|**2024-05-28**|**Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh**|Xiangjun Gao et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes a method for manipulating Gaussian Splatting (3DGS) using a triangular mesh, allowing for various manipulations, including large deformations, local manipulations, and soft body simulations, while maintaining high-quality rendering.|[2405.17811v1](http://arxiv.org/abs/2405.17811v1)|null|
|**2024-05-27**|**F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting**|Xiangyu Sun et.al.|Here is a summary of the paper's abstract and introduction in one sentence under 50 words:The paper proposes Factorized 3D Gaussian Splatting (F-3DGS), a method that reduces the storage requirements of 3DGS by factorizing 3D Gaussians into a small number of points and their attributes, allowing for efficient representation and rendering of 3D scenes while maintaining image quality.|[2405.17083v2](http://arxiv.org/abs/2405.17083v2)|null|
|**2024-05-27**|**PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting**|Zipeng Wang et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The authors propose Pyramidal 3D Gaussian Splatting (PyGS), a method that uses a hierarchical assembly of Gaussians and dynamic weighting to effectively model large-scale scenes with high-fidelity details, achieving a significant performance leap and over 400x faster rendering time.|[2405.16829v3](http://arxiv.org/abs/2405.16829v3)|null|
|**2024-05-26**|**Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors**|Soumava Paul et.al.|The paper proposes SparseSplat360, a method that uses pre-trained 2D diffusion models to fill in missing details and clean novel views, resulting in a multi-view consistent scene representation with coherent details and outperforming existing methods in sparse-view 360 scene reconstruction.|[2405.16517v1](http://arxiv.org/abs/2405.16517v1)|null|
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|The key contributions of the paper are:1. Introduction of Neural Elevation Models (NEMos), which adapt Neural Radiance Fields to a 2.5D continuous and differentiable terrain model.2. Novel method for jointly training a height field and radiance field within a NeRF framework, leveraging quantile regression.3. Path planning algorithm that performs gradient-based optimization of a continuous cost function for minimizing distance, slope changes, and control effort, enabled by differentiability of the height field.4. Experimental results on simulated and real-world terrain imagery, demonstrating NEMo's ability to generate high-quality reconstructions and produce smoother paths compared to discrete path planning methods.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|The key contributions from the paper's abstract and introduction are: A novel High Dynamic Range Gaussian Splatting (HDR-GS) framework for 3D HDR imaging, which can efficiently render novel HDR views and reconstruct LDR images with controllable exposure time, surpassing the state-of-the-art NeRF-based method by 3.84 and 1.91 dB on LDR and HDR novel view synthesis, while enjoying 1000x inference speed and only requiring 6.3% training time.|[2405.15125v2](http://arxiv.org/abs/2405.15125v2)|[link](https://github.com/caiyuanhao1998/hdr-gs)|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper proposes GS-Hider, a steganography framework for 3D Gaussian Splatting (3DGS) that effectively embeds 3D scenes and images into original 3DGS point cloud files, achieving robust security, high fidelity, large capacity, and strong versatility.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents an approach that utilizes ray tracing to improve the ability of Neural Radiance Fields (NeRFs) to render highly specular objects with consistent reflections, outperforming prior methods in view synthesis of scenes with shiny objects while requiring comparable optimization time.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents Neural Directional Encoding (NDE), a feature-grid-like encoding that accurately models high-frequency view-dependent appearances of specular objects, including glossy interreflections, and achieves fast evaluation and high-quality rendering.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The authors propose a two-staged pipeline that normalizes images with varying lighting conditions to improve camera relocalization, using a hash-encoded NeRF and re-devised truncated dynamic low-pass filter and numerical gradient averaging technique, achieving state-of-the-art results on a new dataset with varying lighting conditions.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes LDM, a novel feed-forward framework generating high-fidelity, illumination-decoupled textured meshes from text or single images, utilizing a multi-view diffusion model, transformer, and gradient-based mesh optimization layer to produce high-quality 3D mesh assets within seconds.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose JointRF, an end-to-end joint optimization scheme that represents and compresses dynamic Neural Radiance Fields for volumetric videos, achieving improved quality and compression efficiency by utilizing residual feature grids and sequential feature compression.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose DoGaussian, a distributed method for training 3D Gaussian Splatting (3DGS) on large-scale scenes, which splits scenes into blocks, applies ADMM for consensus, and achieves 6+ times faster training time while maintaining state-of-the-art rendering quality.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|Here is a summary of the key contributions in under 50 words: The paper proposes Gaussian Time Machine (GTM), a real-time rendering method that models time-dependent scene attributes, decomposes color models for improved geometric consistency, and achieves state-of-the-art rendering fidelity on 3 datasets, with speeds 100 times faster than NeRF-based methods.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper introduces the Motion-Based 3D Clothed Humans Synthesis (MOSS) framework, which employs kinematic information to achieve motion-aware Gaussian splitting for realistic clothing deformation and joint details, achieving state-of-the-art visual quality in 3D cloaked human synthesis from monocular videos.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents a novel method that enables the estimation of the 6D pose of an unknown target spacecraft relative to a monocular camera using a Neural Radiance Field (NeRF) and an off-the-shelf spacecraft pose estimation network, trained from a sparse set of images.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|Tianqi Liu et.al.|Here is a single sentence summarizing the paper's key contributions:The paper presents MVSGaussian, a novel generalizable 3D Gaussian representation method that leverages Multi-View Stereo, pixel-aligned Gaussian representations, and a consistent aggregation strategy to achieve state-of-the-art performance with real-time rendering speed and fast per-scene optimization.|[2405.12218v1](http://arxiv.org/abs/2405.12218v1)|null|
|**2024-05-20**|**Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents**|Guanlin Wu et.al.|The key contributions from the paper's abstract and introduction are:The paper discusses the integration of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3D-GS) in 6G networks for immersive communication applications, which require the efficient representation, transmission, and reconstruction of 3D contents. The paper highlights the importance of representing 3D contents with high visual quality, precision, and efficiency, while also considering the limitations and challenges of wireless networks and edge computing. The paper proposes a comprehensive overview of the integration of NeRF and 3D-GS in 6G networks, including over-the-air training of models, model compression, rendering acceleration, and joint computation and communication designs.|[2405.12155v1](http://arxiv.org/abs/2405.12155v1)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|Fotios Logothetis et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The paper presents a novel multi-view photometric stereo (PS) method that leverages neural shape representations and learned renderers to estimate 3D shapes from photometric stereo images. The method models point light attenuation, explicitly raytraces cast shadows, and optimizes a fully neural material renderer, achieving competitive reconstruction accuracy using only 6 lights and matching the state-of-the-art performance when all lights and normal map information are fused.|[2405.12057v1](http://arxiv.org/abs/2405.12057v1)|null|
