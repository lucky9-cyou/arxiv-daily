
### NeRF
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-27**|**F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting**|Xiangyu Sun et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper proposes Factorized 3D Gaussian Splatting (F-3DGS), a novel approach that significantly reduces storage requirements while maintaining image quality by factorizing both coordinates and features of 3D Gaussians, achieving a 90% reduction in storage costs compared to the original 3DGS method.|[2405.17083v1](http://arxiv.org/abs/2405.17083v1)|null|
|**2024-05-27**|**PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting**|Zipeng Wang et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper presents Pyramidal 3D Gaussian Splatting (PyGS) that combines a hierarchical assembly of Gaussians with NeRF initialization, achieving high-fidelity visual results, accelerated rendering performance, and addressing challenges in large-scale scene representation and initialization.|[2405.16829v1](http://arxiv.org/abs/2405.16829v1)|null|
|**2024-05-26**|**Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors**|Soumava Paul et.al.|The paper proposes SparseSplat360, a method that uses pre-trained 2D diffusion models to improve sparse-view reconstruction of 360 scenes, outputting a multi-view consistent scene representation with retained details.|[2405.16517v1](http://arxiv.org/abs/2405.16517v1)|null|
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors introduce Neural Elevation Models (NEMos), a novel representation that adapts Neural Radiance Fields to a 2.5D continuous and differentiable terrain model, allowing for efficient and accurate path planning and terrain reconstruction from aerial imagery.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|The key contributions of the paper are: proposing a novel 3D Gaussian Splatting (HDR-GS) framework for 3D HDR imaging, presenting a Dual Dynamic Range Gaussian point cloud model that can render HDR images and LDR views with controllable exposure time, and establishing a data foundation by recalibrating camera parameters and computing initial points for 3DGS-based methods.|[2405.15125v2](http://arxiv.org/abs/2405.15125v2)|[link](https://github.com/caiyuanhao1998/hdr-gs)|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|The paper proposes a steganography framework, GS-Hider, for 3D Gaussian Splatting (3DGS) to embed 3D scenes and images into original GS point clouds in an invisible manner, accurately extract the hidden messages, and ensure security, fidelity, and flexibility.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|Here is a summary of the key contributions from the abstract and introduction in a single sentence under 50 words:The paper presents an approach that addresses the limitations of Neural Radiance Fields by incorporating ray tracing, allowing for more efficient and high-quality rendering of view-dependent appearance, particularly for shiny objects with detailed reflections and nearby scene content.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The Neural Directional Encoding (NDE) approach presents a novel feature-grid-based spatial encoding for view-dependent appearance rendering, achieving high-quality modeling and fast evaluation for rendering specular objects like shiny metals or glossy paints.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|Here is a summary of the key contributions in one sentence under 50 words:The paper proposes a two-staged pipeline and a hash-encoded NeRF to normalize images with varying lighting and shadow conditions for camera relocalization, improving pose optimization and achieving state-of-the-art results on several datasets.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|Here is a summarized version of the key contributions in a single sentence under 50 words:The paper proposes LDM, a novel feed-forward framework generating high-quality triangular meshes with illumination-decoupled RGB textures from text or single images in seconds, introducing tensorial representation and an adaptive conversion strategy for convergence.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes JointRF, a novel end-to-end scheme that jointly optimizes dynamic NeRF representation and compression, achieving significant quality and compression efficiency improvement through a compact residual feature grid and sequential compression subnetwork.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes DoGaussian, a distributed method for training 3D Gaussian Splatting (3DGS) on large-scale scenes, which reduces training time by 6+ times while ensuring high rendering quality and convergence.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:Gaussian Time Machine (GTM) proposes a real-time rendering method that models time-dependent Gaussian primitives with a lightweight neural network, achieving state-of-the-art rendering fidelity, 100x faster rendering than NeRF-based methods, and disentangling appearance changes for smooth interpolation.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The MOSS framework introduces an innovative approach to single-view clothed human reconstruction, employing kinematic information to achieve motion-aware Gaussian splitting and surface deformation detection, resulting in state-of-the-art visual quality and improved realism in reconstructed surfaces.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:This paper presents a novel method to estimate the 6D pose of an unknown target spacecraft using an "off-the-shelf" spacecraft pose estimation network, trained using images of the target acquired through a monocular camera, leveraging Neural Radiance Fields to extend the scope of existing pose estimation methods to unknown targets.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|Tianqi Liu et.al.|Here is a summarization of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors present MVSGaussian, a generalizable Gaussian Splatting method that efficiently reconstructs unseen scenes, achieving real-time rendering with better synthesis quality and faster fine-tuning initialization through multi-view geometric consistent point cloud aggregation.|[2405.12218v1](http://arxiv.org/abs/2405.12218v1)|null|
|**2024-05-20**|**Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents**|Guanlin Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The paper focuses on the integration of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3D-GS) in 6G wireless networks for immersive communications, which require efficient representation, transmission, and reconstruction of 3D contents. The authors highlight the limitations of traditional 3D representation approaches and introduce NeRF and 3D-GS as promising new techniques that can provide photorealistic rendering results for complex scenes. They discuss the challenges of integrating NeRF and 3D-GS in 6G networks, including storage and transmission of large radiance field models, training and inference of neural networks, and joint computation and communication designs. The paper aims to provide a comprehensive overview of the integration of NeRF and 3D-GS in 6G wireless networks and explores new design approaches for efficient representation, transmission, and reconstruction of 3D contents.|[2405.12155v1](http://arxiv.org/abs/2405.12155v1)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|Fotios Logothetis et.al.|Key contributions from the paper's abstract and introduction:* A novel multi-view photometric stereo (PS) method that leverages per-pixel intensity renderings instead of relying on estimated normals.* The method models point light attenuation and explicitly raytraces cast shadows to best approximate each point's incoming radiance.* A fully neural material renderer is used, allowing for competitive reconstruction accuracy using only 6 lights, and matching the state-of-the-art performance when all lights and normal map information are used.|[2405.12057v1](http://arxiv.org/abs/2405.12057v1)|null|
|**2024-05-19**|**Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems**|Shengxiang Sun et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a modified gradient-based texture optimization method to generate realistic-looking adversarial objects for autonomous driving systems, introducing a "Judge" mechanism to evaluate the realism of generated objects and optimize their texture to balance adversarial effectiveness with realism.|[2405.11629v1](http://arxiv.org/abs/2405.11629v1)|null|
|**2024-05-19**|**R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless Environments**|Huiying Yang et.al.|Here are the key contributions summarized in a single sentence under 50 words:The paper proposes a novel NeRF-based ray tracing method for modeling dynamic electromagnetic fields in Reflective Intelligent Surface (RIS)-enabled environments, enabling accurate prediction of signal field at different receiver locations and efficient RIS deployment.|[2405.11541v1](http://arxiv.org/abs/2405.11541v1)|null|
|**2024-05-18**|**MotionGS : Compact Gaussian Splatting SLAM by Motion Filter**|Xinli Guo et.al.|The paper proposes a novel 3DGS-based SLAM approach, MotionGS, which integrates deep feature extraction, dual keyframe selection, and 3D Gaussian splatting to achieve high-fidelity scene representation and real-time tracking, outperforming existing methods in terms of accuracy and memory usage.|[2405.11129v1](http://arxiv.org/abs/2405.11129v1)|[link](https://github.com/antonio521/motiongs)|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper reviews the integration of Large Language Models (LLMs) with 3D spatial data, highlighting the strengths of LLMs, such as in-context learning and world knowledge, in advancing spatial comprehension and interaction within AI systems, while exploring challenges and applications in 3D environments.|[2405.10255v1](http://arxiv.org/abs/2405.10255v1)|null|
|**2024-05-15**|**From NeRFs to Gaussian Splats, and Back**|Siming He et.al.|The key contributions of the paper are:* A procedure to convert between implicit representations (NeRFs) and explicit representations (Gaussian Splatting) with minor computational cost compared to training the models from scratch.* A compact representation of the scene using NeRFs, which can be modified easily by converting it back to GS and modifying the Gaussians.* A fast rendering capability using GS, which can be used for real-time applications such as localization and planning.* A method to achieve the best of both worlds, combining the advantages of NeRFs (superior PSNR, SSIM, and LPIPS on dissimilar views) and GS (real-time rendering and ability for easily modifying the representation).|[2405.09717v1](http://arxiv.org/abs/2405.09717v1)|[link](https://github.com/grasp-lyrl/nerftogsandback)|
|**2024-05-14**|**Dynamic NeRF: A Review**|Jinwei Lin et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in one sentence under 50 words: The paper reviews the development and implementation principles of Dynamic Neural Radiance Fields (NeRF), a novel method for implicit 3D reconstruction, and analyzes its potential applications, main principles, and key methods, providing a comprehensive understanding of the emerging field.|[2405.08609v1](http://arxiv.org/abs/2405.08609v1)|null|
|**2024-05-13**|**Synergistic Integration of Coordinate Network and Tensorial Feature for Improving Neural Radiance Fields from Sparse Inputs**|Mingyu Kim et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The proposed method synergistically integrates coordinate-based networks and multi-plane representations to improve the performance of neural radiance fields (NeRFs) from sparse inputs, achieving comparable results to explicit encoding with fewer parameters.|[2405.07857v1](http://arxiv.org/abs/2405.07857v1)|[link](https://github.com/mingyukim87/synergynerf)|
|**2024-05-12**|**Point Resampling and Ray Transformation Aid to Editable NeRF Models**|Zhenyang Li et.al.|The paper proposes an implicit ray transformation strategy for NeRF-aided editing, enabling direct manipulation of 3D object poses by operating on neural points in rays, and introduces a plug-and-play inpainting module, DNR, to address empty regions created by object removal, achieving state-of-the-art performance in 3D object removal and inpainting tasks.|[2405.07306v1](http://arxiv.org/abs/2405.07306v1)|null|
|**2024-05-12**|**Hologram: Realtime Holographic Overlays via LiDAR Augmented Reconstruction**|Ekansh Agrawal et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The paper proposes three high-fidelity reconstruction tools for real-time holographic overlays using LiDAR augmented 3D reconstruction. The main contributions are:* Developing a monocular depth estimation approach using a ViT backbone and classical image projections to transform image pixels into voxel representations.* Utilizing the LiDAR and TrueDepth sensors on the iPhone 14 Pro to generate a fused depth frame, upscaled with a SRCNN model, and projecting points into the world coordinate system using the phone's IMU sensor.* Leveraging a deep-learning approach, MICA, to render a facial reconstruction and apply texture + mesh transformations directly through an end-to-end approach, using a modified version of MICA trained on a dataset with depth dimension generated via Marigold.These contributions aim to enable interactive and immersive holographic experiences, including augmented reality, telepresence, and entertainment, on a portable device like the iPhone 14 Pro.|[2405.07178v1](http://arxiv.org/abs/2405.07178v1)|null|
|**2024-05-11**|**TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural Radiance Field Optimization**|Zhen Tan et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose Truncated Depth NeRF (TD-NeRF), a novel approach that jointly optimizes camera poses and radiance fields using monocular depth priors, and introduces three key advancements: truncated depth-based ray sampling, coarse-to-fine training, and robust inter-frame point constraint.|[2405.07027v1](http://arxiv.org/abs/2405.07027v1)|[link](https://github.com/nubot-nudt/td-nerf)|
|**2024-05-10**|**LIVE: LaTex Interactive Visual Editing**|Jinwei Lin et.al.|The paper proposes a novel design method, LIVE, to design interactive LaTeX graphic items, which can represent more information volume than static items and provide an interactive reading experience. LIVE enables the design of interactive graphic items, called Gitems, and automatically analyzes the citation relationships between papers, adding vitality and performance factors to traditional papers, especially review papers.|[2405.06762v1](http://arxiv.org/abs/2405.06762v1)|null|
|**2024-05-10**|**SketchDream: Sketch-based Text-to-3D Generation and Editing**|Feng-Lin Liu et.al.|The key contributions mentioned in the abstract and introduction are:* Proposed a sketch-based text-to-3D generation and editing method, SketchDream, which supports both generation and editing of photo-realistic 3D models with high-quality and detailed control.* Introduced a sketch-based multi-view image generation diffusion model that warps sketches into 3D space and generates multi-view images with 3D consistency.* Designed a 3D attention control module to ensure 3D consistency between the generated images and the original sketch.* Developed a coarse-to-fine editing framework that allows for local editing of 3D models while preserving unedited regions.* Utilized Score Distillation Sampling (SDS) to generate high-quality 3D contents and applied a 2D silhouette loss to improve sketch faithfulness.* Tested the method on diverse categories of objects, achieving high-quality results.|[2405.06461v2](http://arxiv.org/abs/2405.06461v2)|null|
