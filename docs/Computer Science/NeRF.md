
### NeRF
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-28**|**Self-supervised Pre-training for Transferable Multi-modal Perception**|Xiaohao Xu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes NS-MAE, a self-supervised pre-training framework that learns transferable multi-modal representations for autonomous driving, enabling unified optimization and efficient fine-tuning for diverse multi-modal and single-modal perception models.|[2405.17942v1](http://arxiv.org/abs/2405.17942v1)|null|
|**2024-05-28**|**A Refined 3D Gaussian Representation for High-Quality Dynamic Scene Reconstruction**|Bin Zhang et.al.|The key contributions from the paper's abstract and introduction are:1. The introduction of a refined 3D Gaussian representation for high-quality dynamic scene reconstruction, which addresses the concerns of increased memory usage and rendering dynamic scenes.2. The use of a deformable multi-layer perceptron (MLP) network to capture the dynamic offset of Gaussian points and express the color features of points through hash encoding and a tiny MLP to reduce storage requirements.3. The introduction of a learnable denoising mask coupled with denoising loss to eliminate noise points from the scene, further compressing the 3D Gaussian model.4. The utilization of static constraints and motion consistency constraints to mitigate motion artifacts in the points, ensuring the accuracy and efficiency of the rendering framework when processing dynamic scenes.These contributions aim to achieve high-quality and efficient rendering of dynamic scenes, while addressing the challenges of increased memory usage and rendering dynamic scenes.|[2405.17891v1](http://arxiv.org/abs/2405.17891v1)|null|
|**2024-05-28**|**HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal High-Frequency Components for Endoscopic Scene Reconstruction**|Haoyu Zhao et.al.|The paper proposes a novel approach, HFGS, for deformable endoscopic reconstruction, which addresses the challenges of under-reconstruction in both static and dynamic scenes by incorporating deformation fields and introducing Spatial and Temporal High-Frequency Emphasis Reconstruction modules to improve rendering quality.|[2405.17872v1](http://arxiv.org/abs/2405.17872v1)|null|
|**2024-05-28**|**Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh**|Xiangjun Gao et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a method to manipulate 3D Gaussian Splatting (3DGS) using a triangular mesh, enabling controllable and high-quality photo-realistic rendering, while maintaining a high tolerance for mesh accuracy and supporting various 3DGS manipulations.|[2405.17811v1](http://arxiv.org/abs/2405.17811v1)|null|
|**2024-05-27**|**F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting**|Xiangyu Sun et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes Factorized 3D Gaussian Splatting (F-3DGS), a novel approach that reduces storage requirements while maintaining image quality by factorizing coordinates and attributes of 3D Gaussians, enabling real-time rendering and efficient scene representation.|[2405.17083v2](http://arxiv.org/abs/2405.17083v2)|null|
|**2024-05-27**|**PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting**|Zipeng Wang et.al.|The paper presents Pyramidal 3D Gaussian Splatting (PyGS) with NeRF initialization, which achieves high-fidelity visual results and accelerated rendering performance by representing large-scale scenes with a hierarchical assembly of Gaussians, efficiently initialized through a rapidly trained grid-based NeRF, and dynamically weighted clusters to balance detail levels and viewpoints.|[2405.16829v2](http://arxiv.org/abs/2405.16829v2)|null|
|**2024-05-26**|**Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors**|Soumava Paul et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The authors propose SparseSplat360, a method that uses latent diffusion models to improve sparse-view reconstruction of 360-degree scenes with low-cost fine-tuning, outperforming existing methods on the challenging Mip-NeRF360 dataset.|[2405.16517v1](http://arxiv.org/abs/2405.16517v1)|null|
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:* The paper introduces Neural Elevation Models (NEMos), which adapt Neural Radiance Fields (NeRFs) to a 2.5D continuous and differentiable terrain model.* NEMos can be generated from imagery, a low-cost data source, and provide a lightweight representation of terrain through an implicit continuous and differentiable height field.* The paper proposes a novel approach for joint training of a height field and radiance field within a NeRF framework, leveraging quantile regression.* The paper also introduces a path planning algorithm that performs gradient-based optimization of a continuous cost function for minimizing distance, slope changes, and control effort, enabled by the differentiability of the height field.Note that the abstract and introduction do not provide a single sentence summary, but rather a brief overview of the paper's contributions.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|The key contributions from the paper's abstract and introduction are: a novel framework, High Dynamic Range Gaussian Splatting (HDR-GS), that efficiently renders novel HDR views and reconstructs LDR images with a user input exposure time, surpassing state-of-the-art NeRF-based methods by 3.84 and 1.91 dB, and achieving 1000x inference speed and 6.3% training time.|[2405.15125v2](http://arxiv.org/abs/2405.15125v2)|[link](https://github.com/caiyuanhao1998/hdr-gs)|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in one sentence under 50 words:The authors propose GS-Hider, a steganography framework for 3D Gaussian Splatting (3DGS) that embeds 3D scenes and images into original point clouds, ensuring security, fidelity, and flexibility, with robustness, capacity, and adaptability in various applications, including copyright protection and secret communication.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose a novel approach to Neural Radiance Fields (NeRF) that uses ray tracing to render consistent reflections of nearby and distant content, while reducing the computational burden of modeling view-dependent appearance.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The paper presents Neural Directional Encoding (NDE), a novel view-dependent appearance encoding method for rendering specular objects like metals, plastics, and glossy paints, which achieves high-quality modeling of view-dependent effects and fast evaluation. NDE transfers the concept of feature-grid-based spatial encoding to the angular domain, allowing for more accurate modeling of high-frequency angular signals, and introduces a new spatio-spatial parameterization by cone-tracing spatial features to encode near-field interreflections.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|The paper proposes a two-staged camera relocalization pipeline that normalizes images with varying lighting and shadow conditions, uses a hash-encoded NeRF for fast training and robust pose refinement, and introduces a re-devised truncated dynamic low-pass filter and numerical gradient averaging technique to improve gradient computing.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|The paper proposes a novel feed-forward framework, LDM, generating high-fidelity, illumination-decoupled textured meshes from single images or text prompts, and primarily contributes novel multi-view diffusion models, transformer-based SDF prediction, and gradient-based mesh optimization layers.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The authors propose JointRF, a novel end-to-end learning scheme that jointly optimizes dynamic NeRF representation and compression, achieving superior rate-distortion performance and efficient compression of long-sequence radiance fields.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|The paper proposes DoGaussian, a distributed training method for 3D Gaussian Splatting (3DGS) to accelerate training on large-scale scenes, achieving a 6+ times speedup while maintaining state-of-the-art rendering quality.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The Gaussian Time Machine (GTM) models time-dependent attributes of Gaussian primitives with a lightweight MLP, enabling efficient real-time rendering, high-quality reconstructions, and disentanglement of appearance changes and rendering smooth interpolation on dynamic scenes.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors introduce MOSS, a motion-aware framework that leverages kinematic information to achieve realistic clothing deformation in single-view clothed human reconstruction, improving visual quality and addressing local occlusions, and achieves state-of-the-art performance in 3D clothed human synthesis from monocular videos.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:This paper presents a novel method that enables the training of an "off-the-shelf" spacecraft pose estimation network from a sparse collection of images depicting an unknown target spacecraft, leveraging Neural Radiance Fields (NeRFs) to implicitly represent the target and generate a large dataset that captures the diversity of illumination conditions.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|Tianqi Liu et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The authors propose MVSGaussian, a generalizable Gaussian Splatting method that leverages Multi-View Stereo and a pixel-aligned Gaussian representation, and introduce a hybrid Gaussian rendering approach and a consistent aggregation strategy to achieve real-time rendering with better synthesis quality for each scene.|[2405.12218v1](http://arxiv.org/abs/2405.12218v1)|null|
|**2024-05-20**|**Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents**|Guanlin Wu et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper discusses the integration of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3D-GS) with 6G networks to enable efficient representation, transmission, and reconstruction of 3D contents, and proposes novel techniques for over-the-air training and rendering, edge-computing, and joint computation-communication designs.|[2405.12155v1](http://arxiv.org/abs/2405.12155v1)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|Fotios Logothetis et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents a novel multi-view photometric stereo method that leverages per-pixel intensity renderings, models point light attenuation, and optimizes a fully neural material renderer to achieve competitive reconstruction accuracy using only 6 lights.|[2405.12057v1](http://arxiv.org/abs/2405.12057v1)|null|
|**2024-05-19**|**Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems**|Shengxiang Sun et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a modified gradient-based texture optimization method, incorporating a "Judge" mechanism to assess and refine the realism of adversarial objects, enhancing their potential to blend in with real-world environments and evade detection by autonomous driving systems.|[2405.11629v1](http://arxiv.org/abs/2405.11629v1)|null|
|**2024-05-19**|**R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless Environments**|Huiying Yang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose a novel NeRF-based ray tracing method to model dynamic electromagnetic fields in RIS-enabled environments, accurately capturing complex propagation dynamics and representing signal emission and transmission across diverse positions.|[2405.11541v1](http://arxiv.org/abs/2405.11541v1)|null|
|**2024-05-18**|**MotionGS : Compact Gaussian Splatting SLAM by Motion Filter**|Xinli Guo et.al.|Here is a summary of the key contributions in one sentence under 50 words:The paper proposes a novel 3D Gaussian Splatting (3DGS)-based SLAM approach, MotionGS, that integrates deep feature extraction, dual keyframe selection, and 3DGS, achieving state-of-the-art performance in tracking, mapping, and rendering while reducing memory usage.|[2405.11129v1](http://arxiv.org/abs/2405.11129v1)|[link](https://github.com/antonio521/motiongs)|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:This paper provides a comprehensive survey of methodologies enabling large language models (LLMs) to process, understand, and generate 3D data, highlighting the potential of LLMs to advance spatial comprehension and interaction within AI systems.|[2405.10255v1](http://arxiv.org/abs/2405.10255v1)|null|
|**2024-05-15**|**From NeRFs to Gaussian Splats, and Back**|Siming He et.al.|The key contributions of the paper are:* Developing a procedure to convert between parametric scene representations, such as Neural Radiance Fields (NeRFs), and non-parametric scene representations, such as Gaussian Splatting (GS), allowing for the best of both worlds: the superior generalization and compactness of NeRFs, and the fast rendering and easy modification of GS.* Showcasing the effectiveness of this conversion procedure using several datasets and real-world scenarios, achieving strong results in terms of PSNR, SSIM, and LPIPS metrics, and demonstrating the flexibility and robustness of the approach.In summary, the paper presents a novel method for converting between NeRFs and GS, enabling the exploitation of the strengths of both representations, and demonstrating its applicability in various robotic and computer vision tasks.|[2405.09717v1](http://arxiv.org/abs/2405.09717v1)|[link](https://github.com/grasp-lyrl/nerftogsandback)|
|**2024-05-14**|**Dynamic NeRF: A Review**|Jinwei Lin et.al.|The paper's abstract and introduction summarize the development of Neural Radiance Field (NeRF) from its static beginnings to the emerging study of dynamic NeRF, highlighting its potential for practical applications and reviewing its development history, important implementation principles, and comparison of different features.|[2405.08609v1](http://arxiv.org/abs/2405.08609v1)|null|
|**2024-05-13**|**Synergistic Integration of Coordinate Network and Tensorial Feature for Improving Neural Radiance Fields from Sparse Inputs**|Mingyu Kim et.al.|The key contributions from the paper's abstract and introduction are:* The paper proposes a method that synergistically integrates multi-plane representation with coordinate-based network to improve the performance of Neural Radiance Fields (NeRFs) from sparse inputs.* The multi-plane representation is responsible for capturing fine-grained details, while the coordinate-based network captures low-frequency details.* The residual connections between them preserve their inherent properties and allow for efficient parameter allocation.* The paper also proposes a progressive training scheme to accelerate the disentanglement of these two features.Single sentence summary under 50 words: The paper introduces a method that combines multi-plane representation and coordinate-based network to improve NeRFs from sparse inputs, achieving better performance and efficiency.|[2405.07857v1](http://arxiv.org/abs/2405.07857v1)|[link](https://github.com/mingyukim87/synergynerf)|
|**2024-05-12**|**Point Resampling and Ray Transformation Aid to Editable NeRF Models**|Zhenyang Li et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose an implicit ray transformation strategy and a differentiable neural-point resampling module to enable object movement and inpainting in Neural Radiance Fields (NeRF), achieving state-of-the-art performance in 3D object removal and inpainting tasks.|[2405.07306v1](http://arxiv.org/abs/2405.07306v1)|null|
