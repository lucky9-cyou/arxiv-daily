
### NeRF
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-29**|**Neural Radiance Fields for Novel View Synthesis in Monocular Gastroscopy**|Zijie Jiang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The key contributions are:* A novel approach to enable the synthesis of arbitrarily novel viewpoint images within a patient's stomach from pre-captured monocular gastroscopic images using neural radiance fields (NeRF).* The incorporation of geometry priors from a pre-reconstructed point cloud into the training of NeRF to address the performance degradation due to view sparsity in local regions of monocular gastroscopy.* A novel geometry-based loss to both pre-captured observed views and generated unobserved views to effectively constrain the learned geometry by using the point cloud pre-reconstructed by SfM.* High-fidelity image renderings from novel viewpoints within the stomach both qualitatively and quantitatively.|[2405.18863v1](http://arxiv.org/abs/2405.18863v1)|null|
|**2024-05-29**|**NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild**|Weining Ren et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The authors introduce NeRF On-the-go, a plug-and-play module for effective distractor removal, allowing robust novel view synthesis in complex, in-the-wild scenes from casually captured images, with a notable improvement over state-of-the-art techniques.|[2405.18715v1](http://arxiv.org/abs/2405.18715v1)|null|
|**2024-05-28**|**Self-supervised Pre-training for Transferable Multi-modal Perception**|Xiaohao Xu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes NS-MAE, a self-supervised pre-training paradigm for transferable multi-modal representation learning in autonomous driving, enabling efficient fine-tuning for diverse multi-modal and single-modal perception models with improved performance on various 3D perception tasks.|[2405.17942v1](http://arxiv.org/abs/2405.17942v1)|null|
|**2024-05-28**|**A Refined 3D Gaussian Representation for High-Quality Dynamic Scene Reconstruction**|Bin Zhang et.al.|The key contributions from the paper's abstract and introduction are:1. A hybrid representation combining deformation fields, hash encoding, and 3D Gaussian splatting to significantly reduce memory usage while achieving efficient and realistic rendering of dynamic scenes.2. A learnable denoising mask that effectively identifies and removes noise points in 3D Gaussian splatting, thereby enhancing the purity and quality of scene rendering.3. Static constraints and motion consistency constraints introduced to minimize noise in points during motion, ensuring that deformation fields can accurately learn the dynamic offset of points.These contributions aim to address the limitations of existing methods, such as NeRF and 3D-GS, in terms of rendering speed, quality, and memory usage.|[2405.17891v1](http://arxiv.org/abs/2405.17891v1)|null|
|**2024-05-28**|**HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal High-Frequency Components for Endoscopic Scene Reconstruction**|Haoyu Zhao et.al.|Here is a summary of the key contributions from the abstract and introduction in a single sentence under 50 words:The authors propose HFGS, a novel approach for deformable endoscopic reconstruction that addresses under-reconstruction in both static and dynamic scenes by incorporating deformation fields, spatial high-frequency emphasis reconstruction, and temporal high-frequency emphasis reconstruction.|[2405.17872v2](http://arxiv.org/abs/2405.17872v2)|null|
|**2024-05-28**|**Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh**|Xiangjun Gao et.al.|The key contributions of the paper are:* A method that enables 3DGS manipulation, achieving high-quality and photo-realistic rendering, by using a triangular mesh as a proxy to transfer mesh manipulation to 3DGS with 3DGS self-adaptation.* The introduction of a triangle shape aware Gaussian binding strategy with self-adaptation, which has a high tolerance for mesh accuracy and supports various 3DGS manipulations.|[2405.17811v1](http://arxiv.org/abs/2405.17811v1)|null|
|**2024-05-27**|**F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting**|Xiangyu Sun et.al.|The key contributions from the paper's abstract and introduction are:|[2405.17083v2](http://arxiv.org/abs/2405.17083v2)|null|
|**2024-05-27**|**PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting**|Zipeng Wang et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes Pyramidal 3D Gaussian Splatting (PyGS) that addresses challenges in large-scale scene modeling by using a hierarchical pyramidal Gaussian structure, dynamic weighting, and NeRF initialization to achieve fast rendering and capture high-frequency details.|[2405.16829v3](http://arxiv.org/abs/2405.16829v3)|null|
|**2024-05-26**|**Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors**|Soumava Paul et.al.|The paper proposes SparseSplat360, a method that leverages pretrained 2D diffusion models to improve sparse-view reconstruction of 360 3D scenes with low-cost fine-tuning, achieving state-of-the-art results on the challenging Mip-NeRF360 dataset.|[2405.16517v1](http://arxiv.org/abs/2405.16517v1)|null|
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|The key contributions from the paper's abstract and introduction are:* Introducing Neural Elevation Models (NEMos), which adapt Neural Radiance Fields to a 2.5D continuous and differentiable terrain model, allowing for lightweight representation of terrain through an implicit continuous height field.* Developing a novel method for jointly training a height field and radiance field within a NeRF framework, using quantile regression.* Proposing a path planning algorithm that leverages the continuous and differentiable nature of the height field to achieve smoother paths compared to discrete path planning methods.* Demonstrating experiments on simulated and real-world terrain imagery, showing NEMOs' ability to generate high-quality reconstructions and produce smoother paths.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose High Dynamic Range Gaussian Splatting (HDR-GS), a novel framework for HDR novel view synthesis, which surpasses state-of-the-art NeRF-based methods by 1.91 dB on HDR novel view synthesis, enjoys 1000x inference speed, and requires 6.3% training time.|[2405.15125v2](http://arxiv.org/abs/2405.15125v2)|[link](https://github.com/caiyuanhao1998/hdr-gs)|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|The paper proposes a steganography framework, GS-Hider, to embed 3D scenes and images into original 3D Gaussian Splatting (3DGS) point cloud files in an invisible manner, ensuring the security, fidelity, and flexibility of the hidden messages.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|The paper presents a new approach to Neural Radiance Fields (NeRFs) that uses ray tracing to improve the rendering of highly specular objects with view-dependent appearance, achieving faster computation and higher quality reflections.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors present Neural Directional Encoding (NDE), a feature-grid-like neural network that accurately models the appearance of shiny objects by transferring spatial encoding to the angular domain, enabling fast and high-quality rendering of specular objects.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|The paper proposes a two-staged pipeline that normalizes images with varying lighting and shadow conditions to improve camera relocalization, using a hash-encoded NeRF with a re-devised truncated dynamic low-pass filter and numerical gradient averaging technique to optimize pose refinement.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes LDM, a novel feed-forward framework that generates high-fidelity, illumination-decoupled textured meshes from a single image or text prompts, utilizing a multi-view diffusion model, transformer-based SDF field prediction, and gradient-based mesh optimization layer.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes JointRF, a novel end-to-end joint optimization scheme for dynamic Neural Radiance Field (NeRF) representation and compression, achieving improved quality and compression efficiency through a compact residual feature grid and sequential feature compression.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|The paper's key contributions are the proposal of a distributed training method for 3D Gaussian Splatting (3DGS) called DoGaussian, which accelerates training time by 6+ times on large-scale scenes while maintaining state-of-the-art rendering quality, and introduces a recursive scene splitting approach for distributed training with 3D Gaussian consensus ensuring training convergence and stability.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The Gaussian Time Machine (GTM) method proposes a lightweight, real-time rendering solution for 3D scenes with varying appearances, achieving state-of-the-art reconstruction quality, 100 times faster rendering, and smooth appearance interpolation.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper introduces MOSS, a novel framework that synthesizes 3D clothed humans by propagating global motion information to guide Gaussian split and detecting local deformations, achieving state-of-the-art visual quality in 3D reconstruction from monocular videos.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|Here is a summary of the key contributions in the paper's abstract and introduction:The paper presents a novel method that enables an "off-the-shelf" spacecraft pose estimator to be applied on an unknown target by leveraging Neural Radiance Fields (NeRF) models. The method is capable of estimating the 6D pose of an unknown target spacecraft relative to a monocular camera, a crucial step towards autonomous rendezvous and proximity operations in Active Debris Removal missions.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|Tianqi Liu et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper presents MVSGaussian, a generalizable 3D Gaussian representation approach derived from Multi-View Stereo, which efficiently reconstructs unseen scenes with real-time rendering and better synthesis quality, while achieving 13.3x less training computational cost compared to vanilla 3D-GS.|[2405.12218v1](http://arxiv.org/abs/2405.12218v1)|null|
|**2024-05-20**|**Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents**|Guanlin Wu et.al.|The key contributions from the paper's abstract and introduction are:The paper discusses the potential of neural radiance field (NeRF) and 3D Gaussian splatting (3D-GS) techniques for representing and transmitting 3D contents in 6G networks, aiming to enable immersive communications with high-quality rendering results. The paper highlights the need to efficiently represent, transmit, and reconstruct 3D contents over wireless networks, considering the limitations of current techniques and the requirements of emerging 6G applications.|[2405.12155v1](http://arxiv.org/abs/2405.12155v1)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|Fotios Logothetis et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper presents a novel multi-view photometric stereo (MV-PS) method that leverages per-pixel intensity renderings, explicitly models point light attenuation and cast shadows, and uses a fully neural material renderer to achieve competitive reconstruction accuracy using only 6 lights and matching the state-of-the-art performance when all lights and normal map information is fused.|[2405.12057v1](http://arxiv.org/abs/2405.12057v1)|null|
|**2024-05-19**|**Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems**|Shengxiang Sun et.al.|Here is a summarized version of the key contributions from the paper's abstract and introduction:* The paper proposes a method for generating realistic-looking adversarial objects for autonomous driving systems, which can deceive computer vision-based systems without detection.* The approach is based on a novel modification to the gradient-based texture optimization method, which includes an evaluative mechanism (the "Judge") to assess the realism of the generated objects.* The Judge assigns a probability score to the realism of an object, which is then integrated into the loss function to encourage the object renderer to produce realistic and adversarial textures simultaneously.* The paper explores various strategies for optimizing the Judge, including leveraging cutting-edge vision-language models, fine-tuning open-sourced vision-language models, pretraining neurosymbolic systems, and utilizing traditional image processing techniques.|[2405.11629v1](http://arxiv.org/abs/2405.11629v1)|null|
|**2024-05-19**|**R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless Environments**|Huiying Yang et.al.|The key contributions of the paper are summarized as follows:* Developing a two-stage framework for simulating electromagnetic signal propagation in RIS-enabled wireless environments, allowing for accurate characterization of signal dynamics.* Proposing a novel R-NeRF method to model the signal field for any specified RIS placement and receiver location.* Experimental results demonstrating the effectiveness of the proposed method with simulated and measured data.|[2405.11541v1](http://arxiv.org/abs/2405.11541v1)|null|
|**2024-05-18**|**MotionGS : Compact Gaussian Splatting SLAM by Motion Filter**|Xinli Guo et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a novel 3D Gaussian Splatting (3DGS)-based SLAM approach, MotionGS, which integrates deep feature extraction, dual keyframe selection, and 3DGS, achieving high-fidelity scene representation, accurate real-time tracking, and low memory usage.|[2405.11129v1](http://arxiv.org/abs/2405.11129v1)|[link](https://github.com/antonio521/motiongs)|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper provides a comprehensive overview of Large Language Models (LLMs) and 3D spatial data integration, highlighting their potential to advance spatial comprehension and interaction, and discusses current methodologies, applications, and challenges in this domain.|[2405.10255v1](http://arxiv.org/abs/2405.10255v1)|null|
|**2024-05-15**|**From NeRFs to Gaussian Splats, and Back**|Siming He et.al.|Here is a summary of the key contributions mentioned in the abstract and introduction:The paper presents a procedure to convert between implicit representations of a scene, such as neural radiance fields (NeRFs), and explicit representations, such as Gaussian splatting (GS). This conversion enables the benefits of both approaches, including the generalization of NeRFs to new views and the rendering speed of GS. The proposed method achieves state-of-the-art results in terms of PSNR, SSIM, and LPIPS on dissimilar views, while also reducing the computational cost of training and rendering.|[2405.09717v1](http://arxiv.org/abs/2405.09717v1)|[link](https://github.com/grasp-lyrl/nerftogsandback)|
|**2024-05-14**|**Dynamic NeRF: A Review**|Jinwei Lin et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:This review aims to provide a comprehensive overview of the development and implementation principles of Dynamic Neural Radiance Field (NeRF), highlighting its potential in practical applications and analyzing the main principles and techniques from 2021 to 2023.|[2405.08609v1](http://arxiv.org/abs/2405.08609v1)|null|
