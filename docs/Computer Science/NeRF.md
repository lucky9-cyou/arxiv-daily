
### NeRF
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-29**|**Neural Radiance Fields for Novel View Synthesis in Monocular Gastroscopy**|Zijie Jiang et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper proposes a novel approach to enabling the synthesis of arbitrarily novel viewpoint images within a patient's stomach from pre-captured monocular gastroscopic images using neural radiance fields (NeRF), incorporating a geometry-based loss to address view sparsity and noisy reconstructions.|[2405.18863v1](http://arxiv.org/abs/2405.18863v1)|null|
|**2024-05-29**|**NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild**|Weining Ren et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper introduces NeRF On-the-go, a plug-and-play module that efficiently eliminates distractors and achieves faster convergence speed in complex, real-world scenes, enabling robust novel view synthesis from casually captured images.|[2405.18715v1](http://arxiv.org/abs/2405.18715v1)|null|
|**2024-05-28**|**Self-supervised Pre-training for Transferable Multi-modal Perception**|Xiaohao Xu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a self-supervised pre-training framework, NS-MAE, for multi-modal representation learning in autonomous driving, enabling transferable learning across multi-modal and single-modal perception models.|[2405.17942v1](http://arxiv.org/abs/2405.17942v1)|null|
|**2024-05-28**|**A Refined 3D Gaussian Representation for High-Quality Dynamic Scene Reconstruction**|Bin Zhang et.al.|The key contributions from the paper's abstract and introduction are:* The development of a hybrid representation of dynamic scenes, combining the advantages of Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3D-GS).* The introduction of a refined 3D Gaussian representation for high-quality dynamic scene reconstruction, which reduces storage usage and enhances rendering speed.* The proposal of a learnable denoising mask to eliminate noise points from the scene and a novel view synthesis method tailored for dynamic mapping.These contributions aim to overcome the limitations of existing approaches, such as the explicit representation of NeRF, which can be computationally expensive, and the high memory usage of 3D-GS, while achieving efficient and realistic rendering of dynamic scenes.|[2405.17891v1](http://arxiv.org/abs/2405.17891v1)|null|
|**2024-05-28**|**HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal High-Frequency Components for Endoscopic Scene Reconstruction**|Haoyu Zhao et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes HFGS, a novel approach for deformable endoscopic reconstruction that addresses under-reconstruction issues by incorporating deformation fields, spatial high-frequency emphasis reconstruction, and temporal high-frequency emphasis reconstruction, achieving superior rendering quality.|[2405.17872v2](http://arxiv.org/abs/2405.17872v2)|null|
|**2024-05-28**|**Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh**|Xiangjun Gao et.al.|Here is a summary of the key contributions from the abstract and introduction in a single sentence under 50 words:The paper proposes Mani-GS, a method that enables 3D Gaussian Splatting (3DGS) manipulation using a triangular mesh, achieving high-quality photo-realistic rendering while being highly controllable and scalable, with applications in 3D content creation.|[2405.17811v1](http://arxiv.org/abs/2405.17811v1)|null|
|**2024-05-27**|**F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting**|Xiangyu Sun et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose Factorized 3D Gaussian Splatting (F-3DGS), a novel approach that reduces storage requirements of 3D Gaussian Splatting by factorizing coordinates and features, achieving a 90% reduction in storage while maintaining comparable image quality and fast rendering speeds.|[2405.17083v2](http://arxiv.org/abs/2405.17083v2)|null|
|**2024-05-27**|**PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting**|Zipeng Wang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents Pyramidal 3D Gaussian Splatting (PyGS), a method that models large-scale scenes with hierarchical Gaussians and NeRF initialization, achieving higher fidelity and faster rendering performance, surpassing state-of-the-art approaches by over 400 times.|[2405.16829v3](http://arxiv.org/abs/2405.16829v3)|null|
|**2024-05-26**|**Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors**|Soumava Paul et.al.|The paper proposes SparseSplat360, a sparse-view 360-degree scene reconstruction method that employs pre-trained 2D diffusion models and a cascade of in-painting and artifact removal models to generate multi-view consistent scenes with coherent details.|[2405.16517v1](http://arxiv.org/abs/2405.16517v1)|null|
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|The key contributions from the paper's abstract and introduction are:* The introduction of Neural Elevation Models (NEMos), which adapt Neural Radiance Fields to a 2.5D continuous and differentiable terrain model.* The proposal of a novel method for jointly training a height field and radiance field within a NeRF framework, leveraging quantile regression.* The introduction of a path planning algorithm that performs gradient-based optimization of a continuous cost function for minimizing distance, slope changes, and control effort.Note: The abstract and introduction provide an overview of the paper, the methodology, and the contributions of the research.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes HDR-GS, a novel framework for 3D HDR imaging that uses a Dual Dynamic Range Gaussian point cloud model and Parallel Differentiable Rasterization processes to efficiently render novel HDR views and LDR images with controllable exposure time.|[2405.15125v2](http://arxiv.org/abs/2405.15125v2)|[link](https://github.com/caiyuanhao1998/hdr-gs)|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes GS-Hider, a novel steganography framework for 3D Gaussian Splatting (3DGS) that embeds 3D scenes and images into original 3DGS point cloud files in an invisible manner, ensuring robust security, high fidelity, and large capacity.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|The paper introduces an approach that addresses the limitations of Neural Radiance Fields (NeRFs) in rendering highly specular objects with view-dependent appearance, by incorporating ray tracing to synthesize consistent reflections of nearby and distant content, and achieves better performance and efficiency compared to prior methods.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents Neural Directional Encoding (NDE), a novel view-dependent appearance encoding for rendering specular objects, which combines feature-grid-based spatial encoding with cone-tracing to model high-frequency angular signals and capture complex reflections.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|The paper proposes a two-staged pipeline that normalizes images with varying lighting and shadow conditions to improve camera relocalization, utilizing a hash-encoded NeRF and a re-devised truncated dynamic low-pass filter and numerical gradient averaging technique to smoothen the process, achieving state-of-the-art results in camera relocalization under varying lighting conditions.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The authors propose a novel feed-forward framework, LDM, capable of generating high-quality, illumination-decoupled textured mesh from single images or text prompts within seconds, utilizing multi-view diffusion, transformer models, and gradient-based mesh optimization.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes JointRF, a novel end-to-end scheme that jointly optimizes dynamic NeRF representation and compression, achieving superior rate-distortion performance and eliminating the need for multi-stage training.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes DoGaussian, a distributed training method for 3D Gaussian Splatting (3DGS) to accelerate the training process of large-scale scenes, achieving 6+ times faster training time while maintaining rendering quality and state-of-the-art results.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes Gaussian Time Machine (GTM), a real-time rendering method that models time-dependent attributes of Gaussian primitives, achieving state-of-the-art rendering fidelity and 100 times faster rendering speed than NeRF-based methods while disentangling appearance changes.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|The paper introduces Motion-Based 3D Clothed Humans Synthesis (MOSS), a framework that employs kinematic information to achieve motion-aware Gaussian split on the human surface, overcoming limitations in single-view clothed human reconstruction by incorporating global motion information and addressing local occlusions.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The authors propose a novel method that enables the estimation of the 6D pose of an unknown target spacecraft relative to a monocular camera, extending the applicability of model-based Spacecraft Pose Estimation (SPE) methods to unknown targets without requiring the CAD model of the target, by leveraging Neural Radiance Fields (NeRF) to generate a large training set from a sparse collection of images depicting the target.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|Tianqi Liu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors present MVSGaussian, a generalizable Gaussian Splatting approach that efficiently reconstructs unseen scenes with real-time rendering speed and better view synthesis quality, utilizing Multi-View Stereo, pixel-aligned Gaussian representations, and a consistent aggregation strategy.|[2405.12218v1](http://arxiv.org/abs/2405.12218v1)|null|
|**2024-05-20**|**Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents**|Guanlin Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper provides a comprehensive overview of integrating neural radiance field (NeRF) and 3D Gaussian splatting (3D-GS) in 6G networks for efficient representation, transmission, and reconstruction of 3D contents, addressing challenges in distributed training, inference, and transmission over wireless networks.|[2405.12155v1](http://arxiv.org/abs/2405.12155v1)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|Fotios Logothetis et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents a novel multi-view photometric stereo method that leverages per-pixel intensity renderings, model point light attenuation, and explicit raytracing for cast shadows, achieving competitive reconstruction accuracy and demonstrating robustness to poor normals.|[2405.12057v1](http://arxiv.org/abs/2405.12057v1)|null|
|**2024-05-19**|**Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems**|Shengxiang Sun et.al.|Here is a summarized version of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a modified gradient-based texture optimization method to generate realistic-looking adversarial objects for autonomous driving systems, introducing an "Judge" to assess and enhance the realism of generated textures, which can be integrated with various strategies for optimizing the Judge.|[2405.11629v1](http://arxiv.org/abs/2405.11629v1)|null|
|**2024-05-19**|**R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless Environments**|Huiying Yang et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The authors propose a novel NeRF-based ray tracing method to model dynamic electromagnetic fields in RIS-enabled environments, enabling accurate characterization of signal propagation dynamics and efficient RIS deployment for improved communication efficiency.|[2405.11541v1](http://arxiv.org/abs/2405.11541v1)|null|
|**2024-05-18**|**MotionGS : Compact Gaussian Splatting SLAM by Motion Filter**|Xinli Guo et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes a novel 3DGS-based SLAM approach, MotionGS, integrating deep feature extraction, dual keyframe selection, and 3D Gaussian splatting, achieving high-fidelity scene representation, real-time tracking, and accurate mapping with reduced memory usage.|[2405.11129v1](http://arxiv.org/abs/2405.11129v1)|[link](https://github.com/antonio521/motiongs)|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|The paper provides a comprehensive overview of Large Language Models (LLMs) integrated with 3D spatial data, showcasing the unique advantages of LLMs in processing, understanding, and generating 3D data, and highlighting their potential to advance spatial comprehension and interaction within embodied artificial intelligence systems.|[2405.10255v1](http://arxiv.org/abs/2405.10255v1)|null|
|**2024-05-15**|**From NeRFs to Gaussian Splats, and Back**|Siming He et.al.|The key contributions from the paper's abstract and introduction are:* Developing a procedure to convert between implicit representations (Neural Radiance Fields, NeRFs) and explicit representations (Gaussian Splatting, GS) for scene understanding and rendering.* Achieving the best of both worlds by combining the advantages of NeRFs (superior PSNR, SSIM, and LPIPS on dissimilar views, and a compact representation) and GS (real-time rendering and ability for easily modifying the representation).* The computational cost of these conversions is minor compared to training the two from scratch.In summary, the paper presents a novel approach to convert between NeRFs and GS, enabling the benefits of both methods to be combined, which is particularly useful for robotics applications where there is a limited number of views.|[2405.09717v1](http://arxiv.org/abs/2405.09717v1)|[link](https://github.com/grasp-lyrl/nerftogsandback)|
|**2024-05-14**|**Dynamic NeRF: A Review**|Jinwei Lin et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:This review presents a comprehensive overview of Dynamic Neural Radiance Field (NeRF), a novel implicit method for 3D reconstruction and representation, highlighting its development history, key principles, and implementation methods, as well as comparisons and analysis of different features and research projects.|[2405.08609v1](http://arxiv.org/abs/2405.08609v1)|null|
