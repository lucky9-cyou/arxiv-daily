
### NeRF
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-27**|**F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting**|Xiangyu Sun et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose Factorized 3D Gaussian Splatting (F-3DGS), a novel approach that drastically reduces storage requirements while preserving image quality by factorizing 3D Gaussians and attributes using efficient decomposition techniques.|[2405.17083v1](http://arxiv.org/abs/2405.17083v1)|null|
|**2024-05-27**|**PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting**|Zipeng Wang et.al.|Here is a summarized sentence under 50 words:The paper presents Pyramidal 3D Gaussian Splatting, which addresses challenges of scaling 3D Gaussian Splatting to large-scale scenes by introducing pyramidal Gaussians and dynamic weighting, achieving a significant performance leap and rendering speedup over state-of-the-art NeRF-based methods.|[2405.16829v1](http://arxiv.org/abs/2405.16829v1)|null|
|**2024-05-26**|**Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors**|Soumava Paul et.al.|The paper presents SparseSplat360, a method that uses pretrained 2D diffusion models to improve sparse-view reconstruction of 360 scenes by fine-tuning and filling in missing details, achieving superior results on the Mip-NeRF360 dataset.|[2405.16517v1](http://arxiv.org/abs/2405.16517v1)|null|
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|The key contributions from the paper's abstract and introduction are:* The introduction of Neural Elevation Models (NEMos), which combine Neural Radiance Fields (NeRFs) with a height field to create a compact and differentiable terrain representation.* The proposal of a novel method for jointly training the NeRF and height field using quantile regression, which enables the generation of high-quality reconstructions of terrain from imagery.* The development of a path planning algorithm that leverages the continuous and differentiable nature of the height field to generate smoother paths for autonomous ground robots.These contributions address the limitations of traditional terrain representation methods, such as digital elevation models (DEMs), by providing a more lightweight and flexible representation of terrain that can be generated from imagery.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes High Dynamic Range Gaussian Splatting (HDR-GS), a novel framework for 3D HDR imaging, which outperforms state-of-the-art NeRF-based methods in HDR novel view synthesis, achieves 1000x inference speed, and requires 6.3% training time.|[2405.15125v2](http://arxiv.org/abs/2405.15125v2)|[link](https://github.com/caiyuanhao1998/hdr-gs)|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose a novel steganography framework, GS-Hider, for 3D Gaussian Splatting (3DGS) that embeds and extracts 3D scenes and images in an invisible manner, ensuring security, fidelity, capacity, and flexibility, and demonstrating its effectiveness in various applications.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|The authors introduce a novel approach that combines Neural Radiance Fields (NeRFs) with ray tracing to overcome limitations in rendering highly specular objects, allowing for accurate and efficient synthesis of reflective appearances in real-world scenes.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents Neural Directional Encoding (NDE), a novel method that efficiently encodes high-frequency directional signals for specular objects, enabling fast and accurate novel-view synthesis.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes a two-staged pipeline that normalizes images with varying lighting conditions for camera relocalization, utilizing a hash-encoded NeRF with a re-devised truncated dynamic low-pass filter and numerical gradient averaging technique for robust pose optimization.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes LDM, a novel feed-forward framework that generates high-quality textured meshes with decoupled illumination from text or single images in seconds, utilizing a multi-view diffusion model, transformer-based SDF predictor, and gradient-based mesh optimization layer.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|Here is a summary of the key contributions in a single sentence under 50 words:This paper proposes JointRF, a novel end-to-end joint optimization scheme for dynamic Neural Radiance Field representation and compression, achieving superior quality and compression efficiency.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose DoGaussian, a distributed training method for 3D Gaussian Splatting (3DGS) that accelerates training by 6+ times on large-scale scenes while maintaining high rendering quality, by decomposing scenes into blocks and using Alternating Direction Method of Multipliers (ADMM) for consensus.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|The key contributions are: Gaussian Time Machine (GTM) models time-dependent attributes of Gaussian primitives with discrete time embedding vectors, enabling accurate reconstruction of scenes with vastly differing weather and lighting conditions, and achieves state-of-the-art rendering fidelity and real-time rendering capability (100 times faster than NeRF-based methods) on three datasets.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The authors propose Motion-Based 3D Clothed Humans Synthesis (MOSS), a novel framework that incorporates kinematic information to achieve motion-aware 3D reconstruction of clothed humans, addressing limitations of previous methods that overlook the influence of motion on surface deformation.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|The paper's abstract and introduction key contributions are summarized in a single sentence under 50 words:This paper proposes a novel method that leverages Neural Radiance Fields (NeRFs) to enable off-the-shelf spacecraft pose estimation models to be applied on unknown targets, achieved by training a NeRF on a sparse set of images depicting the target spacecraft and generating a large, diverse dataset to train a pose estimation network.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|Tianqi Liu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors present MVSGaussian, a novel 3D Gaussian representation approach that efficiently reconstructs unseen scenes with real-time rendering and generalizability, leveraging Multi-View Stereo, pixel-aligned Gaussian rendering, and consistent aggregation for fast per-scene optimization.|[2405.12218v1](http://arxiv.org/abs/2405.12218v1)|null|
|**2024-05-20**|**Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents**|Guanlin Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper provides a comprehensive overview of integrating neural radiance field (NeRF) and 3D Gaussian splatting (3D-GS) in 6G networks for efficient representation, transmission, and reconstruction of 3D contents, addressing challenges in distributed training, rendering, and transmission.|[2405.12155v1](http://arxiv.org/abs/2405.12155v1)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|Fotios Logothetis et.al.|The key contributions from the paper's abstract and introduction are:* A novel multi-view photometric stereo method that leverages neural shape representations and learned renderers.* The method explicitly models point light attenuation and raytraces cast shadows to best approximate each point's incoming radiance.* The method optimizes a fully neural material renderer and achieves competitive reconstruction accuracy even with only 6 lights.* The method achieves state-of-the-art performance on the DiLiGenT-MV benchmark, outperforming classical approaches and other deep learning-based methods.|[2405.12057v1](http://arxiv.org/abs/2405.12057v1)|null|
|**2024-05-19**|**Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems**|Shengxiang Sun et.al.|The key contributions of the paper can be summarized as:The authors introduce a modified gradient-based texture optimization method to discover realistic-looking adversarial objects, which is integrated into an existing framework using a neural object renderer. They also propose four strategies for creating a reliable "Judge" to evaluate the realism of generated objects.|[2405.11629v1](http://arxiv.org/abs/2405.11629v1)|null|
|**2024-05-19**|**R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless Environments**|Huiying Yang et.al.|The key contributions of this paper are:* A novel modeling approach using Neural Radiance Fields (NeRF) to characterize the dynamics of electromagnetic fields in RIS-enabled wireless environments, enabling accurate modeling of signal propagation and RIS deployment.|[2405.11541v1](http://arxiv.org/abs/2405.11541v1)|null|
|**2024-05-18**|**MotionGS : Compact Gaussian Splatting SLAM by Motion Filter**|Xinli Guo et.al.|The paper proposes a novel 3D Gaussian Splatting (3DGS)-based Simultaneous Localization and Mapping (SLAM) approach, MotionGS, which integrates deep feature extraction, dual keyframe selection, and 3DGS, achieving high-fidelity scene representation and real-time tracking with reduced memory usage.|[2405.11129v1](http://arxiv.org/abs/2405.11129v1)|[link](https://github.com/antonio521/motiongs)|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|Here is a summary of the key contributions in a single sentence under 50 words:This survey provides a comprehensive overview of methodologies enabling large language models to process, understand, and generate 3D data, highlighting their potential to advance spatial comprehension and interaction in embodied AI systems, while emphasizing the need for novel approaches to harness their full potential.|[2405.10255v1](http://arxiv.org/abs/2405.10255v1)|null|
|**2024-05-15**|**From NeRFs to Gaussian Splats, and Back**|Siming He et.al.|Here is a summary of the key contributions from the abstract and introduction:**Key Contributions:**1. Developed a procedure to convert between implicit scene representations (Neural Radiance Fields, NeRFs) and explicit scene representations (Gaussian Splatting, GS).2. Achieved the best of both worlds: superior rendering quality and OS on dissimilar views, and a compact representation.3. Converted NeRFs to GS and vice versa with a minor computational cost compared to training from scratch.**Summary:** The paper introduces a method to convert between implicit scene representations (NeRFs) and explicit scene representations (GS) for 3D scene understanding in robotics. The approach allows for both accurate rendering and compact representation of the scene, with a minor computational cost.|[2405.09717v1](http://arxiv.org/abs/2405.09717v1)|[link](https://github.com/grasp-lyrl/nerftogsandback)|
|**2024-05-14**|**Dynamic NeRF: A Review**|Jinwei Lin et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:This review provides a comprehensive analysis of the development and implementation principles of Dynamic Neural Radiance Field (NeRF), highlighting its potential applications and techniques, with a focus on the key methods and design principles from 2021 to 2023.|[2405.08609v1](http://arxiv.org/abs/2405.08609v1)|null|
|**2024-05-13**|**Synergistic Integration of Coordinate Network and Tensorial Feature for Improving Neural Radiance Fields from Sparse Inputs**|Mingyu Kim et.al.|Here is a summary of the key contributions mentioned in the paper's abstract and introduction:* The proposed method synergistically integrates the multi-plane representation with a coordinate-based network to improve the performance of static and dynamic NeRFs from sparse inputs.* The coordinate-based network handles low-frequency context, while the multi-plane features capture fine-grained details, achieving three main benefits: (1) reduced sensitivity to hyperparameters, (2) stable training through gradual changes in spectral biases, and (3) efficient parameter allocation.* The proposed method is comparable to multi-plane encoding with high denoising penalties in static NeRFs and outperforms baselines in dynamic NeRFs from sparse inputs.* The reduction in the number of parameters by skipping the allocation of a spatial low-resolution grid and replacing it with coordinate-based features does not compromise performance.* The proposed method is simple yet powerful, allowing for faster convergence and better performance compared to previous approaches.|[2405.07857v1](http://arxiv.org/abs/2405.07857v1)|[link](https://github.com/mingyukim87/synergynerf)|
|**2024-05-12**|**Point Resampling and Ray Transformation Aid to Editable NeRF Models**|Zhenyang Li et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes an implicit ray transformation strategy and a differentiable neural-point resampling module to address object movement and empty regions in NeRF-aided editing tasks, achieving state-of-the-art performance in object removal and inpainting tasks.|[2405.07306v1](http://arxiv.org/abs/2405.07306v1)|null|
|**2024-05-12**|**Hologram: Realtime Holographic Overlays via LiDAR Augmented Reconstruction**|Ekansh Agrawal et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The paper proposes a method for generating real-time 3D facial reconstructions using LiDAR augmented 3D reconstruction. The contributions are:1. Three approaches for generating real-time 3D facial reconstructions, each using a different method for estimating depth: monocular depth estimation, LiDAR + TrueDepth fusion, and template modeling.2. A hybrid approach that combines depth estimation and LiDAR sensing to achieve high-fidelity facial reconstructions in real-time.3. A constraint-based methodology that ensures portability, instantaneous reconstruction, and high fidelity of the rendered voxel grid.Note that the introduction highlights the limitations of previous methods for real-time 3D reconstruction, such as the need for highly calibrated scenes, steep computation costs, and failure to render dynamic scenes. The proposed approach aims to overcome these limitations and achieve high-fidelity facial reconstructions in real-time.|[2405.07178v1](http://arxiv.org/abs/2405.07178v1)|null|
|**2024-05-11**|**TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural Radiance Field Optimization**|Zhen Tan et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The authors propose Truncated Depth NeRF (TD-NeRF), a novel approach to jointly optimize camera poses and Neural Radiance Fields (NeRF) using monocular depth priors, which improves pose estimation accuracy and generates more accurate depth geometry, surpassing prior works.|[2405.07027v1](http://arxiv.org/abs/2405.07027v1)|[link](https://github.com/nubot-nudt/td-nerf)|
|**2024-05-10**|**LIVE: LaTex Interactive Visual Editing**|Jinwei Lin et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:This paper proposes LIVE, a novel design method to create interactive LaTex graphic items, enabling more informative and interactive figures and tables, which can aid in writing traditional papers, especially review papers, with improved vitality and performance factors.|[2405.06762v1](http://arxiv.org/abs/2405.06762v1)|null|
|**2024-05-10**|**SketchDream: Sketch-based Text-to-3D Generation and Editing**|Feng-Lin Liu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:* The paper proposes SketchDream, a method for sketch-based text-to-3D generation and editing of photo-realistic contents.* The method addresses the challenge of synthesizing realistic 3D contents from sparse 2D sketches and textual inputs by completing missing appearance details and extending single-view information into the 3D space.* The key contributions of the paper are:	+ The first sketch-based text-to-3D generation and editing method that generates high-quality 3D objects under generalized categories and supports detailed editing of reconstructed or generated NeRFs.	+ A sketch-based multi-view image generation diffusion model that utilizes depth guidance to create spatial correspondence and a 3D attention control module to ensure 3D consistency.	+ A coarse-to-fine editing framework that generates high-quality editing results with a local rendering strategy.|[2405.06461v2](http://arxiv.org/abs/2405.06461v2)|null|
