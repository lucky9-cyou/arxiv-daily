
### Gaussian Splatting
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-29**|**NPGA: Neural Parametric Gaussian Avatars**|Simon Giebenhain et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction under 50 words:The paper proposes Neural Parametric Gaussian Avatars (NPGA), a data-driven method that creates high-fidelity, controllable digital versions of human heads with expressive facial dynamics, improving performance on self-reenactment tasks and exhibiting accurate avatar reconstruction from real-world videos.|[2405.19331v1](http://arxiv.org/abs/2405.19331v1)|null|
|**2024-05-29**|**DGD: Dynamic 3D Gaussians Distillation**|Isaac Labe et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:This paper introduces DGD, a unified 3D representation for both appearance and semantics of a dynamic 3D scene, allowing for the tracking of diverse 3D semantic entities specified using a simple and intuitive interface, and achieving high-quality results that are fast to render for a variety of real and synthetic scenes.|[2405.19321v1](http://arxiv.org/abs/2405.19321v1)|null|
|**2024-05-29**|**Mitigating Disparate Impact of Differential Privacy in Federated Learning through Robust Clustering**|Saber Malekmohammadi et.al.|The paper proposes a novel approached called Robust Clustered Differential Privacy Federated Learning (RC-DPFL) that effectively identifies client clusters in heterogeneous settings while maintaining high accuracy with differential privacy guarantees, addressing performance disparities particularly affecting minority groups.|[2405.19272v1](http://arxiv.org/abs/2405.19272v1)|null|
|**2024-05-29**|**Weak Generative Sampler to Efficiently Sample Invariant Distribution of Stochastic Differential Equation**|Zhiqiang Cai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose a framework for sampling from the invariant distribution of an Ito diffusion process using a weak generative sampler, which is trained using a loss function based on the weak form of the Fokker-Planck equation and normalizing flows.|[2405.19256v1](http://arxiv.org/abs/2405.19256v1)|null|
|**2024-05-29**|**The Kinetic Ion-Temperature-Gradient-driven instability and its localisation**|Eduardo Rodriguez et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors develop a simple, semi-analytical model that captures the key features of Ion Temperature Gradient (ITG) driven localized modes, including wave-particle and magnetic drift resonant effects, field-line dependence, and multiple maxima in the spectra, providing a framework for interpreting numerical simulations.|[2405.19235v1](http://arxiv.org/abs/2405.19235v1)|null|
|**2024-05-29**|**$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation**|Weitian Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes $E^3$Gen, a novel avatar generation method that addresses challenges in generating efficient, expressive, and editable digital avatars by combining 3D Gaussian representation with a generative diffusion model and part-aware deformation module.|[2405.19203v1](http://arxiv.org/abs/2405.19203v1)|null|
|**2024-05-29**|**Model-independent cosmological inference post DESI DR1 BAO measurements**|Purba Mukherjee et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper implements Gaussian process regression to reconstruct the universe's expansion history using Pantheon-Plus SN-Ia compilation and BAO measurements, constraining dark energy scenarios and finding hints of quintessence-like dark energy and a phantom-crossing, and providing model-agnostic constraints on the Hubble parameter and dark energy density.|[2405.19178v1](http://arxiv.org/abs/2405.19178v1)|null|
|**2024-05-29**|**DeepOKAN: Deep Operator Network Based on Kolmogorov Arnold Networks for Mechanics Problems**|Diab W. Abueidda et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper introduces DeepOKAN, a neural operator that uses Kolmogorov-Arnold networks (KANs) and Gaussian radial basis functions (RBFs) to approximate physical operators, presenting improved performance and efficiency compared to traditional architectures.|[2405.19143v1](http://arxiv.org/abs/2405.19143v1)|null|
|**2024-05-29**|**Efficient Black-box Adversarial Attacks via Bayesian Optimization Guided by a Function Prior**|Shuyu Cheng et.al.|Here is a summarized version of the key contributions from the paper's abstract and introduction:The paper proposes Prior-guided Bayesian Optimization (P-BO), a novel algorithm that leverages a surrogate model as a global function prior to improve the efficiency of black-box adversarial attacks. The algorithm models the attack objective with a Gaussian process and updates the posterior distribution based on the observed values of the objective function. Theoretical analysis shows that P-BO's performance may be affected by a bad prior, and an adaptive integration strategy is proposed to automatically adjust the coefficient on the function prior to minimize the regret bound. Experimental results on various models, including image classifiers and large vision-language models, demonstrate the superiority of P-BO in reducing queries and improving attack success rates compared to state-of-the-art black-box attacks.|[2405.19098v1](http://arxiv.org/abs/2405.19098v1)|[link](https://github.com/yibo-miao/pbo-attack)|
|**2024-05-29**|**Implementing arbitrary multi-mode continuous-variable quantum gates with fixed non-Gaussian states and adaptive linear optics**|Fumiya Hanamura et.al.|The key contributions from the paper's abstract and introduction are:* The authors propose a measurement-based method to implement general, multi-mode, and higher-order non-Gaussian gates using fixed non-Gaussian ancillary states and adaptive linear optics.* The method allows for a more resource-efficient and experimentally feasible implementation of multi-mode gates that are important for various applications in optical quantum technology.* The authors demonstrate the efficiency of their method by applying it to several important examples, including the cubic quantum non-demolition gate and the CV Toffoli gate, and their higher-order extensions.The paper also discusses the decomposition of Hamiltonians into quadrature gates, which is a key step in the implementation of non-Gaussian gates. The authors show that this decomposition can be done in a way that reduces the number of gates required, making the implementation more efficient.The paper also introduces the concept of Chow decomposition, which is a technique for decomposing polynomials into simpler forms. The authors use this technique to decompose the polynomial representing the non-Gaussian gate into a sum of monomials, which can then be implemented using linear optics and measurement-based operations.Overall, the paper presents a new method for implementing non-Gaussian gates in optical quantum technology, which has the potential to be more efficient and experimentally feasible than existing methods.|[2405.19067v1](http://arxiv.org/abs/2405.19067v1)|null|
|**2024-05-29**|**Towards a computationally-efficient follow-up pipeline for blind continuous gravitational-wave searches**|Lorenzo Mirasola et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The paper presents a novel framework to evaluate the effectiveness of stochastic continuous gravitational-wave (CW) follow-ups, allowing for a ten-fold reduction in computing cost and simplifying the setup of multi-stage follow-ups. The framework is applied to simulated all-sky searches for unknown neutron stars, both isolated and in binary systems, using Gaussian noise and realistic data from the O3 LIGO-Virgo-KAGRA observing run.|[2405.18934v1](http://arxiv.org/abs/2405.18934v1)|null|
|**2024-05-29**|**Opinion-Unaware Blind Image Quality Assessment using Multi-Scale Deep Feature Statistics**|Zhangkai Ni et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes a Multi-scale Deep Feature Statistics (MDFS) model for opinion-unaware blind image quality assessment (BIQA) that integrates deep features from pre-trained visual models with a statistical analysis model, eliminating the need for human rating data and improving training efficiency.|[2405.18790v1](http://arxiv.org/abs/2405.18790v1)|[link](https://github.com/eezkni/mdfs)|
|**2024-05-29**|**LP-3DGS: Learning to Prune 3D Gaussian Splatting**|Zhaoliang Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The proposed learning-to-prune 3DGS (LP-3DGS) method learns a trainable binary mask to find the optimal pruning ratio automatically, eliminating the need for manual tuning and achieving efficient and high-quality novel view synthesis.|[2405.18784v1](http://arxiv.org/abs/2405.18784v1)|null|
|**2024-05-29**|**Principled Probabilistic Imaging using Diffusion Models as Plug-and-Play Priors**|Zihui Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose a Markov chain Monte Carlo algorithm, PnP-DM, that leverages diffusion models as priors for posterior sampling in Bayesian inverse problems, eliminating the need for approximations and offering more accurate reconstructions and posterior estimation compared to existing methods.|[2405.18782v1](http://arxiv.org/abs/2405.18782v1)|null|
|**2024-05-29**|**LMO-DP: Optimizing the Randomization Mechanism for Differentially Private Fine-Tuning (Large) Language Models**|Qin Yang et.al.|The key contributions of the paper are:1. The proposal of Language Model-based Optimal Differential Privacy (LMO-DP), a non-Gaussian mechanism that generates sub-optimal noise for ensuring strong DP guarantees.2. The design of a novel noise generation mechanism, which is a two-fold mixture distribution derived from an optimal differential privacy framework (R2DP [14]).3. The introduction of a novel offline optimal noise search method to efficiently derive the sub-optimal DP that significantly reduces the noise magnitude.4. The demonstration of the effectiveness of LMO-DP in achieving strong DP guarantees, outperforming existing methods in fine-tuning large language models with strong privacy guarantees.|[2405.18776v1](http://arxiv.org/abs/2405.18776v1)|null|
|**2024-05-29**|**Deep Bayesian Filter for Bayes-faithful Data Assimilation**|Yuta Tarumi et.al.|Here is a single sentence summarizing the key contributions of the paper:The paper proposes Deep Bayesian Filtering (DBF), a novel methodology for non-linear state space models (SSMs) that learns Gaussian inverse observation operators and approximates posteriors on a new latent space, allowing for recursive computation of posteriors without accumulating Monte-Carlo sampling errors.|[2405.18674v1](http://arxiv.org/abs/2405.18674v1)|null|
|**2024-05-28**|**Biclustering a dataset using photonic quantum computing**|Ajinkya Borle et.al.|The authors' key contributions include using boson sampling and Gaussian boson sampling (GBS) to identify biclusters in datasets and proposing a heuristic algorithm for finding clusters using GBS by converting the dataset to a bipartite graph and running GBS on it, with promising preliminary simulation results.|[2405.18622v1](http://arxiv.org/abs/2405.18622v1)|null|
|**2024-05-28**|**From Conformal Predictions to Confidence Regions**|Charles Guille-Escuret et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper introduces CCR, a novel approach that combines conformal prediction intervals for model outputs to establish confidence regions for model parameters, providing finite-sample validity under minimal assumptions on noise and data distribution.|[2405.18601v1](http://arxiv.org/abs/2405.18601v1)|null|
|**2024-05-28**|**Exploring the transition between Quantum and Classical Mechanics**|E. Aldo Arroyo et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:* The paper investigates the transition from quantum to classical mechanics using a one-dimensional free particle model.* The authors find that the quantum probability density derived from the Schrödinger equation for a Gaussian initial wave function coincides with the classical normal distribution of final positions obtained by convolving Gaussian distributions of initial positions and velocities.* However, the authors also show that when the initial state is a superposition of Gaussian wave packets, quantum interference leads to a divergence between classical and quantum descriptions.* To recover the classical distribution from the quantum one, the authors propose a novel method using truncated Fourier analysis, which effectively removes high-frequency components associated with quantum interference.* The authors argue that this method is related to decoherence, a fundamental process in quantum mechanics that explains the transition from the quantum to the classical world.Overall, the paper provides a comprehensive analysis of the classical-quantum correspondence and the mechanisms underlying the emergence of classicality from quantum systems.|[2405.18564v1](http://arxiv.org/abs/2405.18564v1)|null|
|**2024-05-28**|**Covariance Operator Estimation via Adaptive Thresholding**|Omar Al-Ghattas et.al.|The key contributions from the paper's abstract and introduction are:* The paper proposes a covariance operator estimator that adaptively thresholds the sample covariance function using an estimate of the variance components, which is shown to outperform universal threshold and sample covariance estimators in nonstationary settings.* The paper studies sparse covariance operator estimation in an infinite-dimensional function space setting, where the marginal variance varies widely in the domain and the correlation lengthscale is small.* The paper establishes a bound on the operator norm error of the adaptive threshold estimator in terms of two dimension-free quantities: the sparsity level and the expected supremum of the normalized field.* The paper demonstrates the advantage of adaptive threshold estimators over universal threshold and sample covariance estimators in nonstationary settings through numerical simulations.The main contributions are:* A new covariance operator estimator that adaptively thresholds the sample covariance function using an estimate of the variance components.* A bound on the operator norm error of the adaptive threshold estimator in terms of the sparsity level and the expected supremum of the normalized field.* A comparison of the adaptive threshold estimator with universal threshold and sample covariance estimators, showing that the adaptive threshold estimator outperforms the others in nonstationary settings.* A novel theory that allows for covariance models with unbounded marginal variance functions.* Numerical simulations that demonstrate the effectiveness of the adaptive threshold estimator in nonstationary settings.|[2405.18562v1](http://arxiv.org/abs/2405.18562v1)|null|
|**2024-05-28**|**Unisolvence of unsymmetric random Kansa collocation by Gaussians and other analytic RBF vanishing at infinity**|Alvise Sommariva et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proves the almost sure invertibility of unsymmetric random Kansa collocation matrices for the Poisson equation with Dirichlet boundary conditions, using a class of analytic RBFs that include Gaussians, Generalized Inverse MultiQuadrics, and Matern RBFs.|[2405.18550v1](http://arxiv.org/abs/2405.18550v1)|null|
|**2024-05-28**|**Star Stream Velocity Distributions in CDM and WDM Galactic Halos**|Raymond G. Carlberg et.al.|The paper explores the impact of dark matter subhalos on the velocities of stars in thin stellar streams, finding that the distribution of random velocities can be modeled as a Gaussian core with exponential wings, which increase in width as the number of subhalos increases, and provides observational prospects for constraining the nature of galactic dark matter.|[2405.18522v1](http://arxiv.org/abs/2405.18522v1)|null|
|**2024-05-28**|**Effective phase diffusion for spin phase evolution under random nonlinear magnetic field**|Guoxing Lin et.al.|The paper presents a general theoretical description of spin self-diffusion under nonlinear gradient fields, which extends the effective phase diffusion method for linear gradient fields. The main contributions can be summarized as:1. Proposing a method to describe spin diffusion under nonlinear gradient fields, including parabolic and cubic fields.2. Developing a theoretical framework to study phase evolutions in non-linear gradient fields, which shows that there are three types of phase evolutions: phase diffusion, float phase evolution, and shift-based on the starting position.3. Deriving theoretical expressions for phase variance and corresponding NMR signal attenuation, which indicate that signal attenuation obeys Gaussian, Lorentzian, or Mittag-Leffler function attenuations at different times.4. Showing that the proposed method can handle random-order nonlinear gradient fields and simulate diffusion under various magnetic field gradients.5. Comparing the proposed method with existing theories and simulations, demonstrating its versatility and applicability to various experimental conditions.6. Discussing the potential applications of the method in NMR and MRI, including the possibility of using it to measure diffusion coefficients and improve imaging resolution.The key points are:* Nonlinear gradient fields are common in experimental settings, and a new theoretical approach is needed to describe spin diffusion in such fields.* The proposed method is versatile and can be applied to a wide range of experimental conditions.* The method is applicable to both parabolic and cubic fields, which is important for understanding experimental data.* The proposed method can handle random-order nonlinear gradient fields, making it a powerful tool for interpreting experimental results.* The method has potential applications in NMR and MRI, including the measurement of diffusion coefficients and the improvement of imaging resolution.|[2405.18514v1](http://arxiv.org/abs/2405.18514v1)|null|
|**2024-05-28**|**Classifying Overlapping Gaussian Mixtures in High Dimensions: From Optimal Classifiers to Neural Nets**|Khen Cohen et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors derive closed-form expressions for Bayes optimal decision boundaries in binary classification of high-dimensional overlapping Gaussian mixture models, and empirically demonstrate that deep neural networks trained on real-world data approximate these optimal classifiers.|[2405.18427v1](http://arxiv.org/abs/2405.18427v1)|null|
|**2024-05-28**|**GFlow: Recovering 4D World from Monocular Video**|Shizun Wang et.al.|The key contributions from the paper's abstract and introduction are:* The introduction of "AnyV4D", a task that aims to reconstruct 4D dynamic scenes from a single, uncalibrated monocular video input without any camera parameters or prior knowledge of the scene.* The proposal of a new framework called GFlow, which leverages the explicit representation power of 3D Gaussian Splatting to reconstruct dynamic scenes and track points across frames in 3D world coordinates.* The ability of GFlow to achieve high-quality reconstruction and tracking without relying on prior knowledge of the camera pose or scene dynamics.* The versatility of GFlow, which enables rendering novel views of a video scene, segmenting moving objects, and conducting scene-level or object-level editing.|[2405.18426v1](http://arxiv.org/abs/2405.18426v1)|null|
|**2024-05-28**|**3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting**|Qihang Zhang et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes 3DitScene, a unified scene editing framework that leverages language-guided disentangled Gaussian Splatting to enable seamless editing from 2D to 3D, allowing precise control over scene composition and individual objects with both global and local granularity.|[2405.18424v1](http://arxiv.org/abs/2405.18424v1)|null|
|**2024-05-28**|**3D StreetUnveiler with Semantic-Aware 2DGS**|Jingwei Xu et.al.|The key contributions from the paper's abstract and introduction are the introduction of StreetUnveiler, a method that learns a 3D representation of an empty street from crowded in-car camera observations, and its ability to remove unwanted objects and maintain temporal consistency through a time-reversal inpainting framework.|[2405.18416v1](http://arxiv.org/abs/2405.18416v1)|null|
|**2024-05-28**|**Short-time Fokker-Planck propagator beyond the Gaussian approximation**|Julian Kappler et.al.|The key contributions of the paper can be summarized as follows: The authors present a perturbation approach to calculate the short-time propagator of the one-dimensional Fokker-Planck equation to arbitrary order in the time increment, preserving probability exactly and allowing for the evaluation of expectation values of analytical observables to arbitrary accuracy. The approach is validated against an explicit multiplicative-noise system with available analytical solution, and is shown to outperform the widely used Gaussian approximation of the short-time propagator.|[2405.18381v1](http://arxiv.org/abs/2405.18381v1)|null|
|**2024-05-28**|**Improving Linear System Solvers for Hyperparameter Optimisation in Iterative Gaussian Processes**|Jihao Andreas Lin et.al.|The key contributions of the paper are:* A pathwise gradient estimator, which reduces the required number of solver iterations and amortises the computational cost of making predictions.* Warm starting linear system solvers with the solution from the previous step, which leads to faster solver convergence at the cost of negligible bias.* Early stopping linear system solvers after a limited computational budget, which synergises with warm starting and allows solver progress to accumulate over multiple marginal likelihood steps.These techniques provide speed-ups of up to 72× when solving to tolerance, and decrease the average residual norm by up to 7× when stopping early.The paper also presents empirical results on various datasets, including UCI regression datasets, and shows that the proposed methods can improve the performance and efficiency of Gaussian process regression and marginal likelihood optimisation.|[2405.18457v1](http://arxiv.org/abs/2405.18457v1)|null|
|**2024-05-28**|**Warm Start Marginal Likelihood Optimisation for Iterative Gaussian Processes**|Jihao Andreas Lin et.al.|Here are the key contributions from the paper's abstract and introduction:* The paper considers iterative methods for Gaussian processes, which use iterative linear system solvers to approximate marginal likelihood gradients to a specified numerical precision.* The authors introduce a three-level hierarchy of marginal likelihood optimisation for iterative Gaussian processes, which includes:	1. An iterative optimiser (e.g. gradient descent) to maximise the marginal likelihood.	2. A gradient estimator (e.g. Hutchinson's trace estimator) to estimate the gradient of the marginal likelihood.	3. A linear system solver (e.g. conjugate gradients) to solve the systems of linear equations required to compute the gradient.* The authors propose to amortise computations by reusing solutions of linear system solvers as initialisations in the next step, providing a "warm start".* They show that the computational costs of the algorithm are dominated by solving sequential batches of large positive-definite systems of linear equations.The main contributions of the paper are:1. A novel approach to Gaussian process optimisation that allows for a trade-off between compute time and accuracy.2. A warm start technique that reduces the computational cost of the algorithm by reusing previous solutions.3. An evaluation of the warm start technique on several regression tasks, showing that it achieves similar results to the conventional procedure while providing significant speed-ups.The paper also presents experimental results demonstrating the effectiveness of the warm start technique and the efficiency of the algorithm.|[2405.18328v1](http://arxiv.org/abs/2405.18328v1)|null|
