
### Gaussian Splatting
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-29**|**NPGA: Neural Parametric Gaussian Avatars**|Simon Giebenhain et.al.|The paper proposes Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings, which significantly outperforms the previous state-of-the-art avatars on the self-reenactment task by 2.6 PSNR and demonstrates accurate animation capabilities from real-world monocular videos.|[2405.19331v1](http://arxiv.org/abs/2405.19331v1)|null|
|**2024-05-29**|**DGD: Dynamic 3D Gaussians Distillation**|Isaac Labe et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents DGD, a unified 3D representation that jointly optimizes appearance, semantic, and geometric properties for dynamic scenes, enabling novel view synthesis, segmentation, and tracking of 3D semantic entities in real-time.|[2405.19321v1](http://arxiv.org/abs/2405.19321v1)|null|
|**2024-05-29**|**Mitigating Disparate Impact of Differential Privacy in Federated Learning through Robust Clustering**|Saber Malekmohammadi et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The authors propose a novel clustered differentially private federated learning (DPFL) algorithm that effectively identifies clients' clusters in heterogeneous settings, maintaining high accuracy with DP guarantees, and mitigates disparate impact and noise through a hybrid clustering approach and batch size optimization.|[2405.19272v1](http://arxiv.org/abs/2405.19272v1)|null|
|**2024-05-29**|**Weak Generative Sampler to Efficiently Sample Invariant Distribution of Stochastic Differential Equation**|Zhiqiang Cai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The authors propose a framework that employs a weak generative sampler (WGS) to directly generate independent and identically distributed (iid) samples from a transformation map derived from the stationary Fokker-Planck equation. Their method does not require computing the Jacobian determinant or invertibility of the transformation map. The authors' framework uses a loss function based on the weak form of the Fokker-Planck equation and normalizing flows to characterize the invariant distribution and facilitate sample generation from the base distribution. The key contributions are:* A new method for sampling from the invariant distribution of a stochastic differential equation (SDE) using a weak generative sampler (WGS)* A loss function based on the weak form of the Fokker-Planck equation to characterize the invariant distribution* The use of normalizing flows to facilitate sample generation from the base distribution* A framework that does not require computing the Jacobian determinant or invertibility of the transformation map|[2405.19256v1](http://arxiv.org/abs/2405.19256v1)|null|
|**2024-05-29**|**The Kinetic Ion-Temperature-Gradient-driven instability and its localisation**|Eduardo Rodriguez et.al.|Here is a summarized version of the paper's abstract and introduction in a single sentence under 50 words:The authors develop a simplified model for the Ion Temperature Gradient (ITG) mode, retaining wave-particle and magnetic drift resonant effects, and capturing the field-line dependence of the electrostatic potential, providing a semi-analytical formula for the mode's spectrum, capturing multiple maxima observed in gyrokinetic simulations.|[2405.19235v1](http://arxiv.org/abs/2405.19235v1)|null|
|**2024-05-29**|**$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation**|Weitian Zhang et.al.|The paper proposes a novel method called E^3Gen for efficient, expressive, and editable digital avatar generation, which addresses the challenges of unstructured 3D Gaussian representations and expressive animation in generative settings.|[2405.19203v1](http://arxiv.org/abs/2405.19203v1)|null|
|**2024-05-29**|**Model-independent cosmological inference post DESI DR1 BAO measurements**|Purba Mukherjee et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper implements Gaussian process regression to reconstruct the universe's expansion history, using SN-Ia and BAO data, and finds evidence for a quintessence-like dark energy scenario, while also presenting model-agnostic constraints on H0 and the total equation of state.|[2405.19178v1](http://arxiv.org/abs/2405.19178v1)|null|
|**2024-05-29**|**DeepOKAN: Deep Operator Network Based on Kolmogorov Arnold Networks for Mechanics Problems**|Diab W. Abueidda et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper introduces DeepOKAN, a new neural operator that uses Kolmogorov-Arnold networks (KANs) with Gaussian radial basis functions (RBFs) in both the branch and trunk, outperforming traditional DeepONets in terms of computational speed and accuracy for certain mechanics problems.|[2405.19143v1](http://arxiv.org/abs/2405.19143v1)|null|
|**2024-05-29**|**Efficient Black-box Adversarial Attacks via Bayesian Optimization Guided by a Function Prior**|Shuyu Cheng et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The paper proposes a Prior-guided Bayesian Optimization (P-BO) algorithm that leverages a surrogate model as a function prior in black-box adversarial attacks. The algorithm models the attack objective with a Gaussian process whose mean function is initialized as the surrogate model's loss. The paper also proposes an adaptive integration strategy to automatically adjust a coefficient on the function prior by minimizing the regret bound. The P-BO algorithm is shown to be effective in reducing queries and improving attack success rates compared to state-of-the-art black-box attacks.|[2405.19098v1](http://arxiv.org/abs/2405.19098v1)|[link](https://github.com/yibo-miao/pbo-attack)|
|**2024-05-29**|**Implementing arbitrary multi-mode continuous-variable quantum gates with fixed non-Gaussian states and adaptive linear optics**|Fumiya Hanamura et.al.|The key contributions from the paper's abstract and introduction can be summarized in a single sentence under 50 words:The paper proposes a measurement-based method to directly implement general, multi-mode, and higher-order non-Gaussian gates using fixed non-Gaussian ancillary states and adaptive linear optics, which allows for more resource-efficient and experimentally feasible implementation of important multi-mode gates.|[2405.19067v1](http://arxiv.org/abs/2405.19067v1)|null|
|**2024-05-29**|**Towards a computationally-efficient follow-up pipeline for blind continuous gravitational-wave searches**|Lorenzo Mirasola et.al.|Here is a single sentence summarizing the contributions from the paper's abstract and introduction:The authors develop a novel framework to evaluate the effectiveness of stochastic continuous gravitational-wave follow-ups, achieving a ten-fold reduction in computing cost, and apply it to simulated and real O3 Advanced LIGO data, demonstrating a significant sensitivity increase in all-sky searches.|[2405.18934v1](http://arxiv.org/abs/2405.18934v1)|null|
|**2024-05-29**|**Opinion-Unaware Blind Image Quality Assessment using Multi-Scale Deep Feature Statistics**|Zhangkai Ni et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper proposes the Multi-scale Deep Feature Statistics (MDFS) model, which integrates deep features from pre-trained visual models with a statistical analysis model to achieve opinion-unaware blind image quality assessment (OU-BIQA) without relying on human rating data, exhibiting superior consistency with human visual perception and improved generalizability.|[2405.18790v1](http://arxiv.org/abs/2405.18790v1)|[link](https://github.com/eezkni/mdfs)|
|**2024-05-29**|**LP-3DGS: Learning to Prune 3D Gaussian Splatting**|Zhaoliang Zhang et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper proposes a learning-to-prune 3D Gaussian Splatting (LP-3DGS) method that automatically learns an optimal pruning ratio for each scene, eliminating the need for manual tuning, by leveraging a differentiable Gumbel-Sigmoid mask function and importance scores from prior works.|[2405.18784v1](http://arxiv.org/abs/2405.18784v1)|null|
|**2024-05-29**|**Principled Probabilistic Imaging using Diffusion Models as Plug-and-Play Priors**|Zihui Wu et.al.|The paper proposes a Markov chain Monte Carlo algorithm, Plug-and-Play Diffusion Models (PnP-DM), for posterior sampling in Bayesian inverse problems, which leverages diffusion models (DMs) as priors to rigorously solve the denoising problem without approximations. The method incorporates DMs in a principled way, enabling the use of a wide range of pre-trained DMs through a unified interface.|[2405.18782v1](http://arxiv.org/abs/2405.18782v1)|null|
|**2024-05-29**|**LMO-DP: Optimizing the Randomization Mechanism for Differentially Private Fine-Tuning (Large) Language Models**|Qin Yang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose Language Model-based Optimal Differential Privacy (LMO-DP), a novel and non-Gaussian mechanism for privately fine-tuning large language models, achieving strong privacy guarantees and significantly boosting accuracy and convergence rates.|[2405.18776v1](http://arxiv.org/abs/2405.18776v1)|null|
|**2024-05-29**|**Deep Bayesian Filter for Bayes-faithful Data Assimilation**|Yuta Tarumi et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The proposed Deep Bayesian Filtering (DBF) methodology develops a novel, Bayes-faithful approach for data assimilation, constraining posteriors to remain Gaussian through learnable neural networks, and outperforms existing methods in various tasks and conditions.|[2405.18674v1](http://arxiv.org/abs/2405.18674v1)|null|
|**2024-05-28**|**Biclustering a dataset using photonic quantum computing**|Ajinkya Borle et.al.|Here is a summary of the key contributions in one sentence under 50 words:The paper proposes the use of boson and Gaussian boson sampling for biclustering, introducing techniques to embed datasets as unitary matrices and develop simulated annealing and Autonne-Takagi decomposition methods to find biclusters with promising simulation results.|[2405.18622v1](http://arxiv.org/abs/2405.18622v1)|null|
|**2024-05-28**|**From Conformal Predictions to Confidence Regions**|Charles Guille-Escuret et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors introduce CCR, a novel approach that combines conformal prediction intervals for model outputs to establish confidence regions for model parameters, providing finite-sample guarantees under minimal assumptions and applicability to various models and noise types.|[2405.18601v1](http://arxiv.org/abs/2405.18601v1)|null|
|**2024-05-28**|**Exploring the transition between Quantum and Classical Mechanics**|E. Aldo Arroyo et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The key contributions are:* Investigation of the transition from quantum to classical mechanics using a one-dimensional free particle model* Comparison of the classical and quantum probability distributions, finding that they coincide in the case of single Gaussian initial conditions* Introduction of a novel method to recover the classical probability density from the quantum probability density using truncated Fourier transform analysis* Demonstration of the application of decoherence to explain the quantum-to-classical transition.|[2405.18564v1](http://arxiv.org/abs/2405.18564v1)|null|
|**2024-05-28**|**Covariance Operator Estimation via Adaptive Thresholding**|Omar Al-Ghattas et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:Key contributions:* The paper introduces a covariance operator estimator that adaptively thresholds the sample covariance function using an estimate of the variance components.* The authors derive an operator norm bound on the estimation error in terms of the sparsity level of the covariance and the expected supremum of the normalized process.* The paper demonstrates the advantage of adaptive threshold estimators over universal threshold and sample covariance estimators in nonstationary settings.* The authors focus on the infinite-dimensional setting and develop a theory that reveals the key dimension-free quantities that control the estimation error.* The paper shows that adaptive thresholding for highly nonstationary covariance models defined through a scalar parameter that controls both the correlation lengthscale and the range of the marginal variance function leads to an exponential improvement in sample complexity.Main contributions:* The paper provides a novel class of covariance operators that satisfy a weighted Lq-sparsity condition, which allows for covariance models with unbounded marginal variance functions.* The authors establish a bound on the operator norm error of the adaptive threshold estimator in terms of two dimension-free quantities: the sparsity level and the expected supremum of the normalized field.* The paper provides a comparison of the adaptive threshold estimator with other estimators, including the universal threshold and sample covariance estimators.* The authors demonstrate the benefits of adaptive thresholding in nonstationary settings through numerical simulations.|[2405.18562v1](http://arxiv.org/abs/2405.18562v1)|null|
|**2024-05-28**|**Unisolvence of unsymmetric random Kansa collocation by Gaussians and other analytic RBF vanishing at infinity**|Alvise Sommariva et.al.|Here is a summary of the key contributions in 1 sentence under 50 words:The paper proves that unsymmetric Kansa collocation matrices for the Poisson equation with Dirichlet boundary conditions by a class of analytic RBF, including Gaussian, Generalized Inverse MultiQuadrics, and Matern RBF, are almost surely nonsingular for general domains and random distributions of internal and boundary collocation points.|[2405.18550v1](http://arxiv.org/abs/2405.18550v1)|null|
|**2024-05-28**|**Star Stream Velocity Distributions in CDM and WDM Galactic Halos**|Raymond G. Carlberg et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper explores how dark matter subhalos perturb stars in stellar streams, causing an increase in random velocities that can be modeled as a Gaussian and exponential distribution, and discusses the potential for observational prospects to constrain the nature of galactic dark matter.|[2405.18522v1](http://arxiv.org/abs/2405.18522v1)|null|
|**2024-05-28**|**Effective phase diffusion for spin phase evolution under random nonlinear magnetic field**|Guoxing Lin et.al.|The key contributions of the paper can be summarized as follows:1. The authors propose a general theoretical description of spin self-diffusion under nonlinear gradient fields, extending the effective phase diffusion method for linear gradient fields.2. They show that under nonlinear gradient fields, spin diffusion can exhibit three types of phase evolutions: phase diffusion, float phase evolution, and shift-based phase evolution, which can significantly affect the NMR signal.3. The authors derive general expressions for the phase variance and corresponding NMR signal attenuation under nonlinear gradient fields, including parabolic and cubic fields.4. They demonstrate that the signal attenuation obeys Gaussian attenuation for short times, but changes to Lorentzian or Mittag-Leffler function attenuations when the time increases, which is different from Gaussian attenuation.5. The authors show that for spins starting diffusion far away from the origin, the signal attenuation is Gaussian, but the float phase still has an important effect on the total phase shift of even-order gradient fields, which can be used to measure the diffusion coefficient directly.6. They perform random walk simulations to support the obtained theoretical results and demonstrate the versatility of the proposed method for handling random order nonlinear gradient fields.Overall, the paper provides a comprehensive framework for understanding spin self-diffusion under nonlinear gradient fields and has implications for the development of advanced experimental techniques for NMR and MRI.|[2405.18514v1](http://arxiv.org/abs/2405.18514v1)|null|
|**2024-05-28**|**Classifying Overlapping Gaussian Mixtures in High Dimensions: From Optimal Classifiers to Neural Nets**|Khen Cohen et.al.|The paper derives closed-form expressions for Bayes optimal decision boundaries in binary classification of high-dimensional overlapping Gaussian mixture model (GMM) data, and shows that deep neural networks trained on such data approximate these optimal classifiers, with empirical evidence demonstrating correlations between decision thresholds and covariance eigenvectors.|[2405.18427v1](http://arxiv.org/abs/2405.18427v1)|null|
|**2024-05-28**|**GFlow: Recovering 4D World from Monocular Video**|Shizun Wang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper introduces GFlow, a novel framework that reconstructs dynamic 4D scenes from a single monocular video input without camera parameters, using 2D priors to model the world and enable tracking, densification, and camera pose estimation, among other capabilities.|[2405.18426v1](http://arxiv.org/abs/2405.18426v1)|null|
|**2024-05-28**|**3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting**|Qihang Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose 3DitScene, a novel scene editing framework that enables seamless editing from 2D to 3D, allowing for precise control over scene composition and individual objects using language-guided disentangled Gaussian Splatting.|[2405.18424v1](http://arxiv.org/abs/2405.18424v1)|null|
|**2024-05-28**|**3D StreetUnveiler with Semantic-Aware 2DGS**|Jingwei Xu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes StreetUnveiler, a 3D reconstruction method for empty street scenes from in-car camera videos, using a novel combination of hard-label semantic 2D Gaussian Splatting, time-reversal inpainting, and pseudo-labels to enhance temporal consistency and appearance integrity.|[2405.18416v1](http://arxiv.org/abs/2405.18416v1)|null|
|**2024-05-28**|**Short-time Fokker-Planck propagator beyond the Gaussian approximation**|Julian Kappler et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper presents a perturbation approach to calculate the short-time propagator of the one-dimensional Fokker-Planck equation to arbitrary order, preserving probability exactly and enabling the evaluation of expectation values to arbitrary accuracy, and demonstrates the superiority of this approach over the widely used Gaussian approximation.|[2405.18381v1](http://arxiv.org/abs/2405.18381v1)|null|
|**2024-05-28**|**Improving Linear System Solvers for Hyperparameter Optimisation in Iterative Gaussian Processes**|Jihao Andreas Lin et.al.|The key contributions of the paper are:1. **Pathwise gradient estimator**: A new estimator of the marginal likelihood gradient that reduces the number of solver iterations and amortises the computational cost of making predictions.2. **Warm starting linear system solvers**: Reusing linear system solutions to initialise the solver in the subsequent step, resulting in faster convergence and negligible bias.3. **Early stopping linear system solvers**: Stopping the solver after a limited computational budget, allowing the solver to accumulate progress across marginal likelihood steps.These techniques demonstrate significant speed-ups in solving the linear systems required by iterative Gaussian processes, with average speed-ups of up to 72× and decreased average residual norms by up to 7× when stopping early. The experiments show that the pathwise gradient estimator and warm starting linear system solvers improve the performance and efficiency of iterative Gaussian processes on large datasets.|[2405.18457v1](http://arxiv.org/abs/2405.18457v1)|null|
|**2024-05-28**|**Warm Start Marginal Likelihood Optimisation for Iterative Gaussian Processes**|Jihao Andreas Lin et.al.|The key contributions from the paper's abstract and introduction are:1. The authors propose a three-level hierarchy of marginal likelihood optimisation for iterative Gaussian processes, which reduces the computational cost of solving large positive-definite systems of linear equations.2. They introduce a "warm start" approach, which reuses the solutions of linear system solvers as initialisations in the next step, providing a significant speed-up in the computation of the marginal likelihood gradient.3. The authors show that the warm start approach can achieve the same results as the conventional procedure while providing up to a 16x average speed-up among datasets.The main ideas and results of the paper can be summarized as follows:1. The paper proposes a three-level hierarchy of marginal likelihood optimisation for iterative Gaussian processes, which involves iterative optimisation, gradient estimation, and linear system solving.2. The authors show that the dominant computational cost of the method is the solution of large positive-definite systems of linear equations.3. They propose a warm start approach, which reuses the solutions of linear system solvers as initialisations in the next step, providing a significant speed-up in the computation of the marginal likelihood gradient.4. The authors demonstrate the effectiveness of the warm start approach through experimental results, showing that it can achieve the same results as the conventional procedure while providing a significant speed-up.5. The paper also compares the performance of different linear system solvers, such as conjugate gradients, alternating projections, and stochastic gradient descent, and shows that the warm start approach can be effective with all of them.Overall, the paper provides a significant contribution to the field of Gaussian processes by proposing a novel approach to marginal likelihood optimisation that can speed up the computation of the marginal likelihood gradient while achieving the same results as the conventional procedure.|[2405.18328v1](http://arxiv.org/abs/2405.18328v1)|null|
