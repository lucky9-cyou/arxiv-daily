
### Gaussian Splatting
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-29**|**NPGA: Neural Parametric Gaussian Avatars**|Simon Giebenhain et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The authors propose Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings, leveraging a neural parametric head model, 3D Gaussian Splatting, and per-Gaussian latent features to achieve improved expressivity and controllability.|[2405.19331v1](http://arxiv.org/abs/2405.19331v1)|null|
|**2024-05-29**|**DGD: Dynamic 3D Gaussians Distillation**|Isaac Labe et.al.|Here is a summarized sentence under 50 words:The paper proposes DGD, a dynamic 3D representation that can track and segment 3D semantic entities in time and space using a single monocular video as input, enabling novel view synthesis and real-time rendering.|[2405.19321v1](http://arxiv.org/abs/2405.19321v1)|null|
|**2024-05-29**|**Mitigating Disparate Impact of Differential Privacy in Federated Learning through Robust Clustering**|Saber Malekmohammadi et.al.|The key contributions of the paper include a novel clustered DPFL algorithm that effectively identifies clients' clusters in highly heterogeneous settings while maintaining high accuracy with DP guarantees, and a hybrid clustering approach that combines information from both client model updates and loss values, robust to DP/stochastic noises.|[2405.19272v1](http://arxiv.org/abs/2405.19272v1)|null|
|**2024-05-29**|**Weak Generative Sampler to Efficiently Sample Invariant Distribution of Stochastic Differential Equation**|Zhiqiang Cai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in under 50 words:The authors introduce a new framework for sampling from an Ito diffusion process, using a weak generative sampler that employs a transformation map derived from the stationary Fokker-Planck equation. The method is shown to be effective in generating independent and identically distributed samples.|[2405.19256v1](http://arxiv.org/abs/2405.19256v1)|null|
|**2024-05-29**|**The Kinetic Ion-Temperature-Gradient-driven instability and its localisation**|Eduardo Rodriguez et.al.|The paper develops a polynomial-gaussian expansion in the field-following coordinate to describe localized Ion Temperature Gradient (ITG) modes, retaining both wave-particle and magnetic drift resonant effects and capturing the field-line dependence of the electrostatic potential.|[2405.19235v1](http://arxiv.org/abs/2405.19235v1)|null|
|**2024-05-29**|**$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation**|Weitian Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose $E^3$Gen, a novel avatar generation method that efficiently combines 3D Gaussian representation with diffusion-based generation, enabling expressive and editable digital avatars with real-time rendering and pose control.|[2405.19203v2](http://arxiv.org/abs/2405.19203v2)|null|
|**2024-05-29**|**Model-independent cosmological inference post DESI DR1 BAO measurements**|Purba Mukherjee et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper implements Gaussian process regression to reconstruct the universe's expansion history, incorporating multiple datasets, and finds evidence for a quintessence-like dark energy scenario, while also constraining the Hubble parameter and reporting discrepancies between different data sets.|[2405.19178v1](http://arxiv.org/abs/2405.19178v1)|null|
|**2024-05-29**|**DeepOKAN: Deep Operator Network Based on Kolmogorov Arnold Networks for Mechanics Problems**|Diab W. Abueidda et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:This paper presents DeepOKAN, a new neural operator that uses Kolmogorov-Arnold networks and Gaussian radial basis functions to efficiently learn high-dimensionality problems, outperforming traditional DeepONet while requiring fewer learnable parameters.|[2405.19143v1](http://arxiv.org/abs/2405.19143v1)|null|
|**2024-05-29**|**Efficient Black-box Adversarial Attacks via Bayesian Optimization Guided by a Function Prior**|Shuyu Cheng et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The key contributions of this paper are:* The proposal of a Prior-guided Bayesian Optimization (P-BO) algorithm that leverages a surrogate model as a global function prior in black-box adversarial attacks.* The integration of the surrogate model into the Gaussian process to improve the query efficiency of black-box attacks.* The introduction of an adaptive integration strategy to automatically adjust the coefficient on the surrogate model to minimize the regret bound.* Experimental results on various image classifiers and vision-language models demonstrate the superiority of the proposed algorithm in reducing queries and improving attack success rates compared with the state-of-the-art black-box attacks.* The results also show that the data-dependent prior is orthogonal to the proposed function prior, and that the adaptive integration coefficient significantly improves the attack success rates and reduces queries.The paper aims to improve the attack success rate and query efficiency of black-box adversarial attacks by leveraging a surrogate model as a global function prior, and proposing an adaptive integration strategy to automatically adjust the coefficient on the surrogate model.|[2405.19098v1](http://arxiv.org/abs/2405.19098v1)|[link](https://github.com/yibo-miao/pbo-attack)|
|**2024-05-29**|**Implementing arbitrary multi-mode continuous-variable quantum gates with fixed non-Gaussian states and adaptive linear optics**|Fumiya Hanamura et.al.|Here are the key contributions from the paper's abstract and introduction summarized in a single sentence under 50 words:The paper proposes a measurement-based method to implement high-order, multi-mode non-Gaussian gates using fixed non-Gaussian ancillary states and adaptive linear optics, allowing for more efficient and experimentally feasible implementation of important gates in optical quantum technology.|[2405.19067v1](http://arxiv.org/abs/2405.19067v1)|null|
|**2024-05-29**|**Towards a computationally-efficient follow-up pipeline for blind continuous gravitational-wave searches**|Lorenzo Mirasola et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors develop a novel framework to evaluate the effectiveness of continuous gravitational wave follow-ups and demonstrate a ten-fold reduction in computing cost using a new approach, which simplifies the setup of multi-stage follow-ups and enables more computationally-efficient searches for unknown neutron star sources.|[2405.18934v1](http://arxiv.org/abs/2405.18934v1)|null|
|**2024-05-29**|**Opinion-Unaware Blind Image Quality Assessment using Multi-Scale Deep Feature Statistics**|Zhangkai Ni et.al.|The paper proposes a novel blind image quality assessment (BIQA) model, Multi-scale Deep Feature Statistics (MDFS), which integrates deep features from pre-trained visual models with a statistical analysis model to eliminate the need for human rating data and improve training efficiency.|[2405.18790v1](http://arxiv.org/abs/2405.18790v1)|[link](https://github.com/eezkni/mdfs)|
|**2024-05-29**|**LP-3DGS: Learning to Prune 3D Gaussian Splatting**|Zhaoliang Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a learning-to-prune 3D Gaussian Splatting (LP-3DGS) method that automatically learns an optimal pruning ratio for each scene, leveraging a trainable mask and the Gumbel-Sigmoid method, to balance memory usage and rendering quality.|[2405.18784v1](http://arxiv.org/abs/2405.18784v1)|null|
|**2024-05-29**|**Principled Probabilistic Imaging using Diffusion Models as Plug-and-Play Priors**|Zihui Wu et.al.|The paper proposes a Markov chain Monte Carlo (MCMC) algorithm called Plug-and-Play Diffusion Models (PnP-DM) for posterior sampling in general inverse problems. By reducing the problem to sampling the posterior of a Gaussian denoising problem, PnP-DM leverages a general diffusion model formulation as a unified interface to rigorously solve the denoising problem with a range of state-of-the-art diffusion models. This allows for the use of expressive image priors in the form of diffusion models, which can accurately model complex image distributions.|[2405.18782v1](http://arxiv.org/abs/2405.18782v1)|null|
|**2024-05-29**|**LMO-DP: Optimizing the Randomization Mechanism for Differentially Private Fine-Tuning (Large) Language Models**|Qin Yang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose LMO-DP, a novel Language Model-based Optimal Differential Privacy mechanism that achieves sub-optimal noise reduction and supports strong privacy guarantees (e.g., ϵ = 0.3, δ = 10^-10) for fine-tuning large language models.|[2405.18776v1](http://arxiv.org/abs/2405.18776v1)|null|
|**2024-05-29**|**Deep Bayesian Filter for Bayes-faithful Data Assimilation**|Yuta Tarumi et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The paper proposes a novel methodology, Deep Bayesian Filtering (DBF), for data assimilation on nonlinear state space models (SSMs) that avoids accumulating Monte-Carlo sampling errors over time steps by computing the integration over time analytically. DBF constructs new latent variables on a new latent ("fancy") space and assimilates observations by constraining the state transition on fancy space to be linear and learning a Gaussian inverse observation operator. The paper also presents a recursive formula for the filter distribution and discusses the properties of DBF, its applicability to nonlinear dynamics, and its training process.|[2405.18674v1](http://arxiv.org/abs/2405.18674v1)|null|
|**2024-05-28**|**Biclustering a dataset using photonic quantum computing**|Ajinkya Borle et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper explores the application of boson sampling and Gaussian boson sampling to the problem of biclustering, proposing a heuristic that converts a dataset into a bipartite graph and uses GBS to find dense sub-graphs, and reports promising simulation results for future exploration.|[2405.18622v1](http://arxiv.org/abs/2405.18622v1)|null|
|**2024-05-28**|**From Conformal Predictions to Confidence Regions**|Charles Guille-Escuret et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors introduce Conformal Confidence Regions (CCR), a novel approach that employs conformal prediction intervals to establish confidence regions for model parameters under minimal assumptions, providing finite-sample validity and applicability to various machine learning methodologies.|[2405.18601v1](http://arxiv.org/abs/2405.18601v1)|null|
|**2024-05-28**|**Exploring the transition between Quantum and Classical Mechanics**|E. Aldo Arroyo et.al.|According to the abstract and introduction of the paper, the authors investigate the transition from quantum to classical mechanics using a one-dimensional free particle model. They find that, in the classical analysis, the distribution of the particle's final position can be obtained by convolving two Gaussian distributions. In contrast, in the quantum analysis, the temporal evolution provides the final wave function from which the quantum probability density can be calculated. While the classical and quantum analyses initially yield equivalent results, the authors observe a discrepancy when considering a superposition of Gaussian distributions, causing quantum interference to deviate from classical behavior. By proposing a novel method based on truncated Fourier analysis to recover the classical distribution, the authors aim to reveal the connection between classical and quantum probability distributions, addressing the significance of quantum decoherence in this context and providing a deeper understanding of the classical-quantual correspondence.|[2405.18564v1](http://arxiv.org/abs/2405.18564v1)|null|
|**2024-05-28**|**Covariance Operator Estimation via Adaptive Thresholding**|Omar Al-Ghattas et.al.|According to the abstract, the key contributions are:* A new covariance operator estimator that adaptively thresholds the sample covariance function using an estimate of the variance components, which is shown to be optimal in non-stationary settings where the marginal variance varies widely and the correlation lengthscale is small.* An operator norm bound on the estimation error in terms of the sparsity level of the covariance and the expected supremum of the normalized process, derived using recent results from empirical process theory.* Numerical simulations demonstrating the advantage of adaptive thresholding estimators over universal threshold and sample covariance estimators in non-stationary settings.According to the introduction, the paper aims to study sparse covariance operator estimation in infinite-dimensional function spaces, highlighting the importance of exploiting structural assumptions in the design and analysis of estimators.|[2405.18562v1](http://arxiv.org/abs/2405.18562v1)|null|
|**2024-05-28**|**Unisolvence of unsymmetric random Kansa collocation by Gaussians and other analytic RBF vanishing at infinity**|Alvise Sommariva et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proves that unsymmetric random Kansa collocation matrices for the Poisson equation with Dirichlet boundary conditions are almost surely nonsingular for a class of analytic RBFs, including Gaussians, Generalized Inverse MultiQuadrics, and Matern RBFs, without restrictive assumptions on the boundary or internal collocation points.|[2405.18550v1](http://arxiv.org/abs/2405.18550v1)|null|
|**2024-05-28**|**Star Stream Velocity Distributions in CDM and WDM Galactic Halos**|Raymond G. Carlberg et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The paper measures the distribution of random velocities in long, thin stellar streams orbiting in simulated Milky Way-like halos, finding well-modeled radial velocity distributions as a sum of Gaussians and exponentials, which can be used to constrain the nature of galactic dark matter.|[2405.18522v1](http://arxiv.org/abs/2405.18522v1)|null|
|**2024-05-28**|**Effective phase diffusion for spin phase evolution under random nonlinear magnetic field**|Guoxing Lin et.al.|The key contributions of the paper can be summarized as follows:* A general theoretical description of spin self-diffusion under nonlinear gradient fields is proposed, extending the effective phase diffusion method for linear gradient fields.* The proposed method reveals the general features of phase evolutions in nonlinear gradient fields, including phase diffusion, float phase evolution, and shift based on the starting position.* The method shows that the diffusing and float phase evolutions come from the first and second derivatives of the gradient field, respectively.* The phase variance and corresponding NMR signal attenuation are obtained, and the results demonstrate that signal attenuation obeys Gaussian attenuation for a short time, then changes to Lorentzian or Mittag-Leffler function attenuations when time increases.* The paper also discusses the challenges in existing theories, including inconsistencies and limitations, and highlights the advantages of the proposed method, including its versatility and ability to handle nonlinear gradient fields.* The method is applied to calculate the signal attenuation for spins starting diffusion from a random position, and the results agree with the expected values.Overall, the paper provides a new theoretical framework for understanding spin self-diffusion under nonlinear gradient fields, which can have important implications for NMR and MRI applications.|[2405.18514v1](http://arxiv.org/abs/2405.18514v1)|null|
|**2024-05-28**|**Classifying Overlapping Gaussian Mixtures in High Dimensions: From Optimal Classifiers to Neural Nets**|Khen Cohen et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper derives closed-form expressions for Bayes optimal decision boundaries in binary classification of high-dimensional overlapping Gaussian mixture model data and shows that deep neural networks trained for classification approximate these optimal classifiers.|[2405.18427v1](http://arxiv.org/abs/2405.18427v1)|null|
|**2024-05-28**|**GFlow: Recovering 4D World from Monocular Video**|Shizun Wang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper introduces GFlow, a novel framework that reconstructs dynamic 4D worlds from a single monocular video input without camera calibration, leveraging 2D priors to achieve accurate reconstruction and object segmentation, and enables point tracking and camera pose estimation.|[2405.18426v1](http://arxiv.org/abs/2405.18426v1)|null|
|**2024-05-28**|**3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting**|Qihang Zhang et.al.|The paper proposes 3DitScene, a unified scene editing framework that enables precise control over scene composition and individual objects at the 3D level, and seamlessly edits from 2D to 3D using language-guided disentangled Gaussian Splatting.|[2405.18424v1](http://arxiv.org/abs/2405.18424v1)|null|
|**2024-05-28**|**3D StreetUnveiler with Semantic-Aware 2DGS**|Jingwei Xu et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper introduces StreetUnveiler, a method that reconstructs an empty street scene from in-car camera videos by leveraging hard-label semantic 2D Gaussian Splatting and a time-reversal inpainting framework to maintain temporal consistency and scalability, and achieves accurate reconstruction and geometry preservation of occluded regions.|[2405.18416v2](http://arxiv.org/abs/2405.18416v2)|null|
|**2024-05-28**|**Short-time Fokker-Planck propagator beyond the Gaussian approximation**|Julian Kappler et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents a perturbation approach to calculate the short-time propagator of the one-dimensional Fokker-Planck equation to arbitrary order, which allows for the evaluation of expectation values to arbitrary accuracy and is compared to the widely used Gaussian approximation.|[2405.18381v1](http://arxiv.org/abs/2405.18381v1)|null|
|**2024-05-28**|**Improving Linear System Solvers for Hyperparameter Optimisation in Iterative Gaussian Processes**|Jihao Andreas Lin et.al.|The key contributions of the paper are:1. **Pathwise gradient estimator**: A new method to estimate the marginal likelihood gradient, which reduces the required number of solver iterations and amortises the computational cost of making predictions.2. **Warm starting linear system solvers**: Initializing the linear system solver with the solution from the previous step, which leads to faster solver convergence at the cost of negligible bias.3. **Early stopping linear system solvers**: Stopping the linear system solver after a limited computational budget, which synergises with warm starting and allows solver progress to accumulate over multiple marginal likelihood steps.These techniques provide speed-ups of up to 72× when solving to tolerance, and decrease the average residual norm by up to 7× when stopping early.|[2405.18457v1](http://arxiv.org/abs/2405.18457v1)|null|
|**2024-05-28**|**Warm Start Marginal Likelihood Optimisation for Iterative Gaussian Processes**|Jihao Andreas Lin et.al.|Here are the key contributions from the paper's abstract and introduction:* The paper considers iterative methods for marginal likelihood optimisation in Gaussian processes, which allow for a trade-off between computational cost and accuracy.* The authors introduce a three-level hierarchy of marginal likelihood optimisation for iterative Gaussian processes, which identifies that the computational costs are dominated by solving sequential batches of large positive-definite systems of linear equations.* They propose to amortise computations by reusing solutions of linear system solvers as initialisations in the next step, providing a "warm start" that reduces the computational cost.* The authors demonstrate the effectiveness of the warm start method on regression tasks, showing that it achieves the same results as the conventional procedure while providing up to a 16x average speed-up among datasets.Note that the paper focuses on the development of a novel optimization method for Gaussian processes, and the contributions are primarily in the area of optimization and machine learning.|[2405.18328v1](http://arxiv.org/abs/2405.18328v1)|null|
