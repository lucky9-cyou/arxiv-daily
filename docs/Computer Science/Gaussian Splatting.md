
### Gaussian Splatting
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-29**|**NPGA: Neural Parametric Gaussian Avatars**|Simon Giebenhain et.al.|The key contributions of the paper are: proposing Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings, using Neural Parametric Head Models (NPHM) and 3D Gaussian Splatting, and introducing per-Gaussian latent features to increase dynamic expressivity.|[2405.19331v1](http://arxiv.org/abs/2405.19331v1)|null|
|**2024-05-29**|**DGD: Dynamic 3D Gaussians Distillation**|Isaac Labe et.al.|The key contributions of the paper are the introduction of a unified 3D representation called DGD (Dynamic 3D Gaussians Distillation) that captures both the appearance and semantics of a dynamic 3D scene, enabling the segmentation and tracking of 3D semantic entities, and a joint optimization of the appearance and semantic attributes, which jointly affect the geometric properties of the scene.|[2405.19321v1](http://arxiv.org/abs/2405.19321v1)|null|
|**2024-05-29**|**Mitigating Disparate Impact of Differential Privacy in Federated Learning through Robust Clustering**|Saber Malekmohammadi et.al.|The paper proposes a novel clustered differentially private federated learning (DPFL) algorithm that effectively identifies clients' clusters in highly heterogeneous settings while maintaining high accuracy with DP guarantees, addressing performance disparities and disparate impact on minority groups.|[2405.19272v1](http://arxiv.org/abs/2405.19272v1)|null|
|**2024-05-29**|**Weak Generative Sampler to Efficiently Sample Invariant Distribution of Stochastic Differential Equation**|Zhiqiang Cai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose a weak generative sampler framework that leverages normalizing flows to efficiently generate independent and identically distributed samples from the invariant distribution of stochastic differential equations, without relying on computationally expensive calculations or mini-max optimization.|[2405.19256v1](http://arxiv.org/abs/2405.19256v1)|null|
|**2024-05-29**|**The Kinetic Ion-Temperature-Gradient-driven instability and its localisation**|Eduardo Rodriguez et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors develop a semi-analytical model for Ion Temperature Gradient (ITG) driven localized modes, capturing wave-particle and magnetic drift resonant effects, and field-line dependence, and demonstrate its ability to reproduce observed spectra with multiple maxima in stellarators.|[2405.19235v1](http://arxiv.org/abs/2405.19235v1)|null|
|**2024-05-29**|**$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation**|Weitian Zhang et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes a novel avatar generation method, E^3Gen, that efficiently generates high-quality avatars with expressive pose control and editing capabilities, leveraging a novel 3D Gaussian representation and a part-aware deformation module.|[2405.19203v1](http://arxiv.org/abs/2405.19203v1)|null|
|**2024-05-29**|**Model-independent cosmological inference post DESI DR1 BAO measurements**|Purba Mukherjee et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:This study uses Gaussian process regression to reconstruct the universe's expansion history in a model-agnostic manner, incorporating Supernovae Ia and BAO data, which hints at a quintessence-like dark energy scenario and also provides constraints on H0rd and the dark energy equation of state, with evidence for potential disparities between datasets.|[2405.19178v1](http://arxiv.org/abs/2405.19178v1)|null|
|**2024-05-29**|**DeepOKAN: Deep Operator Network Based on Kolmogorov Arnold Networks for Mechanics Problems**|Diab W. Abueidda et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The DeepOKAN is a new neural operator that uses Kolmogorov-Arnold networks (KANs) in the branch and trunk, replacing the traditional multi-layer perceptron (MLP) architecture. The DeepOKAN utilizes Gaussian radial basis functions (RBFs) instead of B-splines, leading to a significant computational speedup without compromising accuracy. The paper proposes the DeepOKAN as a more efficient and accurate alternative to the traditional DeepONet architecture.|[2405.19143v1](http://arxiv.org/abs/2405.19143v1)|null|
|**2024-05-29**|**Efficient Black-box Adversarial Attacks via Bayesian Optimization Guided by a Function Prior**|Shuyu Cheng et.al.|The key contributions from the paper's abstract and introduction are:* A novel Prior-guided Bayesian Optimization (P-BO) algorithm that leverages the surrogate model as a global function prior in black-box adversarial attacks, improving query efficiency and attack success rates.* Theoretical analysis on the regret bound indicates that the performance of P-BO may be affected by a bad prior, and an adaptive integration strategy is proposed to automatically adjust a coefficient on the function prior by minimizing the regret bound.* Experimental results demonstrate the superiority of P-BO in reducing queries and improving attack success rates compared with state-of-the-art black-box attacks on various datasets, including image classifiers, large vision-language models, and defense models.|[2405.19098v1](http://arxiv.org/abs/2405.19098v1)|[link](https://github.com/yibo-miao/pbo-attack)|
|**2024-05-29**|**Implementing arbitrary multi-mode continuous-variable quantum gates with fixed non-Gaussian states and adaptive linear optics**|Fumiya Hanamura et.al.|Here are the key contributions from the paper's abstract and introduction:**Key Contributions:**1. A new method for implementing multi-mode, higher-order non-Gaussian gates using fixed non-Gaussian ancillary states and adaptive linear optics.2. The proposed method allows for a more resource-efficient and experimentally feasible implementation of multi-mode gates, including the two-mode cubic quantum non-demolition gate and the three-mode continuous-variable Toffoli gate.3. The method reduces the number of required Gaussian and non-Gaussian ancillary modes compared to conventional decomposition methods.4. The protocol is compatible with quantum information processing using cluster states.**Method Overview:**The paper proposes a measurement-based method for implementing high-order, multi-mode non-Gaussian gates. The method involves using fixed non-Gaussian ancillary states and adaptive linear optics to implement the gate. The protocol is decomposed into quadrature gates, which are then implemented using linear optics and measurement-based implementation of non-Gaussian operators. The paper also proposes a general methodology for implementing higher-order gates and discusses the advantages of the proposed method.|[2405.19067v1](http://arxiv.org/abs/2405.19067v1)|null|
|**2024-05-29**|**Towards a computationally-efficient follow-up pipeline for blind continuous gravitational-wave searches**|Lorenzo Mirasola et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The authors develop a novel framework to evaluate the effectiveness of stochastic continuous gravitational-wave (CW) follow-ups, resulting in a ten-fold reduction of computing cost for pyfstat, a leading CW follow-up method. Their framework also simplifies the setup of multi-stage follow-ups, making it more efficient without constraining signal models.|[2405.18934v1](http://arxiv.org/abs/2405.18934v1)|null|
|**2024-05-29**|**Opinion-Unaware Blind Image Quality Assessment using Multi-Scale Deep Feature Statistics**|Zhangkai Ni et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper proposes a novel Multi-scale Deep Feature Statistics (MDFS) model that integrates deep features from pre-trained visual models with a statistical analysis model to achieve opinion-unaware blind image quality assessment (BIQA) without relying on human rating data, demonstrating superior consistency with human visual perception and improved generalizability.|[2405.18790v1](http://arxiv.org/abs/2405.18790v1)|[link](https://github.com/eezkni/mdfs)|
|**2024-05-29**|**LP-3DGS: Learning to Prune 3D Gaussian Splatting**|Zhaoliang Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The paper proposes a learning-to-prune 3D Gaussian Splatting (LP-3DGS) methodology that automatically finds the optimal pruning ratio for each scene without manual tuning. This is achieved by introducing a trainable mask applied to the importance score, which leverages the Gumbel-Sigmoid method to make the masking function differentiable and compatible with existing 3DGS training. The paper eliminates the need for manual pruning ratio tuning, which requires multiple rounds of training for each scene.|[2405.18784v1](http://arxiv.org/abs/2405.18784v1)|null|
|**2024-05-29**|**Principled Probabilistic Imaging using Diffusion Models as Plug-and-Play Priors**|Zihui Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose a Markov chain Monte Carlo algorithm, Plug-and-Play Diffusion Models (PnP-DM), that rigorously solves Bayesian inverse problems by integrating diffusion models as expressive image priors, allowing for accurate posterior sampling and reconstruction in various imaging inverse problems.|[2405.18782v1](http://arxiv.org/abs/2405.18782v1)|null|
|**2024-05-29**|**LMO-DP: Optimizing the Randomization Mechanism for Differentially Private Fine-Tuning (Large) Language Models**|Qin Yang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:1. The authors propose a novel Language Model-based Optimal Differential Privacy (LMO-DP) mechanism, which is the first non-Gaussian mechanism that can generate sub-optimal noise to ensure strong differential privacy.2. LMO-DP is the first mechanism to support large language models with strong differential privacy guarantees, such as ϵ = 0.3 and δ = 10^(-10).3. The authors demonstrate that LMO-DP can significantly boost the performance of DP-SGD and other variants with high noise reduction, achieving accuracy boosts of up to 90% for fine-tuning RoBERTa-large on the SST-2 dataset.|[2405.18776v1](http://arxiv.org/abs/2405.18776v1)|null|
|**2024-05-29**|**Deep Bayesian Filter for Bayes-faithful Data Assimilation**|Yuta Tarumi et.al.|The key contributions from the paper's abstract and introduction are:A novel methodology called Deep Bayesian Filtering (DBF) that uses a Gaussian distribution to approximate the inverse observation operator and learns a linear constraint on the state transition model, which allows for the computation of posteriors analytically and avoids accumulating Monte-Carlo sampling errors over time steps.|[2405.18674v1](http://arxiv.org/abs/2405.18674v1)|null|
|**2024-05-28**|**Biclustering a dataset using photonic quantum computing**|Ajinkya Borle et.al.|The paper's abstract and introduction highlight the application of quantum computing models, specifically boson and Gaussian boson sampling, to the problem of biclustering in machine learning, proposing a heuristic method to find clusters in a dataset using GBS and demonstrating promising results in simulations.|[2405.18622v1](http://arxiv.org/abs/2405.18622v1)|null|
|**2024-05-28**|**From Conformal Predictions to Confidence Regions**|Charles Guille-Escuret et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper introduces CCR, a novel approach that combines conformal prediction intervals for model outputs to establish confidence regions for model parameters, providing finite-sample validity under minimal assumptions on the data distribution.|[2405.18601v1](http://arxiv.org/abs/2405.18601v1)|null|
|**2024-05-28**|**Exploring the transition between Quantum and Classical Mechanics**|E. Aldo Arroyo et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The study investigates the transition from quantum to classical mechanics using a one-dimensional free particle model, showing that the quantum probability density coincides with the classical normal distribution under certain conditions, and proposing a novel method to recover the classical distribution from the quantum one using truncated Fourier analysis.|[2405.18564v1](http://arxiv.org/abs/2405.18564v1)|null|
|**2024-05-28**|**Covariance Operator Estimation via Adaptive Thresholding**|Omar Al-Ghattas et.al.|Here is a summary of the key contributions from the paper's abstract and introduction, in a single sentence under 50 words:The paper proposes a covariance operator estimator that adaptively thresholds the sample covariance function using an estimate of the variance components, and derives an operator norm bound on the estimation error in terms of the sparsity level and the expected supremum of the normalized process, improving over universal threshold and sample covariance estimators in nonstationary settings.|[2405.18562v1](http://arxiv.org/abs/2405.18562v1)|null|
|**2024-05-28**|**Unisolvence of unsymmetric random Kansa collocation by Gaussians and other analytic RBF vanishing at infinity**|Alvise Sommariva et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proves that a class of analytic radial basis functions (RBFs) that vanish at infinity, including Gaussians, Generalized Inverse MultiQuadrics, and Matern RBFs, ensure the almost sure invertibility of unsymmetric Kansa collocation matrices for the Poisson equation with Dirichlet boundary conditions.|[2405.18550v1](http://arxiv.org/abs/2405.18550v1)|null|
|**2024-05-28**|**Star Stream Velocity Distributions in CDM and WDM Galactic Halos**|Raymond G. Carlberg et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The distribution of dark matter subhalos in a galactic halo perturbs the orbits of stars in stellar streams, leading to the development of non-Gaussian velocity wings, and the number of subhalos can be constrained by measuring the velocity distribution in these streams.|[2405.18522v1](http://arxiv.org/abs/2405.18522v1)|null|
|**2024-05-28**|**Effective phase diffusion for spin phase evolution under random nonlinear magnetic field**|Guoxing Lin et.al.|The key contributions from the paper's abstract and introduction are:* A general theoretical description of spin self-diffusion under nonlinear gradient fields, which extends the effective phase diffusion method for linear gradient fields.* The proposal of a new method for spin diffusion under nonlinear fields, which can handle random order nonlinear gradient fields.* The demonstration of three types of phase evolutions: phase diffusion, float phase evolution, and shift based on the starting position, which significantly affect the NMR signal.* The calculation of phase variance and corresponding NMR signal attenuation for parabolic and cubic fields, which indicate that signal attenuation obeys Gaussian attenuation for a short time, then changes to Lorentzian or Mittag-Leffler function attenuations when time increases.* The use of random walk simulations to support the obtained theoretical results.* The potential to develop advanced experimental techniques for NMR and MRI.|[2405.18514v1](http://arxiv.org/abs/2405.18514v1)|null|
|**2024-05-28**|**Classifying Overlapping Gaussian Mixtures in High Dimensions: From Optimal Classifiers to Neural Nets**|Khen Cohen et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper derives closed-form expressions for Bayes optimal decision boundaries in binary classification of high-dimensional overlapping Gaussian mixture model (GMM) data, and shows that deep neural networks trained for classification can approximate these optimal classifiers, with implications for probabilistic inference and distillation of statistical patterns.|[2405.18427v1](http://arxiv.org/abs/2405.18427v1)|null|
|**2024-05-28**|**GFlow: Recovering 4D World from Monocular Video**|Shizun Wang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The key contributions of this paper are:* The introduction of a novel framework, GFlow, that can reconstruct dynamic 3D scenes and camera motion from a single, uncalibrated video input.* The ability to represent videos as a flow of 3D Gaussian splatting, allowing for the capture of complex camera movements and scene dynamics.* A sequential optimization process that simultaneously optimizes camera poses and Gaussian points to ensure fidelity among neighboring points and smooth movement across frames.* A pixel-wise densification strategy that can integrate new visual content into the reconstruction.* The ability to track any points across frames in 3D world coordinates without prior training and segment moving objects from the scene in an unsupervised manner.* Experimental results showing the effectiveness of GFlow in reconstructing dynamic scenes, tracking points, and estimating camera poses.|[2405.18426v1](http://arxiv.org/abs/2405.18426v1)|null|
|**2024-05-28**|**3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting**|Qihang Zhang et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The authors propose 3DitScene, a unified scene editing framework that leverages language-guided disentangled Gaussian Splatting to enable seamless editing from 2D to 3D, allowing precise control over scene composition and individual objects at both global and object levels.|[2405.18424v1](http://arxiv.org/abs/2405.18424v1)|null|
|**2024-05-28**|**3D StreetUnveiler with Semantic-Aware 2DGS**|Jingwei Xu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The authors introduce StreetUnveiler, a novel 3D reconstruction method that learns a representation of an empty street from crowded observations captured by in-car cameras, addressing the challenges of long-trajectory scenes, limited observation angles, and object removal. Key contributions include representing the street as hard-label semantic 2D Gaussian Splatting, generating inpainting masks through rendered alpha maps, and proposing a time-reversal inpainting framework for enhancing temporal consistency.|[2405.18416v1](http://arxiv.org/abs/2405.18416v1)|null|
|**2024-05-28**|**Short-time Fokker-Planck propagator beyond the Gaussian approximation**|Julian Kappler et.al.|The key contributions of the paper are:* A perturbation approach to calculate the short-time propagator of the one-dimensional Fokker-Planck equation to in principle arbitrary order in the time increment, which preserves probability exactly.* The perturbation expansion allows for the evaluation of expectation values of analytical observables to in principle arbitrary accuracy.* The authors derive perturbation expansions for the moments of the spatial increment, the finite-time Kramers-Moyal coefficients, and the mean medium entropy production rate.* They validate their perturbative results for an explicit multiplicative-noise system with an available analytical solution.* They compare their perturbative results with those obtained from the widely used Gaussian approximation of the short-time propagator, demonstrating that the Gaussian propagator leads to errors that can be many orders of magnitude larger than those resulting from their perturbation approach.|[2405.18381v1](http://arxiv.org/abs/2405.18381v1)|null|
|**2024-05-28**|**Improving Linear System Solvers for Hyperparameter Optimisation in Iterative Gaussian Processes**|Jihao Andreas Lin et.al.|The key contributions from the paper's abstract and introduction are:1. **Pathwise Gradient Estimator**: The paper introduces a pathwise estimator of the marginal likelihood gradient, which reduces the required number of solver iterations and amortizes the computational cost of making predictions.2. **Warm Starting**: The paper proposes to warm start linear system solvers throughout marginal likelihood optimisation by reusing linear system solutions to initialize the solver in the subsequent step. This results in faster convergence.3. **Early Stopping**: The paper investigates the behavior of linear system solvers on a limited compute budget, where warm starting allows the linear system solver to accumulate solver progress across marginal likelihood steps.4. **Speed-ups and Improved Performance**: The paper demonstrates that these techniques can lead to speed-ups of up to 72× when solving to tolerance, and decrease the average residual norm by up to 7× when stopping early.5. **Large-Scale Experiments**: The paper showcases the effectiveness of these techniques on large-scale datasets, suggesting that they can be used to scale hyperparameter optimisation to very large datasets.|[2405.18457v1](http://arxiv.org/abs/2405.18457v1)|null|
|**2024-05-28**|**Warm Start Marginal Likelihood Optimisation for Iterative Gaussian Processes**|Jihao Andreas Lin et.al.|The key contributions of the paper can be summarized as follows:1. The paper introduces a three-level hierarchy of marginal likelihood optimisation for iterative Gaussian processes, which iteratively solves large positive-definite systems of linear equations to approximate the marginal likelihood gradient.2. The paper proposes to amortise computations by reusing solutions of linear system solvers as initialisations in the next step, providing a "warm start" that reduces the number of required solver iterations and improves the efficiency of the algorithm.3. The paper shows that the warm start method achieves the same results as the conventional procedure while providing a significant speed-up, with an average speed-up of 16x across datasets.4. The paper demonstrates the effectiveness of the warm start method on several datasets using different linear system solvers, including conjugate gradients, alternating projections, and stochastic gradient descent.In particular, the paper's main contributions are:1. The proposal of a warm start method for iterative Gaussian processes, which leverages the reuse of previously computed solutions to improve the efficiency of the algorithm.2. The demonstration of the effectiveness of the warm start method across multiple datasets and linear system solvers.3. The analysis of the computational costs and trade-offs involved in the warm start method, including the number of required solver iterations and the accuracy of the solution.|[2405.18328v1](http://arxiv.org/abs/2405.18328v1)|null|
