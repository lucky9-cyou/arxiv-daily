
### NeRF SLAM
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|The key contributions from the paper's abstract and introduction are:The paper proposes Neural Elevation Models (NEMos), a novel representation that combines a Neural Radiance Field with a height field to leverage the strengths of both, providing a continuous and differentiable terrain model.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**NeB-SLAM: Neural Blocks-based Salable RGB-D SLAM for Unknown Scenes**|Lizhi Bai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose NeB-SLAM, a neural block-based RGB-D SLAM system that can handle unknown scenes, using a divide-and-conquer mapping strategy and adaptive map growth strategy to represent the scene as a set of sub-maps and achieve efficient memory usage.|[2405.15151v1](http://arxiv.org/abs/2405.15151v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|The paper proposes High Dynamic Range Gaussian Splatting (HDR-GS), a novel framework for 3D HDR imaging that can render photo-realistic images from novel viewpoints with a wide dynamic range, controllable exposure time, and faster inference speed.|[2405.15125v1](http://arxiv.org/abs/2405.15125v1)|null|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|The paper proposes a steganography framework, GS-Hider, for hiding 3D scenes and images within 3D Gaussian Splatting (3DGS) point cloud files, which possesses two distinct features: explicit 3D representation and real-time rendering speeds.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**ETA-INIT: Enhancing the Translation Accuracy for Stereo Visual-Inertial SLAM Initialization**|Han Song et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The proposed method, ETA, enhances translation accuracy during initialization in stereo Visual-Inertial SLAM systems by using a 3-DoF Bundle Adjustment and considering IMU measurements and gyroscope bias, achieving improved accuracy and maintaining comparable run-time speed.|[2405.15082v1](http://arxiv.org/abs/2405.15082v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The authors present a method that addresses the limitations of Neural Radiance Fields (NeRFs) in rendering high-frequency view-dependent appearance, particularly specular reflections, by introducing ray tracing to reduce the reliance on large neural networks and improve rendering efficiency and quality.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Synergistic Global-space Camera and Human Reconstruction from Videos**|Yizhou Zhao et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper introduces Synergistic Camera and Human Reconstruction (SynCHMR) to marry human motion reconstruction with monocular SLAM, leveraging human meshes to disambiguate camera motion and reconstruct metric-scale camera poses, scene point clouds, and human meshes in a common world frame.|[2405.14855v1](http://arxiv.org/abs/2405.14855v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper presents Neural Directional Encoding (NDE), a feature-grid-like neural encoding method for rendering specular objects, which encodes view-dependent effects into feature grids and cone-traces spatial features to model high-frequency angular signals, achieving both high-quality modeling of view-dependent effects and fast evaluation.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|Here is a summary of the paper's key contributions in a single sentence under 50 words:This paper proposes a two-staged pipeline for camera relocalization, using a hash-encoded NeRF and a normilizer to account for lighting changes, and introduces a re-devised truncated dynamic low-pass filter and numerical gradient averaging technique to improve pose optimization.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**CoPeD-Advancing Multi-Robot Collaborative Perception: A Comprehensive Dataset in Real-World Environments**|Yang Zhou et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:This paper presents a pioneering, comprehensive, and real-world multi-robot collaborative perception dataset, featuring air-ground robot collaboration, distinct spatial viewpoints, and diverse sensor modalities, to boost research in this area and unlock high-level scene understanding through multi-modal collaborative perception.|[2405.14731v1](http://arxiv.org/abs/2405.14731v1)|[link](https://github.com/arplaboratory/coped)|
|**2024-05-23**|**Efficient Robot Learning for Perception and Mapping**|Niclas VÃ¶disch et.al.|Here are the key contributions from the paper's abstract and introduction:The research focuses on minimizing human effort in deploying perception-based robotic systems to previously unseen environments, leveraging continual learning and reducing human annotations for efficient learning. The contributions include:1. Continual Learning for Robotics: Equipping an autonomous agent with the capability to adapt to unseen domains while retaining high performance on previous domains.2. Label-Efficient Panoptic Segmentation: Rendering an important step towards widespread adoption of panoptic segmentation networks in robotic use cases by reducing the need for densely labeled training data.3. Fusion of Vision and LiDAR: Fusing the complementary information from cameras and LiDAR sensors for robust and accurate mapping and localization.4. Multi-Agent Collaboration: Enabling collaborative dynamic 3D scene graphs for automated driving and exploring the potential of collaborative robot mapping.5. Foundation Models: Exploiting semantically rich image features from frozen DINOv2 backbones for efficient training and adaptation to new domains.|[2405.14688v1](http://arxiv.org/abs/2405.14688v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose LDM, a novel feed-forward framework that generates high-quality textured meshes from a single image or text prompts, using a multi-view diffusion model and a transformer-based SDF field predictor, achieving efficient and high-quality 3D asset generation.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|Here is a summary of the key contributions in a single sentence under 50 words:JointRF proposes a novel end-to-end joint optimization scheme for dynamic NeRF representation and compression, achieving significantly improved quality and compression efficiency by utilizing a compact residual feature grid, sequential feature compression, and end-to-end training.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|The paper proposes DoGaussian, a distributed method for training 3D Gaussian Splatting (3DGS) on large-scale scenes, which achieves state-of-the-art rendering quality and accelerates training by 6+ times while maintaining one global 3DGS model during inference.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Monocular Gaussian SLAM with Language Extended Loop Closure**|Tian Lan et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:MG-SLAM, a monocular Gaussian SLAM system, introduces a loop closure module using CLIP features for high-level environmental understanding, enables real-time mapping without depth information, and achieves drift-corrected tracking and high-fidelity reconstruction on multiple benchmark datasets.|[2405.13748v1](http://arxiv.org/abs/2405.13748v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes Gaussian Time Machine (GTM), a real-time neural rendering method that models long-term appearance variations in 3D scenes, achieving state-of-the-art rendering fidelity and speed, and disentangling appearance changes from geometry for smooth interpolation.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper introduces MOSS, a novel framework that overcomes limitations in single-view clothed human reconstruction by utilizing kinematic information to achieve motion-aware Gaussian split, improving joint details, clothing folds, and deformation accuracy.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents a novel method that enables the estimation of the 6D pose of an unknown target spacecraft relative to a monocular camera using a Neural Radiance Field (NeRF) trained on a sparse set of images, demonstrating the feasibility of autonomous Rendezvous and Proximity Operations.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-21**|**NV-LIO: LiDAR-Inertial Odometry using Normal Vectors Towards Robust SLAM in Multifloor Environments**|Dongha Chung et.al.|Here is a summary of the key contributions in a single sentence under 50 words:This paper presents NV-LIO, a normal vector-based LiDAR-inertial odometry framework for indoor SLAM in multifloor environments, which enhances point cloud registration performance by utilizing normal vectors and adjusting matching uncertainty in degeneracy situations.|[2405.12563v1](http://arxiv.org/abs/2405.12563v1)|[link](https://github.com/dhchung/nv_lio)|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|Tianqi Liu et.al.|The paper introduces MVSGaussian, a novel generalizable 3D Gaussian representation approach derived from Multi-View Stereo that can efficiently reconstruct unseen scenes with real-time rendering speed and fast per-scene optimization, outperforming previous methods in view synthesis quality and computational cost.|[2405.12218v1](http://arxiv.org/abs/2405.12218v1)|null|
|**2024-05-20**|**Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents**|Guanlin Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper provides a comprehensive overview of integrating neural radiance field (NeRF) and 3D Gaussian splatting (3D-GS) in 6G wireless networks, discussing their applications, implementation challenges, and potential solutions for efficient transmission, reconstruction, and rendering of 3D contents.|[2405.12155v1](http://arxiv.org/abs/2405.12155v1)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|Fotios Logothetis et.al.|The key contributions of the paper can be summarized as follows: The authors present a novel multi-view photometric stereo (MV-PS) method that leverages per-pixel intensity renderings and explicitly models point light attenuation and raytracing cast shadows, achieving competitive reconstruction accuracy using only 6 lights and matching the state-of-the-art performance when using all lights and normal map information.|[2405.12057v1](http://arxiv.org/abs/2405.12057v1)|null|
|**2024-05-19**|**Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems**|Shengxiang Sun et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The paper proposes a modified gradient-based texture optimization method to discover realistic-looking adversarial objects for autonomous driving systems. The core innovation is the introduction of an evaluative mechanism, called the 'Judge', which assesses the realism of generated objects and integrates this evaluation into the loss function to encourage the generation of realistic and adversarial objects. The Judge's evaluation is based on three criteria: color similarity, law of traffic, and real-life appearance. The paper explores four strategies for creating a reliable Judge model and analyzes the effectiveness of these strategies using four images.|[2405.11629v1](http://arxiv.org/abs/2405.11629v1)|null|
|**2024-05-19**|**R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless Environments**|Huiying Yang et.al.|Here is a summary of the key contributions from the abstract and introduction:The paper proposes a novel modeling approach using Neural Radiance Fields (NeRF) to characterize the dynamics of electromagnetic fields in RIS-enabled wireless environments. The method, called R-NeRF, uses NeRF-based ray tracing to intuitively capture and visualize the complex dynamics of signal propagation, enhancing understanding of signal behavior in real-world scenarios. The R-NeRF method predicts the signal field for any specified RIS placement and receiver location, facilitating efficient RIS deployment.|[2405.11541v1](http://arxiv.org/abs/2405.11541v1)|null|
|**2024-05-18**|**Outlier-Robust Long-Term Robotic Mapping Leveraging Ground Segmentation**|Hyungtae Lim et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a robust long-term robotic mapping system that can work out of the box in diverse scenarios by employing fast and robust ground segmentation, outlier-robust registration, hierarchical multi-session SLAM, and instance-aware static map building.|[2405.11176v2](http://arxiv.org/abs/2405.11176v2)|null|
|**2024-05-18**|**MotionGS : Compact Gaussian Splatting SLAM by Motion Filter**|Xinli Guo et.al.|The paper proposes a novel 3DGS-based SLAM approach, MotionGS, which integrates deep visual features, dual keyframe selection, and 3D Gaussian splatting to achieve high-fidelity scene representation, accurate tracking, and efficient mapping with less memory usage.|[2405.11129v1](http://arxiv.org/abs/2405.11129v1)|[link](https://github.com/antonio521/motiongs)|
|**2024-05-17**|**CCTNet: A Circular Convolutional Transformer Network for LiDAR-based Place Recognition Handling Movable Objects Occlusion**|Gang Wang et.al.|The key contributions of the paper are the proposal of a Lightweight Circular Convolutional Transformer network (CCTNet) that captures structural information in point clouds and enables cross-dimensional interaction of spatial and channel information, and an Overlap-based loss function that transforms the place recognition task into a regression problem.|[2405.10793v1](http://arxiv.org/abs/2405.10793v1)|null|
|**2024-05-17**|**Occupancy-SLAM: Simultaneously Optimizing Robot Poses and Continuous Occupancy Map**|Liang Zhao et.al.|The paper proposes a novel optimization-based SLAM approach that simultaneously optimizes the robot trajectory and occupancy map using 2D laser scans and odometry, which is different from existing approaches that optimize robot poses before estimating the map.|[2405.10743v1](http://arxiv.org/abs/2405.10743v1)|null|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|The paper discusses the integration of Large Language Models (LLMs) with 3D spatial data, highlighting the unique advantages of LLMs, such as in-context learning and extensive world knowledge, and their potential to advance spatial comprehension and interaction in embodied Artificial Intelligence systems.|[2405.10255v1](http://arxiv.org/abs/2405.10255v1)|null|
|**2024-05-15**|**From NeRFs to Gaussian Splats, and Back**|Siming He et.al.|The key contributions of the paper are:* Developing a procedure to convert between implicit representations (Neural Radiance Fields) and explicit representations (Gaussian Splatting), allowing for the best of both worlds: superior PSNR, SSIM, and LPIPS on dissimilar views, and a compact representation.* Achieving real-time rendering and the ability to easily modify the representation using GS, while maintaining the quality of NeRFs.* Demonstrating the efficiency of the conversion process, with a minor computational cost compared to training the representations from scratch.|[2405.09717v1](http://arxiv.org/abs/2405.09717v1)|[link](https://github.com/grasp-lyrl/nerftogsandback)|
