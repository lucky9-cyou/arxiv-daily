
### NeRF SLAM
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-29**|**Neural Radiance Fields for Novel View Synthesis in Monocular Gastroscopy**|Zijie Jiang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:* The paper presents a novel approach for synthesizing view-independent images of the stomach from monocular gastroscopic data using Neural Radiance Fields (NeRF) with a geometry-based loss.* The geometry-based loss incorporates a point cloud reconstructed from Structure-from-Motion (SfM) and depth smoothness loss to improve the learned geometry and rendering quality.* The approach addresses view sparsity in local regions of monocular gastroscopy by interpolating consecutive observed views and incorporating unobserved views into the training process.* The method is able to synthesize high-quality images for novel viewpoints not included in the training data, demonstrating its potential for improving diagnostic accuracy in gastroscopy.|[2405.18863v1](http://arxiv.org/abs/2405.18863v1)|null|
|**2024-05-29**|**NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild**|Weining Ren et.al.|Here is a summary of the key contributions in one sentence under 50 words:The paper introduces NeRF On-the-go, a simple yet effective approach that enables robust novel view synthesis in complex real-world scenes from casually captured images by efficiently eliminating distractors and achieving faster convergence speed.|[2405.18715v1](http://arxiv.org/abs/2405.18715v1)|null|
|**2024-05-28**|**Self-supervised Pre-training for Transferable Multi-modal Perception**|Xiaohao Xu et.al.|Here is a summarized version of the key contributions from the paper's abstract and introduction:* The paper proposes a self-supervised pre-training framework for multi-modal representation learning, called NS-MAE, for autonomous driving.* NS-MAE learns transferable multi-modal representations by masking and reconstructing input data from multiple sensors, using a unified framework that combines Neural Radiance Fields (NeRF) and Masked Autoencoder (MAE).* The proposed framework enables effectively learning transferable representations across diverse modalities and tasks, with promising results on various 3D perception tasks.|[2405.17942v1](http://arxiv.org/abs/2405.17942v1)|null|
|**2024-05-28**|**A Refined 3D Gaussian Representation for High-Quality Dynamic Scene Reconstruction**|Bin Zhang et.al.|The key contributions from the paper's abstract and introduction are:1. A hybrid representation that combines deformation fields, hash encoding, and 3D Gaussian Splatting to reduce memory usage while achieving efficient and realistic rendering of dynamic scenes.2. A learnable denoising mask that can effectively identify and remove noise points in 3D Gaussian Splatting, enhancing rendering quality.3. Static constraints and motion consistency constraints that minimize noise in points during motion, ensuring accurate and efficient rendering of dynamic scenes.In summary, the paper presents a novel method for rendering dynamic scenes using 3D Gaussian Splatting, which combines deformation fields, hash encoding, and a learnable denoising mask to achieve high-quality and efficient rendering while reducing memory usage.|[2405.17891v1](http://arxiv.org/abs/2405.17891v1)|null|
|**2024-05-28**|**HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal High-Frequency Components for Endoscopic Scene Reconstruction**|Haoyu Zhao et.al.|Here is a summarized version of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose a novel approach, HFGS, that addresses under-reconstruction in static and dynamic scenes using spatial and temporal frequency perspectives, incorporating deformation fields and emphasizing high-frequency components for superior 3D reconstruction quality.|[2405.17872v2](http://arxiv.org/abs/2405.17872v2)|null|
|**2024-05-28**|**Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh**|Xiangjun Gao et.al.|The key contributions from the paper's abstract and introduction are:* Proposing a method to manipulate 3D Gaussian Splatting (3DGS) using a triangular mesh as the proxy, allowing for direct transfer of mesh manipulation to 3DGS with self-adaptation.* Introducing a triangle shape-aware Gaussian binding strategy with self-adaptation, which has a high tolerance for mesh accuracy and supports various 3DGS manipulations.Note: The summary is limited to a single sentence under 50 words.|[2405.17811v1](http://arxiv.org/abs/2405.17811v1)|null|
|**2024-05-27**|**F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting**|Xiangyu Sun et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose Factorized 3D Gaussian Splatting (F-3DGS), a novel approach that reduces storage requirements while maintaining image quality by factorizing 3D Gaussians using efficient decompositions, achieving a 90% reduction in storage while preserving quality and rendering speed.|[2405.17083v2](http://arxiv.org/abs/2405.17083v2)|null|
|**2024-05-27**|**CudaSIFT-SLAM: multiple-map visual SLAM for full procedure mapping in real human endoscopy**|Richard Elvira et.al.|The paper presents CudaSIFT-SLAM, a real-time V-SLAM system that can process complete human colonoscopies, achieving 88% mapping coverage in the C3VD dataset and 53% coverage in a real screening colonoscopy, improving upon ORB-SLAM3's 22% coverage while coping with occlusions, blur, and other challenging scenes.|[2405.16932v1](http://arxiv.org/abs/2405.16932v1)|null|
|**2024-05-27**|**PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting**|Zipeng Wang et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:Pyramidal 3D Gaussian Splatting (PyGS) with NeRF Initialization introduces a hierarchical assembly of Gaussians and a compact weighting network to dynamically determine their influence during rendering, achieving a significant performance leap and over 400 times faster rendering time compared to state-of-the-art NeRF-based methods.|[2405.16829v3](http://arxiv.org/abs/2405.16829v3)|null|
|**2024-05-26**|**Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians**|Erik Sandström et.al.|The paper proposes a novel RGB-only SLAM system that leverages 3D Gaussian Splatting, achieves superior or on-par performance with existing methods in tracking, mapping, and rendering accuracy, and yields small map sizes and fast runtimes.|[2405.16544v1](http://arxiv.org/abs/2405.16544v1)|null|
|**2024-05-26**|**Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors**|Soumava Paul et.al.|The paper presents SparseSplat360, a method that uses a cascade of models to fill in missing details and clean novel views for sparse-view reconstruction of 360-degree scenes, achieving state-of-the-art performance on the Mip-NeRF360 dataset.|[2405.16517v1](http://arxiv.org/abs/2405.16517v1)|null|
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:* The paper introduces Neural Elevation Models (NEMos), a new terrain representation that combines a Neural Radiance Field (NeRF) with a height field, allowing for the generation of high-quality reconstructions from imagery and a lightweight, continuous, and differentiable terrain model.* NEMos can be generated from low-cost imagery and do not require expensive sensors or traditional Digital Elevation Models (DEMs).* The paper proposes a novel method for jointly training a height field and NeRF using quantile regression, which is an effective approach for regressing the desired quantile of vertical NeRF density as height.* The authors also introduce a path planning algorithm that leverages the continuous and differentiable nature of the height field to achieve smoother paths than those obtained via discrete planning over equivalent DEMs.Let me know if you'd like me to summarize the technical sections as well!|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**NeB-SLAM: Neural Blocks-based Salable RGB-D SLAM for Unknown Scenes**|Lizhi Bai et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The authors propose NeB-SLAM, an end-to-end neural RGB-D SLAM system for unknown scenarios, which uses a divide-and-conquer mapping strategy and adaptive map growth strategy to achieve scalability and real-time performance.|[2405.15151v1](http://arxiv.org/abs/2405.15151v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose High Dynamic Range Gaussian Splatting (HDR-GS), a new framework for novel view synthesis that efficiently renders HDR images and reconstructs LDR images with controllable exposure time, outperforming state-of-the-art NeRF-based methods with much faster training and inference speeds.|[2405.15125v2](http://arxiv.org/abs/2405.15125v2)|[link](https://github.com/caiyuanhao1998/hdr-gs)|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes GS-Hider, a steganography framework for 3D Gaussian Splatting (3DGS) that can embed 3D scenes and images into original point cloud files in an invisible manner, ensuring security, fidelity, capacity, and flexibility, with four primary contributions: first-time attempt at 3DGS steganography, robust security, large capacity, and versatility.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**ETA-INIT: Enhancing the Translation Accuracy for Stereo Visual-Inertial SLAM Initialization**|Han Song et.al.|Key contributions from the abstract and introduction:* Introduce a new initialization method for Stereo Visual-Inertial SLAM (VI-SLAM) that enhances translation accuracy during the initialization stage.* The proposed method improves upon existing methods by refines the translation estimate using a 3-DoF Bundle Adjustment (BA) independently, while keeping the rotation estimate fixed.* Unlike other methods, the proposed approach updates the rotation estimate by taking into account IMU measurements and gyroscope bias.* The method is evaluated on the EuRoC dataset, demonstrating its accuracy and effectiveness.|[2405.15082v1](http://arxiv.org/abs/2405.15082v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|The paper introduces a novel approach to Neural Radiance Fields (NeRFs) that leverages ray tracing to render accurate and consistent reflections of both nearby and distant scene content, improving upon prior methods' limitations in rendering view-dependent appearance and reducing the computational burden.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Synergistic Global-space Camera and Human Reconstruction from Videos**|Yizhou Zhao et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes Synergistic Camera and Human Reconstruction (SynCHMR), a novel method that combines visual SLAM and human mesh recovery to reconstruct camera trajectories, human meshes, and scene point clouds in a common world frame from monocular videos.|[2405.14855v1](http://arxiv.org/abs/2405.14855v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|Here is a summary of the paper's abstract and introduction in one sentence under 50 words:The paper presents Neural Directional Encoding (NDE), a feature-grid-based encoding method for rendering specular objects, which improves upon previous methods by effectively modeling high-frequency angular signals and spatially varying directional encoding.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose a two-stage pipeline that normalizes images with varying lighting and shadow conditions for camera relocalization, leveraging a hash-encoded NeRF with a re-designed low-pass filter and numerical gradient averaging technique to achieve state-of-the-art results.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**CoPeD-Advancing Multi-Robot Collaborative Perception: A Comprehensive Dataset in Real-World Environments**|Yang Zhou et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors present a pioneering real-world multi-robot collaborative perception dataset featuring air-ground robot collaboration, diverse sensor modalities, and comprehensive annotations, filling a significant gap in the existing datasets and paving the way for research on high-level scene understanding in multi-robot settings.|[2405.14731v1](http://arxiv.org/abs/2405.14731v1)|[link](https://github.com/arplaboratory/coped)|
|**2024-05-23**|**Efficient Robot Learning for Perception and Mapping**|Niclas Vödisch et.al.|Here is a summary of the key contributions in the paper's abstract and introduction:* The paper aims to minimize human effort in deploying perception-based robotic systems to previously unseen environments by leveraging continual learning and reducing human annotations for efficient learning.* The research focuses on efficient robot learning for perception and mapping, exploring techniques such as Continual Learning for Robotics, Label-Efficient Panoptic Segmentation, and LiDAR-Based Mapping.* The paper proposes novel methods for unsupervised adaptation, such as continual SLAM, online continual learning for depth estimation and panoptic segmentation, and automatic target-less camera-LiDAR calibration.* The research highlights the importance of exploiting vision foundation models for extremely label-efficient training and demonstrates the potential of these approaches for real-world applications.|[2405.14688v1](http://arxiv.org/abs/2405.14688v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose LDM, a feed-forward framework that generates high-quality, textured 3D meshes from a single image or text prompts, utilizing a tensorial SDF representation and decoupled color field, and achieves fast convergence and high-quality results.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose JointRF, a novel end-to-end joint optimization scheme for dynamic NeRF representation and compression, achieving improved quality and compression efficiency by representing dynamic NeRF with a compact residual feature grid and a coefficient feature grid.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper proposes DoGaussian, a distributed training method for 3D Gaussian Splatting (3DGS) that reduces training time by 6+ times and achieves state-of-the-art rendering quality, addressing the challenges of large-scale scene reconstruction with 3DGS.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Monocular Gaussian SLAM with Language Extended Loop Closure**|Tian Lan et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose MG-SLAM, a monocular Gaussian SLAM system with a language-extended loop closure module, which achieves drift-corrected tracking, high-fidelity reconstruction, and a high-level understanding of the environment without requiring RGB-D inputs.|[2405.13748v1](http://arxiv.org/abs/2405.13748v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|The paper proposes Gaussian Time Machine (GTM), a real-time rendering method that models time-dependent scene attributes with discrete time embedding vectors, achieving state-of-the-art rendering fidelity and real-time rendering capabilities for dynamic scenes with varying appearances.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The authors introduce MOSS, a novel framework that leverages kinematic information to achieve motion-aware Gaussian split on the human surface, improving the realism of reconstructed surfaces and achieving state-of-the-art visual quality in 3D clothed human synthesis from monocular videos.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|The key contributions of the paper can be summarized as follows:The paper presents a novel method for estimating the 6D pose of an unknown target spacecraft relative to a monocular camera, leveraging Neural Radiance Fields (NeRFs) models to generate a large dataset of diverse images and train a pose estimation network.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-21**|**NV-LIO: LiDAR-Inertial Odometry using Normal Vectors Towards Robust SLAM in Multifloor Environments**|Dongha Chung et.al.|The paper presents NV-LIO, a normal vector-based LiDAR-inertial odometry (LIO) framework designed for simultaneous localization and mapping (SLAM) in indoor environments with multifloor structures, which addresses the challenges of point cloud registration in confined indoor settings through normal vector extraction and matching uncertainty adjustment.|[2405.12563v2](http://arxiv.org/abs/2405.12563v2)|[link](https://github.com/dhchung/nv_lio)|
