
### NeRF SLAM
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-29**|**Neural Radiance Fields for Novel View Synthesis in Monocular Gastroscopy**|Zijie Jiang et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper proposes a novel method for novel view synthesis of gastric cavity images using neural radiance fields (NeRF), incorporating a geometry-based loss and unobserved view generation to achieve high-fidelity image renderings from novel viewpoints, outperforming existing methods such as Zip-NeRF and DS-NeRF.|[2405.18863v1](http://arxiv.org/abs/2405.18863v1)|null|
|**2024-05-29**|**NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild**|Weining Ren et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:NeRF On-the-go introduces a simple yet effective approach for robustly synthesizing novel views in complex, real-world scenes from casually captured image sequences, efficiently eliminating distractors and achieving faster convergence speed than state-of-the-art methods.|[2405.18715v1](http://arxiv.org/abs/2405.18715v1)|null|
|**2024-05-28**|**Self-supervised Pre-training for Transferable Multi-modal Perception**|Xiaohao Xu et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:This work presents NeRF-Supervised Masked Auto Encoder (NS-MAE), a self-supervised pre-training paradigm for transferable multi-modal representation learning, which learns to reconstruct missing or corrupted input data across multiple modalities using masked multi-modal reconstruction in neural radiance fields.|[2405.17942v1](http://arxiv.org/abs/2405.17942v1)|null|
|**2024-05-28**|**A Refined 3D Gaussian Representation for High-Quality Dynamic Scene Reconstruction**|Bin Zhang et.al.|The key contributions from the paper's abstract and introduction are:* A hybrid representation combining deformation fields, hash encoding, and 3D Gaussian Splatting (3D-GS) to reduce memory usage while achieving efficient and realistic rendering of dynamic scenes.* A learnable denoising mask in conjunction with noise loss to effectively identify and remove noise points in 3D-GS, enhancing rendering quality.* Static constraints and motion consistency constraints to minimize noise in points during motion, ensuring accurate and efficient rendering of dynamic scenes.Note that the paper does not provide a single sentence summary under 50 words, but the above points summarize the main contributions of the paper.|[2405.17891v1](http://arxiv.org/abs/2405.17891v1)|null|
|**2024-05-28**|**HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal High-Frequency Components for Endoscopic Scene Reconstruction**|Haoyu Zhao et.al.|The paper proposes HFGS, a novel approach for deformable endoscopic reconstruction that addresses under-reconstruction challenges from spatial and temporal frequency perspectives, improving rendering quality and dynamic awareness.|[2405.17872v2](http://arxiv.org/abs/2405.17872v2)|null|
|**2024-05-28**|**Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh**|Xiangjun Gao et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes Mani-GS, a method for Gaussian Splatting manipulation with triangular mesh, which enables controllable 3DGS manipulation, high-quality rendering, and a high tolerance for mesh accuracy, outperforming existing methods in various scenarios.|[2405.17811v1](http://arxiv.org/abs/2405.17811v1)|null|
|**2024-05-27**|**F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting**|Xiangyu Sun et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The authors propose Factorized 3D Gaussian Splatting (F-3DGS), a novel approach that significantly reduces storage requirements while maintaining image quality by factorizing dense clusters of Gaussians using efficient decomposition techniques, enabling fast rendering speeds and compact storage.|[2405.17083v2](http://arxiv.org/abs/2405.17083v2)|null|
|**2024-05-27**|**CudaSIFT-SLAM: multiple-map visual SLAM for full procedure mapping in real human endoscopy**|Richard Elvira et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors present CudaSIFT-SLAM, a V-SLAM system that uses SIFT features and brute-force matching to overcome limitations of ORB-SLAM3, achieving real-time processing of human colonoscopies and significantly improving sub-map merging and relocation capabilities.|[2405.16932v1](http://arxiv.org/abs/2405.16932v1)|null|
|**2024-05-27**|**PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting**|Zipeng Wang et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:Pyramidal 3D Gaussian Splatting (PyGS) is a novel approach that enhances the capability of 3D Gaussian Splatting to model large-scale scenes with high-frequency details, accelerating rendering performance by over 400 times compared to state-of-the-art NeRF-based methods.|[2405.16829v3](http://arxiv.org/abs/2405.16829v3)|null|
|**2024-05-26**|**Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians**|Erik Sandstr√∂m et.al.|Here is a summary of the paper's abstract and introduction in one sentence under 50 words:The paper presents an RGB-only SLAM system utilizing a dense 3D Gaussian map representation that adapts to keyframe pose and depth updates, achieving superior or comparable performance to existing methods in tracking, mapping, and rendering accuracy, while yielding small map sizes and fast runtimes.|[2405.16544v1](http://arxiv.org/abs/2405.16544v1)|null|
|**2024-05-26**|**Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors**|Soumava Paul et.al.|The paper proposes SparseSplat360, a method that uses pre-trained 2D diffusion models with fine-tuning to reconstruct a 360-degree 3D scene from sparse views, achieving multi-view consistency and coherent details.|[2405.16517v1](http://arxiv.org/abs/2405.16517v1)|null|
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|According to the abstract and introduction, the key contributions of this paper are:* Neural Elevation Models (NEMos), which integrate a Neural Radiance Field (NeRF) and a height field to generate a continuous and differentiable terrain model from 2D camera images.* A novel method for joint training of the NeRF and height field using quantile regression, which masks out spurious density above the ground in free space and improves the quality of the NeRF representation.* A path planning algorithm that leverages the continuous and differentiable nature of the height field to produce smoother and more efficient paths compared to traditional discrete path planning methods.* Experiment results on simulated and real-world terrain imagery, demonstrating the ability of NEMos to generate high-quality reconstructions and produce smooth paths.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**NeB-SLAM: Neural Blocks-based Salable RGB-D SLAM for Unknown Scenes**|Lizhi Bai et.al.|Here is a summarized sentence under 50 words:The paper proposes NeB-SLAM, a neural block-based, scalable RGB-D SLAM method for unknown scenes, which uses a divide-and-conquer mapping strategy, adaptive map growth, and neural blocks to achieve competitive performance in mapping and tracking.|[2405.15151v1](http://arxiv.org/abs/2405.15151v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes High Dynamic Range Gaussian Splatting (HDR-GS), a novel framework for 3D HDR imaging, which outperforms state-of-the-art NeRF-based methods in HDR novel view synthesis, rendering HDR images, and reconstructing LDR views with controllable exposure time.|[2405.15125v2](http://arxiv.org/abs/2405.15125v2)|[link](https://github.com/caiyuanhao1998/hdr-gs)|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose GS-Hider, a steganography framework for 3D Gaussian Splatting (3DGS) that embeds 3D scenes and images into original GS point clouds while ensuring security, fidelity, and flexibility, addressing the challenges of protecting 3D assets in 3D scene reconstruction and novel view synthesis.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**ETA-INIT: Enhancing the Translation Accuracy for Stereo Visual-Inertial SLAM Initialization**|Han Song et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a method, called ETA, to enhance translation accuracy during initialization in stereo Visual-Inertial SLAM systems, which outperforms existing methods in terms of accuracy and runtime, relying on a 3-DoF Bundle Adjustment and considering IMU measurements and gyroscope bias.|[2405.15082v1](http://arxiv.org/abs/2405.15082v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose a ray-tracing approach to Neural Radiance Fields (NeRFs) that addresses issues with rendering highly specular objects by casting reflection rays from camera points and tracing them through the NeRF geometry, resulting in consistent and high-quality reflections of nearby and distant content.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Synergistic Global-space Camera and Human Reconstruction from Videos**|Yizhou Zhao et.al.|Here is a summary of the key contributions in under 50 words:The paper introduces SynCHMR, a novel approach that marries monocular SLAM and human mesh recovery, allowing for the reconstruction of camera trajectories, human meshes, and dense scene point clouds in a common world frame, addressing depth, scale, and dynamic ambiguities.|[2405.14855v1](http://arxiv.org/abs/2405.14855v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The paper presents Neural Directional Encoding (NDE), a feature-grid-based approach that encodes view-dependent appearance of specular objects, such as shiny metals or glossy paints, with neural radiance fields (NeRF) for rendering specular objects, achieving high-quality modeling of view-dependent effects and fast evaluation, outperforming the state-of-the-art methods on both synthetic and real datasets.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|Here is a summarized version of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a two-staged pipeline for camera relocalization that normalizes images with varying lighting conditions using a hash-encoded NeRF, and addresses the noisy image gradient computing problem with a re-devised truncated dynamic low-pass filter and numerical gradient averaging technique.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**CoPeD-Advancing Multi-Robot Collaborative Perception: A Comprehensive Dataset in Real-World Environments**|Yang Zhou et.al.|The paper's abstract and introduction highlight the lack of real-world datasets for multi-robot collaborative perception, which has led to the construction of a pioneering dataset featuring air-ground robot collaboration, diverse spatial viewpoints, and complementary robot mobilities and sensor modalities.|[2405.14731v1](http://arxiv.org/abs/2405.14731v1)|[link](https://github.com/arplaboratory/coped)|
|**2024-05-23**|**Efficient Robot Learning for Perception and Mapping**|Niclas V√∂disch et.al.|Here is a summary of the key contributions mentioned in the abstract and introduction:* The author aims to investigate how to minimize human effort in deploying perception-based robotic systems to previously unseen environments while leveraging continual learning and reducing human annotations.* The author focuses on label-efficient learning methods, particularly exploiting visual foundation models to train robots for downstream tasks with minimal labeled data.* The author contributes to continual unsupervised adaptation of robotic vision systems to new domains without decreasing performance in prior environments.* The author also works on label-efficient panoptic segmentation, using techniques that enable inexpensive in-domain training via weak supervision, and applying vision foundation models for bootstrapping image representations.* The author proposes novel methods for target-less camera-LiDAR calibration and for efficient robot learning via multi-agent collaboration.|[2405.14688v1](http://arxiv.org/abs/2405.14688v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose LDM, a novel feed-forward framework that generates high-fidelity, illumination-decoupled textured meshes from a single image or text prompts, leveraging tensorial SDF representations and adaptive beta adjustment to achieve faster convergence and higher-quality results.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|The paper proposes JointRF, a novel end-to-end joint optimization scheme for dynamic Neural Radiance Field (NeRF) representation and compression, achieving significantly improved quality and compression efficiency compared to previous methods.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes DoGaussian, a distributed method for training 3D Gaussian Splatting (3DGS) models on large-scale scenes, achieving training time reduction by 6+ times and state-of-the-art rendering quality while maintaining rendering efficiency.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Monocular Gaussian SLAM with Language Extended Loop Closure**|Tian Lan et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper presents MG-SLAM, a monocular Gaussian SLAM system with a language-extended loop closure module, which enables drift-corrected tracking and high-fidelity reconstruction without RGB-D inputs, achieving a high-level understanding of the environment through CLIP feature-based loop closure.|[2405.13748v1](http://arxiv.org/abs/2405.13748v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes Gaussian Time Machine (GTM), a real-time rendering method that models time-dependent Gaussian primitives, achieving state-of-the-art rendering quality and speed, and disentangling appearance changes from geometry for smooth interpolation.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The MOSS framework, comprising KGAS and UID modules, innovative techniques that incorporate kinematic information to achieve motion-aware Gaussian split and surface deformation reconstruction, resulting in state-of-the-art visual quality and improved 3D clothed human synthesis from monocular videos.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents a novel method that enables the training of an off-the-shelf spacecraft pose estimation network from a sparse set of images, using a Neural Radiance Field (NeRF) to generate a large dataset of images depicting the target spacecraft under varying illumination conditions.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-21**|**NV-LIO: LiDAR-Inertial Odometry using Normal Vectors Towards Robust SLAM in Multifloor Environments**|Dongha Chung et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:This paper presents NV-LIO, a normal vector-based LiDAR-inertial odometry framework for simultaneous localization and mapping (SLAM) in indoor environments with multifloor structures, which extracts normal vectors from LiDAR scans for enhanced point cloud registration and addresses degeneracy situations and loop closure.|[2405.12563v2](http://arxiv.org/abs/2405.12563v2)|[link](https://github.com/dhchung/nv_lio)|
