
### NeRF SLAM
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|Here are the key contributions from the paper's abstract and introduction:* Introduction of Neural Elevation Models (NEMos) that adapt Neural Radiance Fields (NeRFs) to a 2.5D continuous and differentiable terrain model.The authors propose a novel method for jointly training a height field and radiance field within a NeRF framework using quantile regression. This allows for generating high-quality reconstructions and producing smoother paths compared to discrete path planning methods.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**NeB-SLAM: Neural Blocks-based Salable RGB-D SLAM for Unknown Scenes**|Lizhi Bai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose NeB-SLAM, a scalable and real-time neural-based RGB-D SLAM system for unknown scenes, featuring a divide-and-conquer mapping strategy and adaptive map growth, which achieves efficient memory usage and reasonable hole-filling.|[2405.15151v1](http://arxiv.org/abs/2405.15151v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|The key contributions of the paper are: proposing a new framework, High Dynamic Range Gaussian Splatting (HDR-GS), for 3D HDR imaging, introducing a Dual Dynamic Range Gaussian point cloud model that can jointly model HDR and LDR colors, and achieving efficient rendering of HDR images and LDR views with controllable exposure time while outperforming state-of-the-art methods.|[2405.15125v1](http://arxiv.org/abs/2405.15125v1)|null|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|The paper proposes GS-Hider, a novel steganography framework for 3D Gaussian Splatting (3DGS) that embeds 3D scenes or images into original 3DGS point cloud files in an invisible manner, enabling copyright protection, encrypted communication, and compression of 3DGS.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**ETA-INIT: Enhancing the Translation Accuracy for Stereo Visual-Inertial SLAM Initialization**|Han Song et.al.|The key contributions from the paper's abstract and introduction are:* A new method for enhancing translation accuracy during the initialization stage of Stereo Visual-Inertial SLAM (VI-SLAM) systems.* The method uses a 3 Degree-of-Freedom (DoF) Bundle Adjustment (BA) to refine the translation estimate independently, while keeping the rotation estimate fixed.* The rotation estimate is updated by considering IMU measurements and gyroscope bias, unlike ORB-SLAM3's rotation which is directly obtained from stereo visual odometry.* The method is evaluated on the public EuRoC dataset, demonstrating improved accuracy compared to existing methods.|[2405.15082v1](http://arxiv.org/abs/2405.15082v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|The key contributions from the paper are a ray-tracing based approach that overcomes limitations of Neural Radiance Fields (NeRFs) in rendering specular objects and synthesizing consistent reflections of nearby and distant content with reduced computational costs.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Synergistic Global-space Camera and Human Reconstruction from Videos**|Yizhou Zhao et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes Synergistic Camera and Human Reconstruction (SynCHMR), which jointly reconstructs camera trajectories, human meshes, and scene point clouds from monocular videos by marrying SLAM and Human Mesh Recovery, addressing depth, scale, and dynamic ambiguities, and achieving consistent reconstructions in a common world frame.|[2405.14855v1](http://arxiv.org/abs/2405.14855v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|The paper presents Neural Directional Encoding (NDE), a novel approach for novel-view synthesis of specular objects, using neural radiance fields (NeRF) to render high-frequency view-dependent appearance, including reflections of other objects in the environment, with fast evaluation and high-quality modeling of glossy interreflections.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|Here is a summary of the abstract and introduction in a single sentence under 50 words:The authors propose a two-staged pipeline that normalizes images with varying lighting and shadow conditions to improve camera relocalization using a hash-encoded NeRF and a revised truncated dynamic low-pass filter, achieving state-of-the-art results under varying lighting conditions.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**CoPeD-Advancing Multi-Robot Collaborative Perception: A Comprehensive Dataset in Real-World Environments**|Yang Zhou et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The authors introduce a pioneering real-world dataset for multi-robot collaborative perception, featuring air-ground robot collaboration with diverse spatial viewpoints, sensor modalities, and coverage ranges, to facilitate research in this underexplored area.|[2405.14731v1](http://arxiv.org/abs/2405.14731v1)|[link](https://github.com/arplaboratory/coped)|
|**2024-05-23**|**Efficient Robot Learning for Perception and Mapping**|Niclas VÃ¶disch et.al.|The key contributions from the paper's abstract and introduction are:* The development of methods for efficient and label-efficient learning of robotic perception and mapping systems, which can adapt to new domains and environments without decreasing performance on previous ones.* The use of continual learning and domain adaptation to address the limitations of traditional deep learning approaches, which rely on large datasets and require significant human annotation.* The exploration of foundation models and transfer learning for robot learning, which can be used to reduce the amount of human annotations required and improve the performance of downstream tasks.* The presentation of several research contributions, including continual SLAM, label-efficient panoptic segmentation, and collaborative robot mapping, which demonstrate the feasibility and effectiveness of the proposed approaches.|[2405.14688v1](http://arxiv.org/abs/2405.14688v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|The paper proposes LDM, a novel feed-forward framework that generates high-fidelity, illumination-decoupled textured mesh from a single image or text prompts, utilizing a multi-view diffusion model, transformer-based model, and gradient-based mesh optimization layer.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|The authors propose JointRF, an end-to-end joint optimization scheme for dynamic NeRF representation and compression, achieving improved quality and compression efficiency by employing a compact residual feature grid and a coefficient feature grid, and sequential feature compression.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose DoGaussian, a distributed method for training large-scale 3D Gaussian Splatting (3DGS) models, which accelerates training by 6+ times while achieving state-of-the-art rendering quality, and introduces a recursive scene decomposition approach to ensure balanced block sizes.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Monocular Gaussian SLAM with Language Extended Loop Closure**|Tian Lan et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors introduce MG-SLAM, a monocular Gaussian SLAM system that represents the global map as 3D Gaussian and uses a language-extended loop closure module based on CLIP features to perform drift-corrected tracking, high-fidelity reconstruction, and high-level understanding of the environment.|[2405.13748v1](http://arxiv.org/abs/2405.13748v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The Gaussian Time Machine (GTM) method achieves state-of-the-art rendering fidelity on 3D scenes with varying appearances, rendering 100 times faster than NeRF-based methods, and disentangling appearance changes while maintaining geometric consistency.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|The paper introduces an innovative framework, MOSS, for single-view clothed human reconstruction, which employs kinematic information to achieve motion-aware Gaussian splitting and addresses local occlusions by detecting significant surfaces and performing geometric reconstruction.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|The paper presents a novel method for estimating the 6D pose of an unknown target spacecraft relative to a monocular camera, essential for autonomous rendezvous and proximity operations in Active Debris Removal missions. The method relies on an in-the-wild Neural Radiance Field (NeRF) trained on a small set of images of the target spacecraft, which enables the use of an "off-the-shelf" spacecraft pose estimation network without requiring the CAD model of the target.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-21**|**NV-LIO: LiDAR-Inertial Odometry using Normal Vectors Towards Robust SLAM in Multifloor Environments**|Dongha Chung et.al.|The paper presents NV-LIO, a normal vector-based LIO framework for SLAM in indoor environments with multifloor structures, which extracts normal vectors from LiDAR scans for correspondence search and addresses degeneracy situations through uncertainty covariance matrix calculation, achieving robust registration and loop closure.|[2405.12563v1](http://arxiv.org/abs/2405.12563v1)|[link](https://github.com/dhchung/nv_lio)|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|Tianqi Liu et.al.|Here is a summary of the key contributions in a single sentence under 50 words:MVSGaussian, a novel generalizable 3D Gaussian representation approach, proposes a hybrid rendering method, a consistent aggregation strategy, and a Multi-View Stereo-derived Gaussian representation, achieving state-of-the-art performance with real-time rendering and fast per-scene optimization.|[2405.12218v1](http://arxiv.org/abs/2405.12218v1)|null|
|**2024-05-20**|**Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents**|Guanlin Wu et.al.|The key contributions mentioned in the abstract and introduction are:* A comprehensive overview of the integration of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3D-GS) in 6G wireless networks.* A review of the basics of radiance field rendering techniques and their applications in wireless networks.* Techniques for over-the-air training of NeRF and 3D-GS models using federated learning methods.* Design of three practical rendering architectures for NeRF and 3D-GS models at the wireless network edge.* Model compression approaches to facilitate the transmission of radiance field models.* Acceleration techniques and joint computation and communication designs to enhance rendering efficiency.* Proposals for semantic communication enabled 3D content transmission designs using radiance field models as semantic knowledge bases.* Exploration of applications of radiance field rendering in wireless applications such as radio mapping and radio imaging.|[2405.12155v1](http://arxiv.org/abs/2405.12155v1)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|Fotios Logothetis et.al.|Here is a summarized version of the key contributions from the paper's abstract and introduction, condensed into a single sentence under 50 words:The paper presents a novel multi-view photometric stereo method that leverages per-pixel intensity renderings, models point light attenuation and cast shadows, and optimizes a fully neural material renderer to achieve competitive reconstruction accuracy using only 6 lights.|[2405.12057v1](http://arxiv.org/abs/2405.12057v1)|null|
|**2024-05-19**|**Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems**|Shengxiang Sun et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The authors propose a novel approach for generating realistic-looking adversarial objects for autonomous driving systems. They introduce a new component called the "Judge" that evaluates the realism of generated objects and incentivizes the generation of both adversarial and realistic characteristics. The authors demonstrate four strategies for developing a robust Judge, and analyze their effectiveness in evaluating the realism of various objects. Overall, the proposed approach provides a more realistic and adaptive way to test the resilience of autonomous driving systems.|[2405.11629v1](http://arxiv.org/abs/2405.11629v1)|null|
|**2024-05-19**|**R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless Environments**|Huiying Yang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose a novel NeRF-based ray tracing method to model dynamic electromagnetic fields in RIS-enabled environments, accurately capturing complex propagation dynamics and predicting signal fields at different receiver locations for efficient RIS deployment.|[2405.11541v1](http://arxiv.org/abs/2405.11541v1)|null|
|**2024-05-18**|**Outlier-Robust Long-Term Robotic Mapping Leveraging Ground Segmentation**|Hyungtae Lim et.al.|The paper proposes a robust long-term robotic mapping system that combines fast ground segmentation, outlier-robust registration, hierarchical multi-session SLAM, and instance-aware static map building to enable robust mapping and localization in diverse real-world scenarios.|[2405.11176v2](http://arxiv.org/abs/2405.11176v2)|null|
|**2024-05-18**|**MotionGS : Compact Gaussian Splatting SLAM by Motion Filter**|Xinli Guo et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a novel 3DGS-based SLAM approach, MotionGS, which integrates deep feature extraction, dual keyframe selection, and 3D Gaussian splatting, achieving state-of-the-art performance in tracking, mapping, and rendering with reduced memory usage.|[2405.11129v1](http://arxiv.org/abs/2405.11129v1)|[link](https://github.com/antonio521/motiongs)|
|**2024-05-17**|**CCTNet: A Circular Convolutional Transformer Network for LiDAR-based Place Recognition Handling Movable Objects Occlusion**|Gang Wang et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The authors propose a lightweight circular convolutional Transformer network, CCTNet, which captures structural information in point clouds, enhances place recognition accuracy, and surpasses comparable methods in KITTI and Ford Campus datasets.|[2405.10793v1](http://arxiv.org/abs/2405.10793v1)|null|
|**2024-05-17**|**Occupancy-SLAM: Simultaneously Optimizing Robot Poses and Continuous Occupancy Map**|Liang Zhao et.al.|Here is a summarized single sentence under 50 words:The paper proposes an optimization-based SLAM approach that simultaneously optimizes robot trajectory and occupancy map using 2D laser scans and odometry information, offering improved accuracy and uncertainty estimation over existing approaches.|[2405.10743v1](http://arxiv.org/abs/2405.10743v1)|null|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|Here is a single sentence summarizing the paper's abstract and introduction, under 50 words:The paper surveys the advancements in integrating Large Language Models (LLMs) with 3D spatial data, highlighting the unique benefits of LLMs, such as in-context learning and open-vocabulary capabilities, and exploring their applications in 3D scene understanding, captioning, and navigation.|[2405.10255v1](http://arxiv.org/abs/2405.10255v1)|null|
|**2024-05-15**|**From NeRFs to Gaussian Splats, and Back**|Siming He et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:* The paper presents an efficient procedure to convert between implicit rendering representations (Neural Radiance Fields, NeRFs) and explicit rendering representations (Gaussian Splatting, GS).* The proposed approach, called GSNeRF, enables the best of both worlds: superior image quality and compact representation of NeRFs, combined with fast rendering and easily modifiable representation of GS.* The authors show that NeRFs generalize better to views that are different from the training data, while GS can render at faster speeds.* The conversion process between NeRFs and GS is minor compared to training the two from scratch.Note: The single sentence summary is under 50 words as requested.|[2405.09717v1](http://arxiv.org/abs/2405.09717v1)|[link](https://github.com/grasp-lyrl/nerftogsandback)|
