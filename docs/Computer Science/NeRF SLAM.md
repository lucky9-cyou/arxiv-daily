
### NeRF SLAM
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|Here is a summarized version of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper introduces Neural Elevation Models (NEMos), a novel representation that combines Neural Radiance Fields with a height field, allowing for continuous and differentiable terrain modeling and path planning, enabling smooth and efficient navigation in challenging terrain.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**NeB-SLAM: Neural Blocks-based Salable RGB-D SLAM for Unknown Scenes**|Lizhi Bai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose NeB-SLAM, a neural block-based dense RGB-D SLAM system for unknown scenarios, which utilizes a divide-and-conquer mapping strategy, adaptive map growth strategy, and neural implicit representations to achieve scalable and real-time SLAM capabilities.|[2405.15151v1](http://arxiv.org/abs/2405.15151v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose High Dynamic Range Gaussian Splatting (HDR-GS), a novel framework for 3D HDR imaging that can efficiently render novel HDR views and reconstruct LDR images with controllable exposure time, outperforming state-of-the-art methods in terms of quality and speed.|[2405.15125v1](http://arxiv.org/abs/2405.15125v1)|null|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors proposed a steganography framework, GS-Hider, for 3D Gaussian Splatting (3DGS) to embed and extract 3D scenes and images while maintaining security, fidelity, and functionality, with applications in copyright protection, encrypted communication, and compression of 3DGS.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**ETA-INIT: Enhancing the Translation Accuracy for Stereo Visual-Inertial SLAM Initialization**|Han Song et.al.|The key contributions from the paper's abstract and introduction are:* A new method for enhancing translation accuracy during the initialization stage in Stereo Visual-Inertial SLAM (VI-SLAM) systems.* The method improves translation estimate using a 3 Degree-of-Freedom (DoF) Bundle Adjustment (BA) independently while keeping rotation estimate fixed.* Unlike ORB-SLAM3, the rotation estimate is updated by considering IMU measurements and gyroscope bias.* The method is evaluated on the EuRoC dataset, demonstrating improved accuracy.* The paper aims to address the limitations of previous initialization methods, including ORB-SLAM3 and Stereo-NEC, by providing a more accurate and efficient solution for VI-SLAM systems.|[2405.15082v1](http://arxiv.org/abs/2405.15082v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|The paper addresses the limitations of Neural Radiance Fields (NeRFs) in rendering highly specular objects with detailed view-dependent appearance, by introducing a ray tracing approach that reduces the reliance on expensive neural networks and enables consistent rendering of reflections of nearby and distant content.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Synergistic Global-space Camera and Human Reconstruction from Videos**|Yizhou Zhao et.al.|The paper introduces Synergistic Camera and Human Reconstruction (SynCHMR), a method that jointly reconstructs 3D human and camera motion in a common global coordinate system from monocular videos, addressing depth, scale, and dynamic ambiguities by combining Human-aware Metric SLAM and Scene-aware SMPL Denoiser.|[2405.14855v1](http://arxiv.org/abs/2405.14855v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors introduce Neural Directional Encoding (NDE), a feature-grid-like encoding for neural radiance fields (NeRF) that accurately models high-frequency angular signals and spatially varying directional effects for rendering specular objects, outperforming the state-of-the-art while achieving fast evaluation.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|The paper proposes a two-staged pipeline that normalizes images with varying lighting and shadow conditions for camera relocalization, utilizing a hash-encoded NeRF to improve pose optimization and a re-devised truncated dynamic low-pass filter and numerical gradient averaging technique to smoothen the process, achieving state-of-the-art results in camera relocalization.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**CoPeD-Advancing Multi-Robot Collaborative Perception: A Comprehensive Dataset in Real-World Environments**|Yang Zhou et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents a pioneering real-world multi-robot collaborative perception dataset, featuring air-ground robot collaboration, distinct spatial viewpoints, and complementary sensor modalities, which aims to facilitate research in this area and unlock high-level scene understanding through multi-modal collaborative perception.|[2405.14731v1](http://arxiv.org/abs/2405.14731v1)|[link](https://github.com/arplaboratory/coped)|
|**2024-05-23**|**Efficient Robot Learning for Perception and Mapping**|Niclas VÃ¶disch et.al.|The key contributions from the paper's abstract and introduction are:* Developing methods for efficient robot learning, focusing on minimizing human effort in deploying perception-based robotic systems to previously unseen environments.* Investigating the use of continual learning and reducing human annotations for efficient learning.* Contributing to the continual unsupervised adaptation of robotic vision systems to new domains while maintaining performance in prior environments.* Developing label-efficient panoptic segmentation techniques that can be trained with as few as ten annotated images.* Introducing a novel method for automatic target-less camera-LiDAR calibration that requires neither human initialization nor special data recording.* Investigating the use of foundation models for processing LiDAR data and developing a mapping approach that fuses surround view camera data with sparse radar measurements.|[2405.14688v1](http://arxiv.org/abs/2405.14688v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes LDM, a feed-forward framework that generates high-quality, illumination-decoupled textured meshes from single images or text prompts using a novel tensorial SDF representation, adaptively converting SDF to density, and integrating a gradient-based mesh optimization layer.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The authors propose JointRF, a novel end-to-end joint optimization scheme that efficiently represents and compresses dynamic Neural Radiance Fields (NeRF) for photo-realistic volumetric videos, achieving superior quality and compression efficiency compared to previous methods.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The authors propose DoGaussian, a method that trains 3D Gaussian Splatting (3DGS) in a distributed manner using a recursive block splitting approach, reducing training time by 6+ times and achieving state-of-the-art rendering quality on large-scale scenes.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Monocular Gaussian SLAM with Language Extended Loop Closure**|Tian Lan et.al.|The authors present MG-SLAM, a monocular SLAM system that uses a 3D Gaussian map representation and a language-extended loop closure module, achieving drift-corrected tracking and high-fidelity reconstruction without requiring depth sensors.|[2405.13748v1](http://arxiv.org/abs/2405.13748v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|Here is a summary of the key contributions in a single sentence under 50 words:Gaussian Time Machine (GTM) is proposed for reconstructing 3D scenes with vastly varying appearances, achieving state-of-the-art rendering fidelity, and real-time rendering at 80FPS, 100 times faster than NeRF-based methods, while disentangling appearance changes and rendering smooth interpolation.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The MOSS framework employs kinematic information to achieve motion-aware 3D clothed human synthesis, introducing the KGAS and UID modules to correct for global motion and local occlusions, achieving state-of-the-art visual quality in 3D clothed human synthesis from monocular videos.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents a novel method that enables the training of an "off-the-shelf" spacecraft pose estimation network from a sparse set of images, using a Neural Radiance Field (NeRF) model to generate a large training set that captures the diversity of pose and illumination conditions.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-21**|**NV-LIO: LiDAR-Inertial Odometry using Normal Vectors Towards Robust SLAM in Multifloor Environments**|Dongha Chung et.al.|Here is a summary of the key contributions mentioned in the abstract and introduction in a single sentence under 50 words:The authors propose NV-LIO, a normal vector-based LiDAR-inertial odometry framework for indoor SLAM in multifloor environments, which enhances point cloud registration performance, addresses degeneracy issues, and achieves robustness in narrow indoor spaces.|[2405.12563v2](http://arxiv.org/abs/2405.12563v2)|[link](https://github.com/dhchung/nv_lio)|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|Tianqi Liu et.al.|The key contributions of the paper can be summarized as: Developing a generalizable 3D Gaussian representation approach, MVSGaussian, derived from Multi-View Stereo, which can efficiently reconstruct unseen scenes with real-time rendering and fast fine-tuning.|[2405.12218v1](http://arxiv.org/abs/2405.12218v1)|null|
|**2024-05-20**|**Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents**|Guanlin Wu et.al.|The key contributions from the paper's abstract and introduction are:* The authors focus on the integration of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3D-GS) in 6G networks, which is essential for immersive communication applications requiring photorealistic rendering of 3D contents.* NeRF and 3D-GS are novel radiance field rendering techniques that can provide photorealistic rendering results for complex scenes, but they pose challenges for transmission, storage, and rendering over wireless networks.* The authors aim to bridge the gap by proposing a comprehensive overview of the integration of NeRF and 3D-GS in 6G networks, focusing on representation, transmission, and reconstruction of 3D contents.|[2405.12155v1](http://arxiv.org/abs/2405.12155v1)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|Fotios Logothetis et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The paper presents a novel multi-view photometric stereo (PS) method that leverages neural shape representations and learnt renderers to estimate 3D shape from photometric stereo images. The method models point light attenuation and explicitly raytraces cast shadows to best approximate each point's incoming radiance, and jointly optimizes a fully neural material renderer with the surface. The method achieves competitive reconstruction accuracy using only 6 lights and matches the state-of-the-art performance when all lights and normal map information are used.|[2405.12057v1](http://arxiv.org/abs/2405.12057v1)|null|
|**2024-05-19**|**Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems**|Shengxiang Sun et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a modified gradient-based texture optimization method to discover realistic-looking adversarial objects for autonomous driving systems, introducing a Judge mechanism to assess and enhance the realism of generated objects.|[2405.11629v1](http://arxiv.org/abs/2405.11629v1)|null|
|**2024-05-19**|**R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless Environments**|Huiying Yang et.al.|The key contributions of this paper are:* Developing a novel modeling approach using Neural Radiance Fields (NeRF) to characterize the dynamics of electromagnetic fields in RIS-enabled wireless environments.* Proposing a two-stage framework that accurately captures signal propagation dynamics from the transmitter to the RIS and from the RIS to the receiver.* Experimentally evaluating the effectiveness of the proposed method using simulated and real-world data, demonstrating significant improvements in predicting signal strength and outperforming existing methods.|[2405.11541v1](http://arxiv.org/abs/2405.11541v1)|null|
|**2024-05-18**|**Outlier-Robust Long-Term Robotic Mapping Leveraging Ground Segmentation**|Hyungtae Lim et.al.|The paper proposes a robust long-term robotic mapping system that combines fast ground segmentation, outlier-robust registration using graduated non-convexity, hierarchical multi-session SLAM, and instance-aware static map building to enable robust localization and mapping in diverse environments and scenarios.|[2405.11176v2](http://arxiv.org/abs/2405.11176v2)|null|
|**2024-05-18**|**MotionGS : Compact Gaussian Splatting SLAM by Motion Filter**|Xinli Guo et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper proposes a novel 3DSG-based SLAM approach, MotionGS, which integrates deep feature extraction, dual keyframe selection, and 3D Gaussian splatting to achieve accurate real-time tracking, high-fidelity reconstruction, and state-of-the-art performance on indoor RGB-D datasets.|[2405.11129v1](http://arxiv.org/abs/2405.11129v1)|[link](https://github.com/antonio521/motiongs)|
|**2024-05-17**|**CCTNet: A Circular Convolutional Transformer Network for LiDAR-based Place Recognition Handling Movable Objects Occlusion**|Gang Wang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes the Circular Convolutional Transformer Network (CCTNet) for place recognition using LiDAR point cloud data, addressing issues in previous methods by introducing Circular Convolution and Range Transformer modules to capture structural information and enhance accuracy in varied scenarios.|[2405.10793v2](http://arxiv.org/abs/2405.10793v2)|null|
|**2024-05-17**|**Occupancy-SLAM: Simultaneously Optimizing Robot Poses and Continuous Occupancy Map**|Liang Zhao et.al.|The paper proposes a novel optimization-based SLAM approach that simultaneously optimizes robot trajectory and occupancy map using 2D laser scans and odometry information, while traditional methods require pre-estimating robot poses, and evaluates its performance through simulations and publicly available datasets.|[2405.10743v1](http://arxiv.org/abs/2405.10743v1)|null|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:This survey paper provides an overview of the methodologies enabling large language models (LLMs) to process, understand, and generate 3D data, highlighting their potential to advance spatial comprehension and interaction within AI systems, and examining current challenges and applications in this domain.|[2405.10255v1](http://arxiv.org/abs/2405.10255v1)|null|
|**2024-05-15**|**From NeRFs to Gaussian Splats, and Back**|Siming He et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The authors develop a procedure to convert between implicit representations of scenes, such as Neural Radiance Fields (NeRFs), and explicit representations, such as Gaussian Splatting (GS). This conversion enables the best of both approaches, achieving superior PSNR, SSIM, and LPIPS on dissimilar views, while also allowing for real-time rendering and easy modification of the representation. The authors demonstrate that their approach outperforms both NeRFs and GS-based methods in various robotics applications, such as localization, planning, and scene understanding.|[2405.09717v1](http://arxiv.org/abs/2405.09717v1)|[link](https://github.com/grasp-lyrl/nerftogsandback)|
