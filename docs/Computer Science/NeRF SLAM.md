
### NeRF SLAM
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:**Abstract:*** Introduced Neural Elevation Models (NEMos), which adapt Neural Radiance Fields (NeRFs) to a 2.5D continuous and differentiable terrain model.* NEMos simultaneously train a height field and radiance field within a NeRF framework, leveraging quantile regression.* Proposed a method for joint training of height field and radiance field.* Demonstrated the ability of NEMos to generate high-quality reconstructions and produce smoother paths compared to discrete path planning methods.* Discussed future work to incorporate features and semantics into the height field, creating a generalized terrain model.**Introduction:*** Discussed the need for navigation methods that can effectively model and leverage those models for safe and efficient navigation in challenging terrain environments.* Introduced Neural Elevation Models, which combine the strengths of NeRFs in capturing complex terrain detail and the suitability of Digital Elevation Models (DEMs) for path planning.* Noted that traditional terrain representations (DEMs) rely on expensive sensors, while methods like multi-view stereo (MVS) and photogrammetry rely on inexpensive cameras, but are limited by their discrete representation.* Stated that Neural Elevation Models (NEMos) can be generated from imagery, providing a lightweight representation of terrain through an implicit continuous and differentiable height field.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**NeB-SLAM: Neural Blocks-based Salable RGB-D SLAM for Unknown Scenes**|Lizhi Bai et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose NeB-SLAM, a scalable and real-time RGB-D SLAM system for unknown scenes, which uses a divide-and-conquer mapping strategy and adaptive map growth to achieve full coverage and efficient memory usage.|[2405.15151v1](http://arxiv.org/abs/2405.15151v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes High Dynamic Range Gaussian Splatting (HDR-GS), a novel framework for 3D HDR imaging, which renders HDR views and reconstructs LDR images with controllable exposure time, outperforming state-of-the-art NeRF-based methods in terms of quality and speed.|[2405.15125v1](http://arxiv.org/abs/2405.15125v1)|null|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose a novel steganography framework, GS-Hider, for 3D Gaussian Splatting (3DGS) that embeds and extracts 3D scenes and images in an invisible manner, ensuring security, fidelity, capacity, and flexibility, and demonstrating its effectiveness in various applications.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**ETA-INIT: Enhancing the Translation Accuracy for Stereo Visual-Inertial SLAM Initialization**|Han Song et.al.|The key contributions of the paper are:* A new approach for enhancing translation accuracy during the initialization stage of Stereo Visual-Inertial SLAM, which improves upon the existing methods by separately estimating rotation using IMU integration and then refining translation estimation using a 3-DoF bundle adjustment.* The method efficiently estimates rotation and translation separately, which leads to improved accuracy and robustness in challenging scenarios.* The proposed approach demonstrates excellent performance on the EuRoC dataset and shows significant potential for practical applications in real-world visual-inertial state estimation tasks.|[2405.15082v1](http://arxiv.org/abs/2405.15082v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|The authors propose a novel approach to improve Neural Radiance Fields (NeRFs) by introducing ray tracing, which enables the rendering of consistent and high-quality specular reflections of nearby and distant content, outperforming prior methods in terms of view synthesis and requiring comparable optimization time to state-of-the-art view synthesis models.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Synergistic Global-space Camera and Human Reconstruction from Videos**|Yizhou Zhao et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper introduces Synergistic Camera and Human Reconstruction (SynCHMR) that jointly reconstructs metric-scale camera poses and scene point clouds with human meshes in a common world frame, addressing depth, scale, and dynamic ambiguities by integrating human-aware metric SLAM and scene-aware SMPL denoising.|[2405.14855v1](http://arxiv.org/abs/2405.14855v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors introduce Neural Directional Encoding (NDE), a view-dependent appearance encoding for NeRF that improves the modeling of high-frequency angular signals and global illumination effects, enabling fast and accurate rendering of specular objects.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes a two-staged pipeline for camera relocalization, normalizing images with varying lighting and shadow conditions using a hash-encoded NeRF, and introduces a re-devised truncated dynamic low-pass filter and numerical gradient averaging technique to improve pose optimization.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**CoPeD-Advancing Multi-Robot Collaborative Perception: A Comprehensive Dataset in Real-World Environments**|Yang Zhou et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents a pioneering real-world multi-robot collaborative perception dataset, featuring air-ground robot collaboration, diverse sensor modalities, and high-level perception annotation, to facilitate research in this area and unlock the potential for high-level scene understanding through multi-modal collaborative perception.|[2405.14731v1](http://arxiv.org/abs/2405.14731v1)|[link](https://github.com/arplaboratory/coped)|
|**2024-05-23**|**Efficient Robot Learning for Perception and Mapping**|Niclas VÃ¶disch et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:* The abstract highlights the importance of holistic scene understanding for autonomous robots and the need to minimize human effort in deploying perception-based robotic systems to new environments.* The introduction outlines the challenges in traditional learning-based approaches that require large amounts of labeled data and proposes to leverage continual learning and label-efficient methods to overcome these limitations.* The paper focuses on using vision foundation models to enable extremely label-efficient training for vision-based robotic systems, including panoptic segmentation and SLAM.* The author's previous research contributions include:	+ Continual SLAM that enables robotic agents to adapt to new environments while retaining performance on previous domains.	+ Label-efficient panoptic segmentation that requires only a few annotated images to train.	+ Automatic target-less camera-LiDAR calibration that doesn't require human supervision or special data collection.	+ Collaborative robot mapping that fuses camera and LiDAR data.Note that the paper's abstract and introduction do not explicitly summarize the author's ongoing and future research directions, which are mentioned towards the end of the paper.|[2405.14688v1](http://arxiv.org/abs/2405.14688v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes LDM, a novel feed-forward framework that generates high-quality, illumination-decoupled textured meshes from single images or text prompts, introducing a tensorial SDF representation and adaptive conversion strategy to improve convergence speed and quality.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose JointRF, a novel end-to-end joint optimization scheme for dynamic NeRF representation and compression, achieving superior quality and compression efficiency, and eliminate the need for multi-stage training, with efficient representation and compression of 4D radiance fields.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|The paper proposes DoGaussian, a distributed training method for 3D Gaussian Splatting (3DGS) that accelerates training by 6+ times while maintaining state-of-the-art rendering quality, tackling the challenges of high GPU memory requirements and long training times for large-scale scenes, by decomposing the scene into blocks and using the Alternating Direction Method of Multipliers (ADMM) for consensus on shared 3D Gaussians.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Monocular Gaussian SLAM with Language Extended Loop Closure**|Tian Lan et.al.|The paper presents MG-SLAM, a monocular Gaussian SLAM system that addresses the limitations of existing methods by incorporating a language-extended loop closure module based on CLIP features to correct drift errors and achieve high-fidelity reconstruction without depth information.|[2405.13748v1](http://arxiv.org/abs/2405.13748v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words: The Gaussian Time Machine (GTM) model efficiently reconstructs 3D scenes with varying appearances by disentangling scene variations using a lightweight neural time encoder, achieving state-of-the-art rendering fidelity while rendering 100 times faster than NeRF-based counterparts.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|The paper introduces an innovative framework, MOSS, that combines kinematic information to achieve motion-aware Gaussian split on the human surface, which significantly improves 3D clothed human synthesis from monocular videos by capturing realistic clothing deformation and restoring detailed joints and fine clothing folds.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|The paper's key contributions are: (1) a novel method to enable an off-the-shelf spacecraft pose estimation network to be trained on an unknown target without knowing its CAD model; and (2) using an in-the-wild Neural Radiance Field to generate a large dataset for training the pose estimation network.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-21**|**NV-LIO: LiDAR-Inertial Odometry using Normal Vectors Towards Robust SLAM in Multifloor Environments**|Dongha Chung et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents NV-LIO, a normal vector-based LiDAR-inertial odometry framework designed for indoor SLAM in multifloor environments, addressing challenges such as degeneracy and wrong correspondences to achieve robust registration and mapping.|[2405.12563v1](http://arxiv.org/abs/2405.12563v1)|[link](https://github.com/dhchung/nv_lio)|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|Tianqi Liu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose MVSGaussian, a generalizable 3D Gaussian representation approach that efficiently reconstructs unseen scenes with real-time rendering and better synthesis quality, achieved through novel view synthesis, hybrid Gaussian rendering, and efficient aggregation strategies.|[2405.12218v1](http://arxiv.org/abs/2405.12218v1)|null|
|**2024-05-20**|**Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents**|Guanlin Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper provides a comprehensive overview of integrating neural radiance field (NeRF) and 3D Gaussian splatting (3D-GS) techniques in 6G wireless networks for efficient representation, transmission, and reconstruction of 3D contents, addressing challenges in communication, computation, and storage resources.|[2405.12155v1](http://arxiv.org/abs/2405.12155v1)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|Fotios Logothetis et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents a novel multi-view photometric stereo method that leverages per-pixel intensity renderings, models point light attenuation, and optimizes a fully neural material renderer to achieve competitive reconstruction accuracy using only 6 lights.|[2405.12057v1](http://arxiv.org/abs/2405.12057v1)|null|
|**2024-05-19**|**Searching Realistic-Looking Adversarial Objects For Autonomous Driving Systems**|Shengxiang Sun et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a modified gradient-based texture optimization method, incorporating a "Judge" mechanism to assess and refine the realism of adversarial objects, enhancing their feasibility for real-world applications and improving the robustness of autonomous driving systems.|[2405.11629v1](http://arxiv.org/abs/2405.11629v1)|null|
|**2024-05-19**|**R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless Environments**|Huiying Yang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction:The paper proposes a novel modeling approach using Neural Radiance Fields (NeRF) to characterize the dynamics of electromagnetic fields in RIS-enabled wireless environments. The key contributions include:* Developing a two-stage framework for simulating electromagnetic signal propagation in RIS-enabled wireless environments.* Proposing a novel R-NeRF method to model the signal field for any specified RIS placement and receiver location.* Experimental evaluations using both simulated and real-world data validate the benefits of the proposed methodology.Please note that the summary is limited to 50 words as per your request.|[2405.11541v1](http://arxiv.org/abs/2405.11541v1)|null|
|**2024-05-18**|**Outlier-Robust Long-Term Robotic Mapping Leveraging Ground Segmentation**|Hyungtae Lim et.al.|The paper proposes a robust long-term robotic mapping system that can work out of the box, addressing catastrophic failures in untrained scenarios, and enabling fast ground segmentation, outlier-robust registration, hierarchical multi-session SLAM, and instance-aware static map building.|[2405.11176v2](http://arxiv.org/abs/2405.11176v2)|null|
|**2024-05-18**|**MotionGS : Compact Gaussian Splatting SLAM by Motion Filter**|Xinli Guo et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction, under 50 words:A novel 3D Gaussian Splatting-based SLAM approach, MotionGS, integrates deep visual feature extraction, dual keyframe selection, and 3D Gaussian representation to achieve high-fidelity reconstruction, accurate tracking, and low memory usage, outperforming existing methods.|[2405.11129v1](http://arxiv.org/abs/2405.11129v1)|[link](https://github.com/antonio521/motiongs)|
|**2024-05-17**|**CCTNet: A Circular Convolutional Transformer Network for LiDAR-based Place Recognition Handling Movable Objects Occlusion**|Gang Wang et.al.|The paper proposes a novel lightweight circular convolutional Transformer network (CCTNet) to address the issues of "restricted receptive fields" and "excessive focus on local regions" in range image-based networks, achieving superior performance in place recognition with improved accuracy and robustness in scenarios with movable objects.|[2405.10793v1](http://arxiv.org/abs/2405.10793v1)|null|
|**2024-05-17**|**Occupancy-SLAM: Simultaneously Optimizing Robot Poses and Continuous Occupancy Map**|Liang Zhao et.al.|Here is a summary of the key contributions in a single sentence under 50 words:This paper proposes an optimization-based SLAM approach that jointly optimizes robot trajectories and occupancy maps using 2D laser scans and odometry, allowing for accurate map estimation and robot trajectory estimation together, with the ability to obtain uncertainties for the map.|[2405.10743v1](http://arxiv.org/abs/2405.10743v1)|null|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|Here is a summarized sentence under 50 words:The paper surveys advancements in integrating Large Language Models (LLMs) with 3D data, highlighting the potential of LLMs for spatial comprehension and interaction, and provides an overview of methodologies, applications, and challenges in this domain.|[2405.10255v1](http://arxiv.org/abs/2405.10255v1)|null|
|**2024-05-15**|**From NeRFs to Gaussian Splats, and Back**|Siming He et.al.|The key contributions from the paper's abstract and introduction are:* The authors propose a method to convert between implicit and explicit representations of 3D scenes, including neural radiance fields (NeRFs) and Gaussian splatting (GS). This allows for the benefits of both approaches: NeRFs' generalization to unseen views and GS's fast rendering and easy modification.* The authors demonstrate that their approach achieves state-of-the-art results on several benchmarks, including high-quality rendering and robustness to changes in the scene.* The authors suggest that this technique is useful for robotics applications, such as localization, planning, and scene understanding, where fast rendering and modification of scene representations are critical.* The authors also highlight the limitations of the current conversion approach, including the need for fine-tuning of the GS-based representation and the potential for divergence between NeRFs and GS.|[2405.09717v1](http://arxiv.org/abs/2405.09717v1)|[link](https://github.com/grasp-lyrl/nerftogsandback)|
