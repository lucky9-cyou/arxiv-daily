
### NeRF SLAM
|Publish Date|Title|Authors|Contributions|PDF|Code|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-05-29**|**Neural Radiance Fields for Novel View Synthesis in Monocular Gastroscopy**|Zijie Jiang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a novel view synthesis method for monocular gastroscopic images using neural radiance fields (NeRF) that incorporates geometry priors from a pre-reconstructed point cloud to improve rendering quality and recover geometry.|[2405.18863v1](http://arxiv.org/abs/2405.18863v1)|null|
|**2024-05-29**|**NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild**|Weining Ren et.al.|The authors introduce NeRF On-the-go, a novel approach that can synthesize photorealistic views in complex, dynamic scenes with moving objects, shadows, and lighting changes from casually captured image sequences, achieving a significant improvement over state-of-the-art methods and robustness to distractors, while also reducing convergence time.|[2405.18715v1](http://arxiv.org/abs/2405.18715v1)|null|
|**2024-05-28**|**Self-supervised Pre-training for Transferable Multi-modal Perception**|Xiaohao Xu et.al.|The key contributions of this paper are:• A self-supervised pre-training framework for multi-modal representation learning in autonomous driving, called NS-MAE, which can be applied to multi-modal and single-modal perception models.• A unified optimization of transferable multi-modal representations via plug-and-play designs in the spirit of multi-modal reconstruction in neural radiance fields.• The application of NS-MAE in various advanced single-modal and multi-modal perception models and the verification of the transferability of multi-modal representation derived via NS-MAE on diverse 3D perception tasks with diverse amounts of fine-tuning data.|[2405.17942v1](http://arxiv.org/abs/2405.17942v1)|null|
|**2024-05-28**|**A Refined 3D Gaussian Representation for High-Quality Dynamic Scene Reconstruction**|Bin Zhang et.al.|The key contributions of the paper are:* A hybrid representation combining deformation fields, hash encoding, and 3D-GS, which significantly reduces memory usage and achieves efficient and realistic rendering of dynamic scenes.* A learnable denoising mask in conjunction with noise loss, which effectively identifies and removes noise points in 3D-GS, enhancing rendering quality.* Static constraints and motion consistency constraints that minimize noise in points during motion, ensuring accurate and efficient rendering of dynamic scenes.Please note that the response is extracted from the abstract, introduction, and related work sections.|[2405.17891v1](http://arxiv.org/abs/2405.17891v1)|null|
|**2024-05-28**|**HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal High-Frequency Components for Endoscopic Scene Reconstruction**|Haoyu Zhao et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes HFGS, a novel approach for deformable endoscopic reconstruction addressing under-reconstruction issues, incorporating deformation fields and frequency emphasis reconstruction to improve rendering quality and dynamic awareness in neural rendering.|[2405.17872v2](http://arxiv.org/abs/2405.17872v2)|null|
|**2024-05-28**|**Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh**|Xiangjun Gao et.al.|Here is a summarized version of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper proposes Mani-GS, a method that enables 3D Gaussian Splatting (3DGS) manipulation using a triangular mesh, allowing for large deformation, local manipulation, and soft body simulation while maintaining high-quality rendering and being tolerant of mesh accuracy defects.|[2405.17811v1](http://arxiv.org/abs/2405.17811v1)|null|
|**2024-05-27**|**F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting**|Xiangyu Sun et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper introduces Factorized 3D Gaussian Splatting (F-3DGS), a novel approach that significantly reduces storage requirements while preserving image quality by factorizing coordinates and attributes of 3D Gaussians, allowing for rapid rendering speeds and compact storage.|[2405.17083v2](http://arxiv.org/abs/2405.17083v2)|null|
|**2024-05-27**|**CudaSIFT-SLAM: multiple-map visual SLAM for full procedure mapping in real human endoscopy**|Richard Elvira et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents CudaSIFT-SLAM, a V-SLAM system that overcomes the limitations of ORB-SLAM3 by using SIFT features, brute-force matching, and CudaSIFT GPU implementation, achieving real-time processing of human colonoscopies and improved mapping coverage and merge detection.|[2405.16932v1](http://arxiv.org/abs/2405.16932v1)|null|
|**2024-05-27**|**PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting**|Zipeng Wang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents Pyramidal 3D Gaussian Splatting (PyGS), a method that models large-scale scenes with hierarchical Gaussians and NeRF initialization, achieving 400x faster rendering time and capturing high-frequency details, while addressing challenges of scaling 3D Gaussian Splatting to large scenes.|[2405.16829v3](http://arxiv.org/abs/2405.16829v3)|null|
|**2024-05-26**|**Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians**|Erik Sandström et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes a RGB-only SLAM system that combines frame-to-frame tracking with a dense 3D Gaussian map representation, enabling online map deformations and accurate surface reconstruction, with improved tracking, mapping, and rendering accuracy.|[2405.16544v1](http://arxiv.org/abs/2405.16544v1)|null|
|**2024-05-26**|**Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors**|Soumava Paul et.al.|The paper proposes SparseSplat360, which uses pre-trained 2D diffusion models with fine-tuning to reconstruct 360-degree scenes from sparse views, improving performance and quality relative to existing methods.|[2405.16517v1](http://arxiv.org/abs/2405.16517v1)|null|
|**2024-05-24**|**Neural Elevation Models for Terrain Mapping and Path Planning**|Adam Dai et.al.|Here is a summary of the key contributions from the abstract and introduction:* The paper introduces Neural Elevation Models (NEMos), a novel representation that adapts Neural Radiance Fields to a 2.5D continuous and differentiable terrain model, leveraging inexpensive camera imagery and 3D reconstruction.* The authors propose a novel method for jointly training a radiance field and height field within a NeRF framework, using quantile regression to learn the height information.* They also introduce a path planning algorithm that performs gradient-based optimization of a continuous cost function for minimizing distance, slope changes, and control effort, enabled by the differentiability of the height field.* The paper presents experiments on simulated and real-world terrain imagery, demonstrating the ability of NEMos to generate high-quality reconstructions and produce smoother paths compared to discrete path planning methods.These contributions address several limitations of traditional terrain representations, such as digital elevation models (DEMs), by providing a more compact, continuous, and differentiable representation of terrain elevation, and enabling more efficient and accurate path planning.|[2405.15227v1](http://arxiv.org/abs/2405.15227v1)|null|
|**2024-05-24**|**NeB-SLAM: Neural Blocks-based Salable RGB-D SLAM for Unknown Scenes**|Lizhi Bai et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The authors propose NeB-SLAM, an end-to-end neural RGB-D SLAM system for unknown scenes that employs a divide-and-conquer mapping strategy and adaptive map growth strategy to achieve scalable and real-time mapping and tracking.|[2405.15151v1](http://arxiv.org/abs/2405.15151v1)|null|
|**2024-05-24**|**HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting**|Yuanhao Cai et.al.|The key contributions of this paper are: proposing the High Dynamic Range Gaussian Splatting (HDR-GS) framework for 3D HDR imaging, introducing a Dual Dynamic Range Gaussian point cloud model that can jointly model HDR and LDR colors, and establishing a data foundation for 3DGS-based methods in HDR imaging by recalibrating camera parameters and computing initial points.|[2405.15125v2](http://arxiv.org/abs/2405.15125v2)|[link](https://github.com/caiyuanhao1998/hdr-gs)|
|**2024-05-24**|**GS-Hider: Hiding Messages into 3D Gaussian Splatting**|Xuanyu Zhang et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The GS-Hider steganography framework, a novel 3DGS steganography method, effectively conceals multimodal messages into 3DGS point cloud files, while ensuring security, fidelity, capacity, and flexibility for 3D scene reconstruction and novel view synthesis applications.|[2405.15118v1](http://arxiv.org/abs/2405.15118v1)|null|
|**2024-05-23**|**ETA-INIT: Enhancing the Translation Accuracy for Stereo Visual-Inertial SLAM Initialization**|Han Song et.al.|The key contributions from the paper's abstract and introduction are:* The proposed method enhances the translation accuracy during the initialization stage of Stereo Visual-Inertial SLAM systems by using a 3 Degree-of-Freedom (DoF) Bundle Adjustment (BA) independently, while keeping the rotation estimate fixed.* The method updates the rotation estimate by considering IMU measurements and gyroscope bias, unlike ORB-SLAM3's rotation estimate which is directly obtained from stereo visual odometry and may yield inferior results in challenging scenarios.* The proposed method demonstrates improved performance in terms of accuracy while maintaining a comparable runtime speed compared to Stereo-NEC.Note: The paper's abstract and introduction provide a brief overview of the problem, the limitations of existing methods, and the proposed solution. The main contributions are highlighted in the abstract, while the introduction provides more context and background information.|[2405.15082v1](http://arxiv.org/abs/2405.15082v1)|null|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|Dor Verbin et.al.|The paper introduces an approach based on ray tracing to improve Neural Radiance Fields (NeRFs) in rendering highly specular objects, synthesizing consistent reflections of nearby and distant content, and reducing computational expense.|[2405.14871v1](http://arxiv.org/abs/2405.14871v1)|null|
|**2024-05-23**|**Synergistic Global-space Camera and Human Reconstruction from Videos**|Yizhou Zhao et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper introduces SynCHMR, a novel approach that jointly reconstructs camera trajectories, human meshes, and dense scene point clouds in a common world frame, addressing depth, scale, and dynamic ambiguities through synergistic human-aware metric SLAM and scene-aware SMPL denoising.|[2405.14855v1](http://arxiv.org/abs/2405.14855v1)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|Liwen Wu et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose Neural Directional Encoding (NDE), a feature-grid-like encoding of neural radiance fields for rendering specular objects, which efficiently models high-frequency view-dependent appearance and interreflection effects, outperforming state-of-the-art methods in view synthesis and achieving real-time inference.|[2405.14847v1](http://arxiv.org/abs/2405.14847v1)|null|
|**2024-05-23**|**Camera Relocalization in Shadow-free Neural Radiance Fields**|Shiyao Xu et.al.|Here is a summary of the key contributions in one sentence under 50 words:The paper proposes a two-staged pipeline to normalize images with varying lighting and shadow conditions for camera relocalization, implemented with a hash-encoded NeRF, a re-designed truncated dynamic low-pass filter, and a numerical gradient averaging technique, achieving state-of-the-art results in camera relocalization under varying lighting conditions.|[2405.14824v1](http://arxiv.org/abs/2405.14824v1)|null|
|**2024-05-23**|**CoPeD-Advancing Multi-Robot Collaborative Perception: A Comprehensive Dataset in Real-World Environments**|Yang Zhou et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper presents the first real-world multi-robot collaborative perception dataset, featuring air-ground robot collaboration with heterogeneous sensor modalities, spatial viewpoints, and coverage ranges, and provides annotations for pose estimation and high-level perception tasks.|[2405.14731v1](http://arxiv.org/abs/2405.14731v1)|[link](https://github.com/arplaboratory/coped)|
|**2024-05-23**|**Efficient Robot Learning for Perception and Mapping**|Niclas Vödisch et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The paper investigates how to minimize human effort in deploying perception-based robotic systems to previously unseen environments through continual learning and reducing human annotations for efficient learning.|[2405.14688v1](http://arxiv.org/abs/2405.14688v1)|null|
|**2024-05-23**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|Rengan Xie et.al.|The paper proposes LDM, a novel feed-forward framework generating high-fidelity, illumination-decoupled textured meshes from single images or text prompts, achieving high-quality outputs in seconds.|[2405.14580v1](http://arxiv.org/abs/2405.14580v1)|null|
|**2024-05-23**|**JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field Representation and Compression**|Zihan Zheng et.al.|Here is a summary of the key contributions from the paper's abstract and introduction in a single sentence under 50 words:The authors propose JointRF, an end-to-end joint optimization scheme for dynamic Neural Radiance Fields, which significantly improves quality and compression efficiency for dynamic and long-sequence radiance fields.|[2405.14452v1](http://arxiv.org/abs/2405.14452v1)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|Yu Chen et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The authors propose DoGaussian, a distributed training method for 3D Gaussian Splatting (3DGS), which accelerates training by 6+ times and achieves state-of-the-art rendering quality for large-scale scenes, while reducing memory footprint and training time.|[2405.13943v1](http://arxiv.org/abs/2405.13943v1)|null|
|**2024-05-22**|**Monocular Gaussian SLAM with Language Extended Loop Closure**|Tian Lan et.al.|Here is a summary of the paper's abstract and introduction in a single sentence under 50 words:The paper proposes MG-SLAM, a monocular Gaussian SLAM system that uses 3D Gaussian representation and a language-extended loop closure module to achieve drift-corrected tracking, high-fidelity reconstruction, and a high-level understanding of the environment.|[2405.13748v1](http://arxiv.org/abs/2405.13748v1)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|Licheng Shen et.al.|Here is a summary of the key contributions in a single sentence under 50 words:The paper proposes Gaussian Time Machine (GTM), a real-time rendering method that models time-dependent attributes of Gaussian primitives, allowing for accurate reconstruction of dynamic scenes with varying appearances, achieving state-of-the-art rendering fidelity and rendering 100 times faster than NeRF-based methods.|[2405.13694v1](http://arxiv.org/abs/2405.13694v1)|null|
|**2024-05-21**|**MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video**|Hongsheng Wang et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper proposes MOSS, a novel framework that incorporates kinematic information to achieve motion-aware Gaussian split on the human surface, improving 3D clothed human reconstruction accuracy and realism, particularly in scenes with large-scale motion.|[2405.12806v1](http://arxiv.org/abs/2405.12806v1)|null|
|**2024-05-21**|**Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations**|Antoine Legrand et.al.|Here is a single sentence summarizing the key contributions from the paper's abstract and introduction:The paper presents a novel method that enables an "off-the-shelf" spacecraft pose estimator to be applied on an unknown target, leveraging a Neural Radiance Field (NeRF) model to generate a diverse training set from a sparse collection of images, which is then used to train a pose estimation network for autonomous rendezvous and proximity operations.|[2405.12728v1](http://arxiv.org/abs/2405.12728v1)|null|
|**2024-05-21**|**NV-LIO: LiDAR-Inertial Odometry using Normal Vectors Towards Robust SLAM in Multifloor Environments**|Dongha Chung et.al.|The key contributions of the paper are a normal vector-based LIO framework, NV-LIO, designed for SLAM in indoor environments with multifloor structures, and an approach that extracts normal vectors from LiDAR scans to enhance point cloud registration performance, addressing issues in confined indoor settings such as degeneracy and incorrect correspondences.|[2405.12563v2](http://arxiv.org/abs/2405.12563v2)|[link](https://github.com/dhchung/nv_lio)|
